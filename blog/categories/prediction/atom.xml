<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: prediction | John Beieler]]></title>
  <link href="http://johnb30.github.com/blog/categories/prediction/atom.xml" rel="self"/>
  <link href="http://johnb30.github.com/"/>
  <updated>2013-12-28T12:59:02-06:00</updated>
  <id>http://johnb30.github.com/</id>
  <author>
    <name><![CDATA[John Beieler]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Prediction and Data Availability]]></title>
    <link href="http://johnb30.github.com/blog/2013/12/28/prediction-and-data-availability/"/>
    <updated>2013-12-28T12:23:00-06:00</updated>
    <id>http://johnb30.github.com/blog/2013/12/28/prediction-and-data-availability</id>
    <content type="html"><![CDATA[<p>Recently, there has been a large amount of news published regarding the ongoing
conflicts in the Central African Republic and South Sudan. Jay Ulfelder has
<a href="http://dartthrowingchimp.wordpress.com/2013/12/26/a-banner-year-of-the-wrong-kind/">written</a>
fairly extensively about the mass killings that are ongoing in these states.
Like Jay, I’m interested, generally, in the prediction of various
“events-of-interest,” which is a heading that mass atrocities fall under. I’m
also rather involved in using and evaluating the
<a href="http://gdelt.utdallas.edu">GDELT</a> dataset for various purposes. So, when it
became obvious that the conflicts in the Central African Republic and
South Sudan had taken a turn for the worse, I wondered if it would have been
possible to predict these events in advance. This also begins to fit nicely
with conversations I’ve had on
<a href="https://twitter.com/gutelius/status/408780327687954432">Twitter</a> and elsewhere
about the goals, evaluation, and implications of the various forecasts of
politically-relevant events. I’ll have more to say in a later post about
these issues, but I hope to start shedding a bit of light with
an examination of prediction and data availability in GDELT.</p>

<!-- more -->


<h3>Event Prediction and GDELT</h3>

<p>While much of the discussion of predictions tends to center around the
algorithms and methods used to make the predictions, a much more salient issue
for forecasters of political events is data availability. It’s fairly well known
that political data is <a href="http://dartthrowingchimp.wordpress.com/2013/11/24/singing-the-missing-data-blues/">hard to get</a>,
but many are hopeful the GDELT data <a href="http://asecondmouse.wordpress.com/2013/08/26/gdelt-is-not-the-data-fairy/">will change this</a>
since GDELT is created
through a fully automated process, is updated daily, and has a broader
coverage than probably any other political event dataset. GDELT is reliant
on media coverage, however, which creates a specific set of issues beyond the
infrequent data updates that human-curated projects experience. Given this,
my first step when asking if it was possible to predict events in CAR and
SSD was to examine the data availability for these two countries. The
results were a bit disappointing. The below plots show the daily counts of
the four major categories of events that are often used in this type of
work: verbal cooperation, material cooperation, verbal conflict, and
material conflict. The first plot shows data for the Central African Republic
for the entire date range that
GDELT covers, while the second shows a truncated range that takes into
account the sparsity of data in the earlier years of GDELT’s coverage.</p>

<p><img src="/blog/downloads/misc/car_full_range.png"></p>

<p><img src="/blog/downloads/misc/car_reduced_range.png"></p>

<p>These plots show the relatively weak coverage that even a comprehensive dataset
such as GDELT has for more remote countries such as the Central African
Republic. This isn’t a problem that is easily overcome, either using news media
or crowd-sourced data, as Jay Ulfelder points out in a great <a href="http://dartthrowingchimp.wordpress.com/2013/12/10/the-fog-of-war-is-patchy/">blog post</a>.
The story is similar for South Sudan, as the figures below show.</p>

<p><img src="/blog/downloads/misc/ssd_fixed_y.png"></p>

<p>Given this, what causes me to worry about forecasting political events is not
particularly the accuracy of the algorithms, or the things that we forecast
(when the appropriate conditions exist, we do <a href="http://papers.ssrn.com/sol3/papers.cfm?abstract_id=2225130">fairly well</a>). My main
worry focuses on data availability; you can't forecast something for which data
doesn't exist. It’s an interesting conundrum since the
places that are more “interesting” in terms of forecasting are also those that
will have poor coverage, and thus poor data. I don’t have an answer for this
issue, and I don’t really think an easy one exists. I do believe, however, that
this is a major hurdle for predictive analytics in the social sciences.</p>

<h3>Data</h3>

<p>The data used to make the figures in this post is available
<a href="/blog/downloads/misc/car_ssd_data.zip">here</a>.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Terrorism Studies and Prediction]]></title>
    <link href="http://johnb30.github.com/blog/2013/04/29/terrorism-studies-and-prediction/"/>
    <updated>2013-04-29T13:44:00-05:00</updated>
    <id>http://johnb30.github.com/blog/2013/04/29/terrorism-studies-and-prediction</id>
    <content type="html"><![CDATA[<p>This semester I'm taking a class on terrorism. Overall I've found the class
very enjoyable; the topic of political violence is one that is always
fascinating. With that said, I found one issue that would repeatedly pop up
in the readings for the seminar. Almost every paper would demarcate some type
of theory, discuss the data used, and run some statistical tests, which is
pretty standard social science research. The issue arose in the "Discussion"
or "Conclusion" sections. Almost invariably the authors would discuss the
practical implications of their research, which is fine until the dreaded
<strong>prediction</strong> word appears. Then claims about the predictive accuracy of
the models used in the paper would rear their ugly heads. These models were
explicitly <em>not</em> predictive models. This became the soap box that I would
drag out repeatedly throughout the semester. Finally, I decided to put my own
assertions to the test and see how some models performed on out-of-sample
predictive tests.</p>

<!-- more -->


<h3>The Paper</h3>

<p>The week I chose to perform this analysis we discussed papers that focused on
counterterrorism policies. One paper was <a href="http://jcr.sagepub.com/content/54/2/237">"Foreign Aid Versus Military Intervention in the War on Terror"</a>
(behind a paywall) by Jean-Paul Azam and Veronique Thelen. A full discussion
of their theory is beyond the scope of this post, but the authors' basic
argument is that foreign investment can reduce the number of transnational
terrorist events for a number of reasons. They test this general assertion
using a variety of models. Luckily, they provide replication data for their
paper on the Journal of Conflict Resolution website linked above. I set off
to exactly replicate the model presented in Table 2, Model 4 for those
following along in the paper.</p>

<h3>The Replication</h3>

<p>The first step to replicating the paper was to run their Stata <code>do-file</code> on
the data they provided. This is done to ensure that I have exactly the same
variables in my models that they do in theirs, specifically the residualized
variables they present. As a note, I will avoid commenting on the general
modeling choices made in the paper; this post is only about the predictive
accuracy of models on out-of-sample data. I work with what the authors worked
with. After running the <code>do-file</code>, I check to make sure that the results
generated by Stata are the same as those presented in the paper, which is
roughly true in the case of the coefficient estimates. I then take the data,
including the newly generated variables, and read it into Python using
<code>statsmodels</code>. After the data is read in, I replicate the formula used in
the paper, generate a train-test split of 75% and 25%, and then fit the
negative binomial model. The random 75%-25% split is appropriate here since
the data is cross sectional and not time series in nature.</p>

<p>```python
from sklearn import cross_validation
from sklearn.ensemble import RandomForestRegressor
import statsmodels.api as sm
import numpy as np
import patsy</p>

<p>data = sm.iolib.genfromdta('rep_data.dta', pandas = True)</p>

<h1>Make an R-style formula</h1>

<p>form = 'arqade ~ gdppc + logpop + odapc + secenroll + oecd + cad + ssh + ethnic + law + rodapc + rsecenroll'
y, X = patsy.dmatrices(form, data, return_type = 'dataframe')</p>

<h1>Generate 75%-25% train-test split</h1>

<p>X_train, X_test, y_train, y_test = cross_validation.train_test_split(X, y, test_size=0.25, random_state=0)</p>

<h1>Fit the model</h1>

<p>mod = sm.GLM(y_train, X_train, family = sm.families.NegativeBinomial()).fit()</p>

<p>print mod.summary()</p>

<h1>Generalized Linear Model Regression Results                  </h1>

<p>Dep. Variable:                      y   No. Observations:                   99
Model:                            GLM   Df Residuals:                       87
Model Family:        NegativeBinomial   Df Model:                           11
Link Function:                    log   Scale:                   1.54679011354
Method:                          IRLS   Log-Likelihood:                -190.88
Date:                Sun, 21 Apr 2013   Deviance:                       132.43
Time:                        16:32:56   Pearson chi2:                     135.</p>

<h1>No. Iterations:                    17                                         </h1>

<h2>coef    std err          t      P>|t|      [95.0% Conf. Int.]</h2>

<p>const          7.5576      4.717      1.602      0.113        -1.687    16.802
x1          5.861e-05    4.2e-05      1.397      0.166     -2.36e-05     0.000
x2             0.1561      0.203      0.768      0.444        -0.242     0.554
x3            -0.0339      0.014     -2.380      0.019        -0.062    -0.006
x4            -0.0633      0.019     -3.315      0.001        -0.101    -0.026
x5             0.9221      0.878      1.050      0.297        -0.800     2.644
x6             5.5344      2.225      2.487      0.015         1.173     9.896
x7            -3.3479      0.888     -3.771      0.000        -5.088    -1.608
x8             0.7016      0.200      3.510      0.001         0.310     1.093
x9            -0.2140      0.261     -0.820      0.415        -0.726     0.298
x10            0.0441      0.018      2.477      0.015         0.009     0.079</p>

<h1>x11            0.0724      0.022      3.304      0.001         0.029     0.115</h1>

<p>```</p>

<p>The results of this analysis roughly match the results presented in the paper.
Some differences are to be expected, of course, due to the reduced number of
observations. Since these results seem to match those in the paper, the
out-of-sample forecasts can be run. This is done using the <code>predict</code> method
of the <code>mod</code> object. The true and predicted results are then compared using
the root mean squared error. I'm not completely confident in the use of the
RMSE, but it is a quick and easy way to perform a comparison in this case.</p>

<p>``` python
y_pred = mod.predict(X_test)</p>

<h1>Calculate RMSE</h1>

<p>rmse = np.sqrt(np.mean(np.square(y_test - y_pred)))</p>

<p>print 'The RMSE is {}'.format(rmse)
The RMSE is 22.7156218086
```</p>

<p>The value of the RMSE error shows that the predicted values tend to be off by
about 23 events. This indicates that the model does not predict the impact
of foreign aid on terrorist events well. Admittedly, this analysis may not be
the most favorable to Azam and Thelen. In order to give them the benefit of
the doubt, I also used another modeling technique to see if this inaccuracy persists.
I used a random forest to fit the same model used in the negative binomial
and then examined the RMSE.</p>

<p>``` python</p>

<h1>Create the RF regressor object</h1>

<p>reg = RandomForestRegressor(1000, n_jobs=-1)</p>

<h1>Fit the model</h1>

<p>reg.fit(X_train, y_train)</p>

<h1>Predict using the X_test data</h1>

<p>rf_pred = reg.predict(X_test)</p>

<h1>Calculate RMSE</h1>

<p>rmse = np.sqrt(np.mean(np.square(y_test - rf_pred)))</p>

<p>print 'The RMSE is {}'.format(rmse)
The RMSE is 21.7068829972
```</p>

<p>Even using a method that is possibly more robust, the prediction still doesn't
seem very satisfactory.</p>

<h3>Models in the Social Sciences</h3>

<p>In case it was not explicitly clear in the beginning of this post, I want to
make it clear now: this analysis is not a direct critique of Azam and Thelen.
I think their theory is sound and it makes sense that foreign investment and
the other variables they analyze can make a difference in terrorism. I also
do not think that the type of inferential statistics used by Azam and Thelen,
and commonly used elsewhere in political science, are wrong in all cases.
I do believe we can gain knowledge from models like those
presented by Azam and Thelen. What I do take issue with, however, is the call
to arms that is frequently sounded in the conclusions of papers, which are
often directed towards policy makers, that the results of the paper predict
phenomenon <em>X</em> and thus policy <em>Y</em> should be enacted. Especially when policy
<em>Y</em> is something like military intervention. In order to make these types of
claims, some type of out-of-sample test should be performed in order to
determine the effect of the variables under examination.</p>
]]></content>
  </entry>
  
</feed>
