<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: GDELT | John Beieler]]></title>
  <link href="http://johnb30.github.com/blog/categories/gdelt/atom.xml" rel="self"/>
  <link href="http://johnb30.github.com/"/>
  <updated>2013-07-06T17:00:14-05:00</updated>
  <id>http://johnb30.github.com/</id>
  <author>
    <name><![CDATA[John Beieler]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Making GDELT Downloads Easy]]></title>
    <link href="http://johnb30.github.com/blog/2013/06/01/making-gdelt-downloads-easy/"/>
    <updated>2013-06-01T00:06:00-05:00</updated>
    <id>http://johnb30.github.com/blog/2013/06/01/making-gdelt-downloads-easy</id>
    <content type="html"><![CDATA[<h2>Downloading GDELT</h2>

<p>Yesterday, I started downloading the GDELT data from the <a href="gdelt.utdallas.edu">website</a>,
having previously pulled the data from the servers at Penn State.
Having to navigate the website and download each file individually
caused me far more frustration that it should have, not due to the
design of the website but due to my own impatience. I'll have more
to say about the general state of data distribution in the social
sciences in another post, but for now it's enough to say that I'm
not a fan of downloading data by hand. Because of this, I wrote some
scripts in Python to help with downloading the data. There are two
scripts, <code>download_historical.py</code> and <code>download_daily.py</code> to
download either the historical files or the continuously-updated
daily files. The code is on <a href="https://github.com/johnb30/gdelt_download">github</a>.
I've copied the contents of the README file below so you can determine
if the scripts would be useful for you.</p>

<!-- more -->


<h3>README</h3>

<h2>GDELT Download</h2>

<p>The GDELT data is spread across multiple files, with a new file added each day.
Downloading each and every file is not a fun endeavor. These scripts were
written in order to aid in the download of the GDELT data. The first script
<code>download_historical.py</code> is aimed at downloading the historical data, and the
previous daily updated. The second script, <code>download_daily.py</code>, is aimed
at downloading the new files that are uploaded to the GDELT website each day.
This script enables the user to either call the script each day to fetch the
newest upload, or to run the process in the background to download the new
updates each day at 10:00am.</p>

<p>Each script implements a 30 second delay where appropriate in order to
avoid swamping the server.</p>

<h2><code>download_historical</code> Usage</h2>

<p>The script has three modes: <code>daily</code>, <code>single</code>, and <code>range</code>.</p>

<p><em>Note</em>: If you wish to use the <code>daily</code> mode, the <code>requests</code> and <code>lxml</code> libaries
are necessary. You can install both using <code>pip install library_name</code>. The script
also makes use of <code>argparse</code>, which is included in the standard libary from
Python 2.7+. If you are using an older version, it is necessary to install
<code>argparse</code> using <code>pip</code> or <code>easy_install</code>.</p>

<h3>Daily:</h3>

<p>The daily mode downloads the daily updates that are currently uploaded to the
GDELT website.</p>

<h4>Usage:</h4>

<p><code>python download_historical.py daily -d ~/gdelt/ -U</code></p>

<p>Where <code>-d</code> is the flag for the directory to which the files should be written,
and <code>-U</code> is the optional flag indicating whether each downloaded file should
be unzipped.</p>

<h3>Single:</h3>

<p>The single mode downloads the updates for a single year.</p>

<h4>Usage:</h4>

<p><code>python download_historical.py single -y 1979 -d ~/gdelt/ -U</code></p>

<p>Where <code>-y</code> is the flag that indicates which year should be downloaded, <code>-d</code>
is the flag for the directory to which the files should be written, and <code>-U</code>
is the optional flag indicating whether each downloaded file should be unzipped.</p>

<h3>Range:</h3>

<p>The range mode downloads the updates for a range of years.</p>

<h4>Usage:</h4>

<p><code>python download_historical.py range -y 1979-2012 -d ~/gdelt/ -U</code></p>

<p>Where <code>-y</code> is the flag that indicates which years should be downloaded, <code>-d</code>
is the flag for the directory to which the files should be written, and <code>-U</code>
is the optional flag indicating whether each downloaded file should be unzipped.</p>

<h2><code>download_daily</code> Usage</h2>

<p>The script has two modes: <code>fetch</code>, and <code>schedule</code>.</p>

<p><em>Note</em>: If you wish to use the <code>schedule</code> mode, the <code>schedule</code> library
is necessary. You can install using <code>pip install schedule</code>.</p>

<h3>Fetch:</h3>

<p>The fetch mode downloads only the current date's update.</p>

<h4>Usage:</h4>

<p><code>python download_daily.py fetch -d ~/gdelt/ -U</code></p>

<p>Where <code>-d</code> is the flag for the directory to which the files should be written,
and <code>-U</code> is the optional flag indicating whether each downloaded file should
be unzipped.</p>

<h3>Schedule:</h3>

<p>The fetch schedule mode sets the script to run in the background and request
each day at 10:00am that date's upload from the server. In order to work, the
script must be left running in a terminal tab. The use of a utility such as
<code>screen</code> or <code>tmux</code> is recommended in order to allow the program to run
unmonitored in the background.</p>

<h4>Usage:</h4>

<p><code>python download_daily.py schedule -d ~/gdelt/ -U</code></p>

<p>Where <code>-d</code> is the flag for the directory to which the files should be written,
and <code>-U</code> is the optional flag indicating whether each downloaded file should
be unzipped.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Actor Codes in GDELT]]></title>
    <link href="http://johnb30.github.com/blog/2013/04/23/actor-codes-in-gdelt/"/>
    <updated>2013-04-23T18:40:00-05:00</updated>
    <id>http://johnb30.github.com/blog/2013/04/23/actor-codes-in-gdelt</id>
    <content type="html"><![CDATA[<p>As I'm sure anyone who has begun exploring the GDELT data has noticed, there
are a large number of unique actors coded in the data. While working on a
project for a colleague, I began to wonder exactly how many unique actor
codes exist in the dataset, and what the maximum level of actor code complexity
is within the dataset. For those who don't know, the actor codes are created
by chaining together three character CAMEO actor codes, which results in actor
codes that look like <code>USAMIL</code>, which indicates the United States military. The complexity
of the actor code, then, refers to how many of these three character codes are
chained together.</p>

<h4>Results</h4>

<p>Using the Python code available <a href="https://gist.github.com/johnb30/5448269">here</a>, I iterated over the dataset and
identified each unique actor code in the data, along with how many times the
unique code appeared. The resulting data is available
<a href="/blog/downloads/unique_actors.csv">here</a>. The results indicate that there are <strong>22,857</strong>
unique actors in the dataset, with a maximum of <strong>6</strong> three character codes chained
together. The following two plots show the top twenty actor codes overall,
along with the top twenty USA actor codes.</p>

<p><img src="/blog/downloads/top20.png"></p>

<p><img src="/blog/downloads/usa_top20.png"></p>

<p>This analysis shows the incredible complexity that is present in the actor codes,
which I would argue is the most important, fascinating, and challenging part
of working with event data on this scale.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[GDELT, Big Data, and Theory]]></title>
    <link href="http://johnb30.github.com/blog/2013/04/12/gdelt/"/>
    <updated>2013-04-12T14:56:00-05:00</updated>
    <id>http://johnb30.github.com/blog/2013/04/12/gdelt</id>
    <content type="html"><![CDATA[<p>I made the remark on Twitter that it seemed like GDELT week due to a
<a href="http://ideas.foreignpolicy.com/posts/2013/04/10%20what_can_we_learn_from_the_last_200_million_things_that_happened_in_the_world">Foreign Policy</a>
piece about the dataset, Phil and Kalev's <a href="http://eventdata.psu.edu/papers.dir/ISA.2013.GDELT.pdf">paper</a>
for the ISA 2013 meeting, and a <a href="https://dartthrowingchimp.wordpress.com/2013/04/10/the-future-of-political-science-just-showed-up/">host</a>
of <a href="http://badhessian.org/gdelt-and-social-movements/">blog</a>
<a href="https://willopines.wordpress.com/2013/04/11/excitement-about-gdelt-and-some-personal-intellectual-history/">posts</a>
about the data. So, in the spirit of GDELT week, I thought I would throw my hat into
the ring. But instead of taking the approach of lauding the new age that is approaching
for political and social research due to the monstrous scale of the data now available, I thought
I would write a little about the issues that come along with dealing with such massive data.</p>

<h3>Dealing with GDELT</h3>

<p>As someone who has spent the better part of the past 8 months dealing with the GDELT dataset,
including <a href="http://johnbeieler.org/blog/2013/04/04/how-do-i-gdelt-subsetting-and-aggregating-the-gdelt-dataset/">writing</a> a little about
working with the data, I feel that I have a somewhat unique perspective. The long and the
short of my experience is: working with data on this scale is hard. This may strike some
as obvious, especially given the cottage industry that has sprung up around Hadoop and
and other services for processing data. GDELT is 200+ million events spread across several
years. Each year of the <a href="http://eventdata.psu.edu/data.dir/GDELT.html">reduced data</a> is in
a separate file and contains information about many, many different actors. This is part of
what makes the data so intriguing and useful, but the data is also unlike data such as the
ever-popular <a href="http://www.correlatesofwar.org/COW2%20Data/MIDs/MID310.html">MID data</a> in
political science that is easily managed in a program like Stata or <code>R</code>. The data requires
subsetting, massaging, and aggregating; having so much data can, at some points, become
overwhelming. What states do I want to look at? What type of actors? What type of actions?
What about substate actors? Oh, what about the dyadic interactions? These questions and
more quickly come to the fore when dealing with data on this scale. So while the GDELT
data offers an avenue to answer some existing questions, it also brings with it many
potential problems.</p>

<h3>Careful Research</h3>

<p>So, that all sounds kind of depressing. We have this new, cool dataset that could
be tremendously useful, but it also presents many hurdles. What, then, should we
as social science researchers do about it? My answer is careful theorizing and
thinking about the processes under examination. This might be a "well, duh"
moment to those in the social sciences, but I think it is worth saying when
there are some heralding <a href="http://www.wired.com/science/discoveries/magazine/16-07/pb_theory">"The End of Theory"</a>.
This type of large-scale data does not reduce theory and the scientific
method to irrelevance. Instead, theory is elevated to a position of
higher importance. What states do I want to look at? What type of
actions? Well, what does the theory say? As Hilary Mason noted in
a <a href="https://twitter.com/hmason/status/294930646122504192">tweet</a>:</p>

<blockquote><p>Data tells you whether to use A or B. Science tells you what A and B should be in the first place.</p></blockquote>

<p>Put into more social-scientific language, data tells us the relationship
between A and B, while science tells us what A and B should be and what type
of observations should be used. The data under examination in a given study
should be driven by careful consideration of the processes of interest.
This idea should not, however, be construed as a rejection of "big data" in the
social sciences. I personally believe the exact opposite; give me as many features,
measures, and observations as possible and let algorithms sort out what is important.
Instead, I think the social sciences, and science in general, is about asking
interesting questions of the data that will often require more finesse than taking an
"ANALYZE ALL THE DATA" approach. Thus, while datasets like GDELT provide new opportunities,
they are not opportunities to relax and let the data do the talking. If anything, big data
generating processes will require more work on the part of the researcher than previous
data sources.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[How Do I GDELT?: Subsetting and Aggregating the GDELT Dataset]]></title>
    <link href="http://johnb30.github.com/blog/2013/04/04/how-do-i-gdelt-subsetting-and-aggregating-the-gdelt-dataset/"/>
    <updated>2013-04-04T19:15:00-05:00</updated>
    <id>http://johnb30.github.com/blog/2013/04/04/how-do-i-gdelt-subsetting-and-aggregating-the-gdelt-dataset</id>
    <content type="html"><![CDATA[<h3>GDELT</h3>

<p>Over the past week, the Global Data on Events, Location and Tone (GDELT)
dataset was finally released to the general public. The data is available
at the Penn State event data <a href="http://eventdata.psu.edu/data.dir/GDELT.html">website</a>.
We at Penn State had the
good fortune to have access to this dataset for many months before its public
release. This allowed us to gain some experience working with this massive
collection of data. As a brief background, GDELT is comprised of event
data records spanning 1979 - mid 2012. The events are coded according to the
<a href="http://eventdata.psu.edu/data.dir/cameo.html">CAMEO</a> coding scheme, with the
addition of a "QuadCategory," which separates the events into the material conflict,
material cooperation, verbal conflict, and verbal conflict categories. The data is
spread across 33 different files, each of which is substantially large on its own.
This makes it fairly difficult to work with, and almost guarantees that some
subset of the data is necessary in order to perform analysis. Phil Schrodt has
included some programs with the data to aid in this subsetting, but I thought there
might be some who would prefer to get their hands dirty and write some of their
own code. Given this, I thought I would share some of the knowledge I gained
while working with the GDELT dataset.</p>

<p>For the purposes of this brief introduction, I will work under the assumption
that the desired events are those that originate from the United States, are
directed at some type of state actor, and are either verbal cooperation or
conflict. The following code, written in Python, demonstrates how this subset
might be selected from the GDELT data. The code also assumes the reader
has the <code>pandas</code> and <code>path</code> Python modules installed. Both can be installed using
the normal <code>pip install</code> method. Finally, the complete code is available as
a gist on <a href="https://gist.github.com/johnb30/5316196">github</a>.</p>

<p>Before starting, it is always useful to take a peek at the data. This is as
simple as opening up the terminal and using <code>head 1979.reduced.txt</code>. Doing this
shows the various columns in the reduced data and how they are arranged. We
can see that the date is in the 0 index, with <code>Actor1Code</code> and <code>Actor2Code</code>
in spots 1 and 2, respectively. Additionally, <code>EventCode</code> is located in spot 3,
while the <code>QuadCategory</code> variable is, fittingly, in position 4. These indices
will prove crucial when it comes time to split and subset the data.</p>

<!-- more -->


<h3>Code</h3>

<p>``` python
from path import path
import pandas as pd</p>

<p>allActors = ['AFG', 'ALA', 'ALB', 'DZA', 'ASM', 'AND', 'AGO', 'AIA', 'ATG',</p>

<pre><code>        'ARG', 'ARM', 'ABW', 'AUS', 'AUT', 'AZE', 'BHS', 'BHR', 'BGD', 
        'BRB', 'BLR', 'BEL', 'BLZ', 'BEN', 'BMU', 'BTN', 'BOL', 'BIH', 
        'BWA', 'BRA', 'VGB', 'BRN', 'BGR', 'BFA', 'BDI', 'KHM', 'CMR',
        'CAN', 'CPV', 'CYM', 'CAF', 'TCD', 'CHL', 'CHN', 'COL', 'COM', 
        'COD', 'COG', 'COK', 'CRI', 'CIV', 'HRV', 'CUB', 'CYP', 'CZE', 
        'DNK', 'DJI', 'DMA', 'DOM', 'TMP', 'ECU', 'EGY', 'SLV', 'GNQ', 
        'ERI', 'EST', 'ETH', 'FRO', 'FLK', 'FJI', 'FIN', 'FRA', 'GUF',
        'PYF', 'GAB', 'GMB', 'GEO', 'DEU', 'GHA', 'GIB', 'GRC', 'GRL',
        'GRD', 'GLP', 'GUM', 'GTM', 'GIN', 'GNB', 'GUY', 'HTI', 'VAT', 
        'HND', 'HKG', 'HUN', 'ISL', 'IND', 'IDN', 'IRN', 'IRQ', 'IRL', 
        'IMY', 'ISR', 'ITA', 'JAM', 'JPN', 'JOR', 'KAZ', 'KEN', 'KIR',
        'PRK', 'KOR', 'KWT', 'KGZ', 'LAO', 'LVA', 'LBN', 'LSO', 'LBR', 
        'LBY', 'LIE', 'LTU', 'LUX', 'MAC', 'MKD', 'MDG', 'MWI', 'MYS',
        'MDV', 'MLI', 'MLT', 'MHL', 'MTQ', 'MRT', 'MUS', 'MYT', 'MEX',
        'FSM', 'MDA', 'MCO', 'MNG', 'MTN', 'MSR', 'MAR', 'MOZ', 'MMR',
        'NAM', 'NRU', 'NPL', 'NLD', 'ANT', 'NCL', 'NZL', 'NIC', 'NER',
        'NGA', 'NIU', 'NFK', 'MNP', 'NOR', 'PSE', 'OMN', 'PAK', 'PLW', 
        'PAN', 'PNG', 'PRY', 'PER', 'PHL', 'PCN', 'POL', 'PRT', 'PRI', 
        'QAT', 'REU', 'ROM', 'RUS', 'RWA', 'SHN', 'KNA', 'LCA', 'SPM', 
        'VCT', 'WSM', 'SMR', 'STP', 'SAU', 'SEN', 'SRB', 'SYC', 'SLE', 
        'SGP', 'SVK', 'SVN', 'SLB', 'SOM', 'ZAF', 'ESP', 'LKA', 'SDN',
        'SUR', 'SJM', 'SWZ', 'SWE', 'CHE', 'SYR', 'TJK', 'TZA', 'THA', 
        'TGO', 'TKL', 'TON', 'TTO', 'TUN', 'TUR', 'TKM', 'TCA', 'TUV',
        'UGA', 'UKR', 'ARE', 'GBR', 'USA', 'VIR', 'URY', 'UZB', 'VUT', 
        'VEN', 'VNM', 'WLF', 'ESH', 'YEM', 'ZMB', 'ZWE']
</code></pre>

<p>quad_codes = ['2', '3']</p>

<p>filepaths = path.getcwd().files('*.reduced.txt')
output = list()
```</p>

<p>This first portion is simply importing the necessary modules, defining the list
of state actors, obtaining the relevant filepaths, and defining an empty list,
which will serve as the container for the subset of the data.
As a brief note, the <code>path</code> module is really fantastic and makes working with
filepaths and directories extremely simple.</p>

<p>``` python
for path in filepaths:</p>

<pre><code>data = open(path, 'r')
print 'Just read in the %s data...' % path
for line in data:
    line = line.replace('\n', '')
    split_line = line.split('\t')
    condition1 = split_line[1][0:3] == 'USA'
    condition2 = split_line[2][0:3] != 'USA'
    condition3 = split_line[2][0:3] in allActors
    condition4 = split_line[4] in quad_codes
</code></pre>

<p>```</p>

<p>The next portion of code iterates over the filepaths obtained using <code>path</code>.
Each file is then opened and iterated over line by line. Each line
has any new-line characters replaced, which makes the data easier to work with,
and is then split on the basis of tab characters ('\t'). The following four lines
define the logical conditions for subsetting the data. The first condition
indicates that the first three characters of <code>Actor1Code</code> should be
'USA', while <code>condition2</code> states that the first three characters of
<code>Actor2Code</code> should <em>not</em> equal 'USA'. <code>condition3</code> checks if the first three
characters of <code>Actor2Code</code> are in the <code>allActors</code> list defined earlier, while
<code>condition4</code> checks if the <code>QuadCategory</code> is one of the desired values.</p>

<p>``` python</p>

<pre><code>try:
    if all([condition1, condition2, condition3, condition4]):
        output.append(split_line)
except IndexError:
    pass
</code></pre>

<p>```</p>

<p>The above code simply checks if all of the various conditions were met, and
if so appends the <code>split_line</code> to <code>output</code>. This code is wrapped in a try-except
statement since there can be some malformed lines floating in the data, but this
should not affect the actual event data. The try-except statements allow for
an attempt at a certain block of code, and if an error is raised, in this case
an <code>IndexError</code>, for some other actions to occur.</p>

<p>``` python
header = open(filepaths[0], 'r').readline().split('\t')
subset = pd.DataFrame(output, columns = header)</p>

<p>subset['year'] = subset['Day'].map(lambda x: int(str(x)[0:4]))
subset['month'] = subset['Day'].map(lambda x: int(str(x)[4:6]))</p>

<p>keep_columns = ['year', 'month', 'Actor1Code', 'Actor2Code', 'QuadCategory']
subset = subset[keep_columns]
```</p>

<p>Once the code from the previous sections is finished, the subset of the data is
contained in <code>output</code>, which is a list-of-lists with the internal lists representing
the individual events. It is possible, using the <code>pandas</code> library, to convert this
list-of-lists to a pandas DataFrame object, with the header drawn from the first line
of the first file in <code>filepaths</code>. In order to aggregate the data to a specific time period,
it is useful to break out the <code>Day</code> variable into months and years using the <code>.map</code>
functionality of a Series object in <code>pandas</code>. The <code>.map</code> functionality is combined
with a lambda function to select eaach observation in the series and slice it
in order to obtain the year and month. Finally, the last two lines of the above
code reduce the data down to only the columns relevant for this subset.</p>

<p>``` python
subset['verbal_coop'] = 0
subset['verbal_conf'] = 0</p>

<p>subset['verbal_coop'][subset['QuadCategory'] == '2'] = 1
subset['verbal_conf'][subset['QuadCategory'] == '3'] = 1</p>

<p>subset_grouped = subset.groupby(['year', 'month', 'Actor1Code',
'Actor2Code'], as_index = False)
subset_aggregated = subset_grouped.sum()</p>

<p>subset_aggregated.to_csv('gdelt_subset.csv', index = False)
```</p>

<p>Now that the data is in a properly formatted DataFrame, the next step is to create
variables from which to draw counts of the various event types. Two variables,
<code>verbal_coop</code> and <code>verbal_conf</code> are created and assigned values of zero. Then,
these variables are assigned values of one if the <code>QuadCategory</code> matches the
value for that event type. This functionality in <code>pandas</code> is similar that of
<code>R</code>, and I plan on doing a more in-depth tutorial on <code>pandas</code> at a later date.
With the event variables created, the data can be grouped and aggregated. <code>pandas</code>
has the <code>.groupby</code> functionality for DataFrames, which allows you to group the data
by specific variables. For the purposes of this dataset, a multi-level grouping is
desired, with groups created by the year, the month, and the dyad. Once this grouping
is created, the values are summed within each grouping leading to the final, aggregated
dataset. This final data can be saved using the <code>.to_csv</code> method.</p>

<p>And there you have it, you now have a subset of the GDELT dataset.
This is a relatively simple task thanks to Python's built in tools. It is
possible to make this task run more quickly using parallel processing, as
outlined in my previous <a href="http://johnbeieler.org/blog/2012/12/07/parallel-data-subsetting/">post</a>
on parallel subsetting. As a brief recap, it is simply
a matter of using the <code>jobilb</code> module, wrapping the above subsetting code in
a function, and adding something along the lines of</p>

<pre><code>data = Parallel(n_jobs=-1)(delayed(subset)(x) for x in list_of_paths)
</code></pre>

<p>to the script. I hope this brief intro will prove helpful to those who wish
to use GDELT in their own research. This tutorial only scratched the surface
of working with event data, and there is much more to consider beyond just
what subset you will select. A good resource on working with event data
was written by <a href="http://jayyonamine.com/wp-content/uploads/2012/06/Working-with-Event-Data-A-Guide-to-Aggregation-Choices.pdf">Jay Yonamine</a>
and will likely prove useful to those engaging in event-data research.</p>
]]></content>
  </entry>
  
</feed>
