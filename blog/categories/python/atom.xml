<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Python | John Beieler]]></title>
  <link href="http://johnb30.github.com/blog/categories/python/atom.xml" rel="self"/>
  <link href="http://johnb30.github.com/"/>
  <updated>2013-04-23T19:31:18-04:00</updated>
  <id>http://johnb30.github.com/</id>
  <author>
    <name><![CDATA[John Beieler]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[How Do I GDELT?: Subsetting and Aggregating the GDELT Dataset]]></title>
    <link href="http://johnb30.github.com/blog/2013/04/04/how-do-i-gdelt-subsetting-and-aggregating-the-gdelt-dataset/"/>
    <updated>2013-04-04T19:15:00-04:00</updated>
    <id>http://johnb30.github.com/blog/2013/04/04/how-do-i-gdelt-subsetting-and-aggregating-the-gdelt-dataset</id>
    <content type="html"><![CDATA[<h3>GDELT</h3>

<p>Over the past week, the Global Data on Events, Location and Tone (GDELT)
dataset was finally released to the general public. The data is available
at the Penn State event data <a href="http://eventdata.psu.edu/data.dir/GDELT.html">website</a>.
We at Penn State had the
good fortune to have access to this dataset for many months before its public
release. This allowed us to gain some experience working with this massive
collection of data. As a brief background, GDELT is comprised of event
data records spanning 1979 - mid 2012. The events are coded according to the
<a href="http://eventdata.psu.edu/data.dir/cameo.html">CAMEO</a> coding scheme, with the
addition of a "QuadCategory," which separates the events into the material conflict,
material cooperation, verbal conflict, and verbal conflict categories. The data is
spread across 33 different files, each of which is substantially large on its own.
This makes it fairly difficult to work with, and almost guarantees that some
subset of the data is necessary in order to perform analysis. Phil Schrodt has
included some programs with the data to aid in this subsetting, but I thought there
might be some who would prefer to get their hands dirty and write some of their
own code. Given this, I thought I would share some of the knowledge I gained
while working with the GDELT dataset.</p>

<p>For the purposes of this brief introduction, I will work under the assumption
that the desired events are those that originate from the United States, are
directed at some type of state actor, and are either verbal cooperation or
conflict. The following code, written in Python, demonstrates how this subset
might be selected from the GDELT data. The code also assumes the reader
has the <code>pandas</code> and <code>path</code> Python modules installed. Both can be installed using
the normal <code>pip install</code> method. Finally, the complete code is available as
a gist on <a href="https://gist.github.com/johnb30/5316196">github</a>.</p>

<p>Before starting, it is always useful to take a peek at the data. This is as
simple as opening up the terminal and using <code>head 1979.reduced.txt</code>. Doing this
shows the various columns in the reduced data and how they are arranged. We
can see that the date is in the 0 index, with <code>Actor1Code</code> and <code>Actor2Code</code>
in spots 1 and 2, respectively. Additionally, <code>EventCode</code> is located in spot 3,
while the <code>QuadCategory</code> variable is, fittingly, in position 4. These indices
will prove crucial when it comes time to split and subset the data.</p>

<h3>Code</h3>

<p>``` python
from path import path
import pandas as pd</p>

<p>allActors = ['AFG', 'ALA', 'ALB', 'DZA', 'ASM', 'AND', 'AGO', 'AIA', 'ATG',</p>

<pre><code>        'ARG', 'ARM', 'ABW', 'AUS', 'AUT', 'AZE', 'BHS', 'BHR', 'BGD', 
        'BRB', 'BLR', 'BEL', 'BLZ', 'BEN', 'BMU', 'BTN', 'BOL', 'BIH', 
        'BWA', 'BRA', 'VGB', 'BRN', 'BGR', 'BFA', 'BDI', 'KHM', 'CMR',
        'CAN', 'CPV', 'CYM', 'CAF', 'TCD', 'CHL', 'CHN', 'COL', 'COM', 
        'COD', 'COG', 'COK', 'CRI', 'CIV', 'HRV', 'CUB', 'CYP', 'CZE', 
        'DNK', 'DJI', 'DMA', 'DOM', 'TMP', 'ECU', 'EGY', 'SLV', 'GNQ', 
        'ERI', 'EST', 'ETH', 'FRO', 'FLK', 'FJI', 'FIN', 'FRA', 'GUF',
        'PYF', 'GAB', 'GMB', 'GEO', 'DEU', 'GHA', 'GIB', 'GRC', 'GRL',
        'GRD', 'GLP', 'GUM', 'GTM', 'GIN', 'GNB', 'GUY', 'HTI', 'VAT', 
        'HND', 'HKG', 'HUN', 'ISL', 'IND', 'IDN', 'IRN', 'IRQ', 'IRL', 
        'IMY', 'ISR', 'ITA', 'JAM', 'JPN', 'JOR', 'KAZ', 'KEN', 'KIR',
        'PRK', 'KOR', 'KWT', 'KGZ', 'LAO', 'LVA', 'LBN', 'LSO', 'LBR', 
        'LBY', 'LIE', 'LTU', 'LUX', 'MAC', 'MKD', 'MDG', 'MWI', 'MYS',
        'MDV', 'MLI', 'MLT', 'MHL', 'MTQ', 'MRT', 'MUS', 'MYT', 'MEX',
        'FSM', 'MDA', 'MCO', 'MNG', 'MTN', 'MSR', 'MAR', 'MOZ', 'MMR',
        'NAM', 'NRU', 'NPL', 'NLD', 'ANT', 'NCL', 'NZL', 'NIC', 'NER',
        'NGA', 'NIU', 'NFK', 'MNP', 'NOR', 'PSE', 'OMN', 'PAK', 'PLW', 
        'PAN', 'PNG', 'PRY', 'PER', 'PHL', 'PCN', 'POL', 'PRT', 'PRI', 
        'QAT', 'REU', 'ROM', 'RUS', 'RWA', 'SHN', 'KNA', 'LCA', 'SPM', 
        'VCT', 'WSM', 'SMR', 'STP', 'SAU', 'SEN', 'SRB', 'SYC', 'SLE', 
        'SGP', 'SVK', 'SVN', 'SLB', 'SOM', 'ZAF', 'ESP', 'LKA', 'SDN',
        'SUR', 'SJM', 'SWZ', 'SWE', 'CHE', 'SYR', 'TJK', 'TZA', 'THA', 
        'TGO', 'TKL', 'TON', 'TTO', 'TUN', 'TUR', 'TKM', 'TCA', 'TUV',
        'UGA', 'UKR', 'ARE', 'GBR', 'USA', 'VIR', 'URY', 'UZB', 'VUT', 
        'VEN', 'VNM', 'WLF', 'ESH', 'YEM', 'ZMB', 'ZWE']
</code></pre>

<p>quad_codes = ['2', '3']</p>

<p>filepaths = path.getcwd().files('*.reduced.txt')
output = list()
```</p>

<p>This first portion is simply importing the necessary modules, defining the list
of state actors, obtaining the relevant filepaths, and defining an empty list,
which will serve as the container for the subset of the data.
As a brief note, the <code>path</code> module is really fantastic and makes working with
filepaths and directories extremely simple.</p>

<p>``` python
for path in filepaths:</p>

<pre><code>data = open(path, 'r')
print 'Just read in the %s data...' % path
for line in data:
    line = line.replace('\n', '')
    split_line = line.split('\t')
    condition1 = split_line[1][0:3] == 'USA'
    condition2 = split_line[2][0:3] != 'USA'
    condition3 = split_line[2][0:3] in allActors
    condition4 = split_line[4] in quad_codes
</code></pre>

<p>```</p>

<p>The next portion of code iterates over the filepaths obtained using <code>path</code>.
Each file is then opened and iterated over line by line. Each line
has any new-line characters replaced, which makes the data easier to work with,
and is then split on the basis of tab characters ('\t'). The following four lines
define the logical conditions for subsetting the data. The first condition
indicates that the first three characters of <code>Actor1Code</code> should be
'USA', while <code>condition2</code> states that the first three characters of
<code>Actor2Code</code> should <em>not</em> equal 'USA'. <code>condition3</code> checks if the first three
characters of <code>Actor2Code</code> are in the <code>allActors</code> list defined earlier, while
<code>condition4</code> checks if the <code>QuadCategory</code> is one of the desired values.</p>

<p>``` python</p>

<pre><code>try:
    if all([condition1, condition2, condition3, condition4]):
        output.append(split_line)
except IndexError:
    pass
</code></pre>

<p>```</p>

<p>The above code simply checks if all of the various conditions were met, and
if so appends the <code>split_line</code> to <code>output</code>. This code is wrapped in a try-except
statement since there can be some malformed lines floating in the data, but this
should not affect the actual event data. The try-except statements allow for
an attempt at a certain block of code, and if an error is raised, in this case
an <code>IndexError</code>, for some other actions to occur.</p>

<p>``` python
header = open(filepaths[0], 'r').readline().split('\t')
subset = pd.DataFrame(output, columns = header)</p>

<p>subset['year'] = subset['Day'].map(lambda x: int(str(x)[0:4]))
subset['month'] = subset['Day'].map(lambda x: int(str(x)[4:6]))</p>

<p>keep_columns = ['year', 'month', 'Actor1Code', 'Actor2Code', 'QuadCategory']
subset = subset[keep_columns]
```</p>

<p>Once the code from the previous sections is finished, the subset of the data is
contained in <code>output</code>, which is a list-of-lists with the internal lists representing
the individual events. It is possible, using the <code>pandas</code> library, to convert this
list-of-lists to a pandas DataFrame object, with the header drawn from the first line
of the first file in <code>filepaths</code>. In order to aggregate the data to a specific time period,
it is useful to break out the <code>Day</code> variable into months and years using the <code>.map</code>
functionality of a Series object in <code>pandas</code>. The <code>.map</code> functionality is combined
with a lambda function to select eaach observation in the series and slice it
in order to obtain the year and month. Finally, the last two lines of the above
code reduce the data down to only the columns relevant for this subset.</p>

<p>``` python
subset['verbal_coop'] = 0
subset['verbal_conf'] = 0</p>

<p>subset['verbal_coop'][subset['QuadCategory'] == '2'] = 1
subset['verbal_conf'][subset['QuadCategory'] == '3'] = 1</p>

<p>subset_grouped = subset.groupby(['year', 'month', 'Actor1Code',
'Actor2Code'], as_index = False)
subset_aggregated = subset_grouped.sum()</p>

<p>subset_aggregated.to_csv('gdelt_subset.csv', index = False)
```</p>

<p>Now that the data is in a properly formatted DataFrame, the next step is to create
variables from which to draw counts of the various event types. Two variables,
<code>verbal_coop</code> and <code>verbal_conf</code> are created and assigned values of zero. Then,
these variables are assigned values of one if the <code>QuadCategory</code> matches the
value for that event type. This functionality in <code>pandas</code> is similar that of
<code>R</code>, and I plan on doing a more in-depth tutorial on <code>pandas</code> at a later date.
With the event variables created, the data can be grouped and aggregated. <code>pandas</code>
has the <code>.groupby</code> functionality for DataFrames, which allows you to group the data
by specific variables. For the purposes of this dataset, a multi-level grouping is
desired, with groups created by the year, the month, and the dyad. Once this grouping
is created, the values are summed within each grouping leading to the final, aggregated
dataset. This final data can be saved using the <code>.to_csv</code> method.</p>

<p>And there you have it, you now have a subset of the GDELT dataset.
This is a relatively simple task thanks to Python's built in tools. It is
possible to make this task run more quickly using parallel processing, as
outlined in my previous <a href="http://johnbeieler.org/blog/2012/12/07/parallel-data-subsetting/">post</a>
on parallel subsetting. As a brief recap, it is simply
a matter of using the <code>jobilb</code> module, wrapping the above subsetting code in
a function, and adding something along the lines of</p>

<pre><code>data = Parallel(n_jobs=-1)(delayed(subset)(x) for x in list_of_paths)
</code></pre>

<p>to the script. I hope this brief intro will prove helpful to those who wish
to use GDELT in their own research. This tutorial only scratched the surface
of working with event data, and there is much more to consider beyond just
what subset you will select. A good resource on working with event data
was written by <a href="http://jayyonamine.com/wp-content/uploads/2012/06/Working-with-Event-Data-A-Guide-to-Aggregation-Choices.pdf">Jay Yonamine</a>
and will likely prove useful to those engaging in event-data research.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Web Scraping Tutorial]]></title>
    <link href="http://johnb30.github.com/blog/2013/02/08/web-scraping-tutorial/"/>
    <updated>2013-02-08T20:39:00-05:00</updated>
    <id>http://johnb30.github.com/blog/2013/02/08/web-scraping-tutorial</id>
    <content type="html"><![CDATA[<p>I had the opportunity to give a short tutorial on web scraping for the Event
Data class here at Penn State. I used an IPython notebook to give the
presentation and I've put the code in a gist. The link to the IPython notebook
is <a href="http://nbviewer.ipython.org/4743272">http://nbviewer.ipython.org/4743272/</a>.</p>

<p>The PITF project I make reference to is hosted on
<a href="https://github.com/johnb30/atrocitiesProject">github</a>.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Parallel Data Subsetting]]></title>
    <link href="http://johnb30.github.com/blog/2012/12/07/parallel-data-subsetting/"/>
    <updated>2012-12-07T18:26:00-05:00</updated>
    <id>http://johnb30.github.com/blog/2012/12/07/parallel-data-subsetting</id>
    <content type="html"><![CDATA[<h3>The Challenge</h3>

<p>I've been working with some data that is spread over multiple
tab-delimited files, and is rather large (on the order of 20-30gb). The task has
been to comb through the data, and extract observations (rows) if they match
certain characteristics. Specifically, each file is iterated over and if a field
within a line matches a given value then the line should be extracted and
appended to the final output. This task is relatively straightforward in Python,
but to iterate over all of the files takes around 45 minutes. While this isn't
an exorbiant amount of time, fast is always better. This problem is also
embarassingly parallel; the different files do not need to communicate their
results to each other, the results simply need to be stacked into a final array
in order to be saved to a file. Thus began my saga to implement a parallel
version of a script I wrote to iterate over the files and select the lines.</p>

<h3>The Code</h3>

<p><div><script src='https://gist.github.com/4237347.js?file='></script>
<noscript><pre><code>import numpy as np
from joblib import Parallel, delayed

def subset(file):
    dataOut = []
    data = open(file, 'r')
    data.readline()
    for line in data:
        splitLine = line.split('\t')
        if splitLine[3] == '57':
            dataOut.append(splitLine)
    return dataOut

def stack(list_of_data, hold_data):
    for i in xrange(len(list_of_data)):
        current = np.array(data[i])
        hold = np.vstack((hold_data, current))
    return hold

if __name__ == &quot;__main__&quot;:
    filepath = ['testData1.txt', 'testData2.txt']

    hold  = []
    temp = open(filepath[0], 'r')
    hold.append(temp.readline().split('\t'))
    data = Parallel(n_jobs=-1)(delayed(subset)(x) for x in filepath)
    finalData = stack(data, hold)
</code></pre></noscript></div>
</p>

<p>As with my other posts I'll walk through the code line by line.</p>

<ul>
<li><p>1-2 Just imports. I'll be using the <code>Parallel</code> and
<code>delayed</code> functions from the joblib module.</p></li>
<li><p>4-13 Defining the function to subset out the data. The code is fairly easy to
understand here. The file is opened, the first line is read since this
contains the column names and should not be appended to the result. Then each
line is iterated over, split on the basis of the tabs, and appended if it
meets a certain criteria.</p></li>
<li><p>15-19 Function to stack the data. The joblib <code>Parallel</code> function will return a
list of lists, with each list within the list being the results from the
individual files. The <code>stack</code> function iterates over the list, converts the
inner lists to numpy arrays, and stacks the current data with the previous
data.</p></li>
<li><p>22-28 Running the script. The main focus here is on line 27. The first
argument that <code>Parallel</code> takes is the number of jobs to be used. Setting
<code>n_jobs</code> to -1 says to use all possible cores. The second argument is the
function to be run in parallel. The joblib docs indicate that "The delayed
function is a simple trick to be able to create a tuple
(function, args, kwargs) with a function-call syntax." So delayed is passed
the <code>subset</code> function with the arg <code>x</code>, which represents the file to be opened
as held in <code>filepath</code>. This data is then stored as a list of lists, and
stacked using the <code>stack</code> function.</p></li>
</ul>


<p>Some quick prelimenary examinations shows that this parallel implementation is
much, much faster than running in serial. Running on two files is almost
instananeous, which is a drastic improvement.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[R Magic and Bootstrapped t-test]]></title>
    <link href="http://johnb30.github.com/blog/2012/11/29/r-magic-and-bootstrapped-t-test/"/>
    <updated>2012-11-29T21:53:00-05:00</updated>
    <id>http://johnb30.github.com/blog/2012/11/29/r-magic-and-bootstrapped-t-test</id>
    <content type="html"><![CDATA[<p>Following up on my last post, I wanted a way to test my bootstrapped t-test
function against the regular t-test function in R. While I was able to do this
by copy-pasting between R and a Python shell, this was less than ideal. I then
saw, however, a <a href="http://nbviewer.ipython.org/4166681/">post</a> by Christopher
Fonnesbeck that discussed the use of the rmagic function in ipython, which can
be loaded using the %load_ext magic function. So, with this in mind, I decided
to test it out using a comparison between my bootstrap function and the
<code>t.test</code> function in R. As a note, the rmagic extension requires
<a href="http://rpy.sourceforge.net/rpy2.html">rpy2</a>, so just <code>pip install rpy2</code> and
you should be good to go.</p>

<p>```
import bootFunction</p>

<p>%load_ext rmagic
%R treatment = c(24,25,28,28,28,29,29,31,31,35,35,35)
%R control = c(21,22,24,27,27,28,29,32,32)
%Rpull treatment control</p>

<p>bootFunction.bootstrap_t_test(treatment, control, direction = "greater")</p>

<p>%R print(t.test(treatment, control, alternative = "greater"))
```</p>

<p>I first import the set of functions from the bootFunction. I then load the
rmagic extension using the <code>%load_ext</code> magic function. Using the <code>%R</code> magic
function I then defined two vectors of data, treatment and control, in the R
space. I then used <code>%Rpull</code> to pull the two vectors from the R space into the
Python shell. The two variables become structured numpy arrays.
I then perform the bootstrapped t-test as described in the earlier post.
Finally, using the <code>%R</code> magic function again, I print out the results of the
<code>t.test</code> function in R using the same data. The p-values aren't exactly the
same, as is to be expected, but are at least within the same ballpark (the R
t-test gives .05, while the boostrap function has returned a range between .05
and .03).</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Bootstrapped t-test]]></title>
    <link href="http://johnb30.github.com/blog/2012/11/28/bootstrapping-t-test/"/>
    <updated>2012-11-28T19:47:00-05:00</updated>
    <id>http://johnb30.github.com/blog/2012/11/28/bootstrapping-t-test</id>
    <content type="html"><![CDATA[<p>The below code is to perform a nonparametric two-sample t-test using
bootstrapping. First I present the code, and will then follow up with a
line-by-line description of what's going on.</p>

<p><div><script src='https://gist.github.com/4166016.js?file='></script>
<noscript><pre><code>from __future__ import division
import numpy as np
import pandas as pd
import random

def sample(data):
    sample = [random.choice(data) for _ in xrange(len(data))]
    return sample

def bootstrap_t_test(treatment, control, nboot = 1000, direction = &quot;less&quot;):
    ones = np.vstack((np.ones(len(treatment)),treatment))
    treatment = ones.conj().transpose()
    zeros = np.vstack((np.zeros(len(control)), control))
    control = zeros.conj().transpose()
    Z = np.vstack((treatment, control))
    tstat = np.mean(treatment[:,1])-np.mean(control[:,1])
    tboot = np.zeros(nboot)
    for i in xrange(nboot):
        sboot = sample(Z)
        sboot = pd.DataFrame(np.array(sboot), columns=['treat', 'vals'])
        tboot[i] = np.mean(sboot['vals'][sboot['treat'] == 1]) - np.mean(sboot['vals'][sboot['treat'] == 0]) - tstat
    if direction == &quot;greater&quot;:
        pvalue = np.sum(tboot&gt;=tstat-0)/nboot
    elif direction == &quot;less&quot;:
        pvalue = np.sum(tboot&lt;=tstat-0)/nboot
    else:
        print 'Enter a valid arg for direction'

    print 'The p-value is %f' % (pvalue)
</code></pre></noscript></div>
</p>

<ul>
<li><p>1-4: Just some imports. We need the floating point division from the
future module, numpy, pandas, and the random module.</p></li>
<li><p>6-8: Defining a function <code>sample</code> that samples with replacement from the
dataset, creating a new dataset of the same length as the original data.
This makes use of the <code>random.choice</code> function, which samples one item
randomly from the data. This function is called the same number of times as
there are observations in the data.</p></li>
<li><p>10-17: Defining a function to perform the t-test, with two data inputs,
the number of repititions to be performed, and the direction of the
alternative hypothesis. First a 2 x <em>n</em> matrix is defined, with row 1 being
all ones and row 2 being the data. This is then flipped to create an <em>n</em> x 2
matrix. The same procedure is then repeated for the control data, except with
0s instead of 1s. These two matrices are then stacked on top of each other.
<code>tstat</code> is the difference between the two groups, and tboot is a vector of
zeros with length equal to the number of repititions for the bootstrap.</p></li>
<li><p>18-21: This for-loop actually performs the bootstrap for the number of times
indicated by <code>nboot</code>. First, a sample of the data (<code>Z</code>) is taken using the
<code>sample</code> function defined above. This is then transformed into a pandas
DataFrame, and given appropriate column names. Finally, the difference in
means of the two groups is taken for each iteration of the loop and stored in
the appropriate location in <code>tboot</code>.</p></li>
<li><p>22-28: This is simply calculating a proportion of samples that were greater or
less than the test statistic, based on the direction of the alternative
hypothesis. The final line (29) then prints the p-value as a float.</p></li>
</ul>

]]></content>
  </entry>
  
</feed>
