<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: GDELT, | John Beieler]]></title>
  <link href="http://johnb30.github.com/blog/categories/gdelt-/atom.xml" rel="self"/>
  <link href="http://johnb30.github.com/"/>
  <updated>2013-07-03T16:45:28-05:00</updated>
  <id>http://johnb30.github.com/</id>
  <author>
    <name><![CDATA[John Beieler]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Mapping Protest Data]]></title>
    <link href="http://johnb30.github.com/blog/2013/07/03/mapping-protest-data/"/>
    <updated>2013-07-03T16:03:00-05:00</updated>
    <id>http://johnb30.github.com/blog/2013/07/03/mapping-protest-data</id>
    <content type="html"><![CDATA[<h3>GDELT and Protests</h3>

<p>Given the recent spate of protests around the world, there was some discussion
between <a href="http://dartthrowingchimp.wordpress.com/">Jay Ulfelder</a>,
<a href="https://www.utdallas.edu/~pxb054000/pbrandt/Home.html">Patrick Brandt</a>,
<a href="http://www.kalevleetaru.com/">Kalev Leetaru</a>,
<a href="http://eventdata.psu.edu/">Phil Schrodt</a>, and myself about the possibility of using
<a href="http://gdelt.utdallas.edu/">GDELT</a> to examine some of
the protest activity. Much of this still remains in the discussion stage, but
some data was pulled from GDELT, and I decided to venture into the world of
map making. As a caveat, I've never really worked with geographic visualization
of data, and this is my first cut at this type of work. So, without further
ado, the map is located at <a href="http://cdb.io/14RHla0">http://cdb.io/14RHla0</a>.</p>

<h3>Data</h3>

<p>The data used to create the map contains all CAMEO codes that begin with <code>14</code>,
which is the general category for "protest" events for the year 2013. Including
data from earlier than 2013 made the map much too cluttered.
A potential issue with the use of this CAMEO category is that it picks up governments protesting other governments, politicians protesting
policies, etc. Thus why the U.S. is blank; it was a shining beacon of protest
activity that distracted from the other parts of the map. If anyone is
interested I can put the U.S. data back in and regenerate the map. The data
was grouped by the latitude and longitude coordinates, and a count of protest
events at each location is included. If you zoom in, you are able to see the
individuals points, which when clicked provide information about the location
and number of events.</p>

<p>The main takeaway from this map seems to be that GDELT does a pretty good job
of capturing the broad trends of protest activity; the areas that are "bright"
are those that would generally be expected to be so.</p>

<h3>Note on Tools</h3>

<p>The map was created using the fantastic <a href="http://cartodb.com/">CartoDB</a> along with CSS provided
by <a href="http://www.joshuastevens.net/">Josh Stevens</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Using Hive with Social Science Data]]></title>
    <link href="http://johnb30.github.com/blog/2013/06/16/using-hive-with-social-science-data/"/>
    <updated>2013-06-16T16:05:00-05:00</updated>
    <id>http://johnb30.github.com/blog/2013/06/16/using-hive-with-social-science-data</id>
    <content type="html"><![CDATA[<h3>Processing Data With Hive</h3>

<p>Working with big data is not a particularly easy task. There is a lot of commentary on the web about what constitutes "big"
data. The <a href="http://gdelt.utdallas.edu">GDELT</a> dataset, which is the focus of this post, is over 40 gigabytes uncompressed, so this
is not a discussion of "Google-size" data. It is, however, more than most social scientists are used to dealing with in one
pass. I've attempted to chronicle my history of working with the GDELT dataset to draw interesting conclusions about the world
using event data. I've been relatively successful so far, but I felt that it was possible
to make the data easier to work with. Towards this end, I began to explore SQL and database technologies to use as a subsetting
method. I finally landed on <a href="https://hive.apache.org/">Hive</a>, which is part of the <a href="https://hadoop.apache.org/">Hadoop</a> ecosystem.
Hive allows you to run SQL queries (Hive's language is actually called HiveQL) on top
of the map/reduce framework for computation. The data is distributed (mapped) across multiple nodes in a server cluster and queries
are run atomically on this set of the data. This distributed data is then recombined (reduced) back into a single output form.</p>

<p>Using Hive, it is possible to run fairly complex queries across the entirety of the GDELT dataset in roughly five minutes.
This speed is possible thanks to Amazon's <a href="https://aws.amazon.com/elasticmapreduce/">Elastic MapReduce</a> environment, which makes
use of Elastic Cloud Compute (EC2) resources as the computational backend. EC2 makes it cheap and easy to rent a large cluster of
servers for cheap; as an example, I have used 40 servers at ~$0.10/hr per server. Thus, the combination of Hive and Amazon Web Services makes it
remarkably easy to get up and running with quick queries over this very interesting dataset. The rest of this post shows you how.</p>

<!-- more -->


<h3>Preparing The Data</h3>

<p>Before doing any actual analyses with the data it must be loaded into the Amazon Web Services (AWS) environment. The primary
service for storing data on AWS is the "Simple Storage Solution" (S3). To upload data to S3, first sign in to your AWS account
and head to the AWS Management Console. Once at the console, you should select the S3 services, as seen below.</p>

<p><img src="/blog/downloads/hive_post/1_aws_console.png"></p>

<p>Within the S3 console, you should create a new bucket by clicking the "Create Bucket" button at the top left of the screen.
The bucket name should be globally unique, i.e., it does not appear anywhere else in the S3 system. Your name is likely a good
choice. Once you have selected a bucket name, select the "Create" option rather than "Set Up Logging >". The next step is to
navigate to the created bucket and create a new folder; I chose "gdelt" as the folder name.</p>

<p><img src="/blog/downloads/hive_post/2_s3_setup.png"></p>

<p>You should navigate to this new folder and begin uploading the GDELT data. For the purposes of this tutorial only a couple
years are necessary. I recommend downloading the 1979 and 1980 data and uploading only these years to the <code>&lt;YOURNAME&gt;/gdelt</code>
folder on S3 as a start. As a recap, you should download the desired <a href="http://gdelt.utdallas.edu/data.html">GDELT data</a> and
upload these files to the S3 bucket and folder that you created. The Hive queries used later in this post will load all of
the data within a folder into the table, so make sure that only the GDELT data is contained in the bucket/folder path.</p>

<p>Loading the entire dataset onto S3 is a lengthy process. I have the entire dataset uploaded, along with
continual, daily updates, and I am planning on making this bucket public, but a lot depends on the potential costs.
I will have more to say about this as I find out more information.</p>

<h3>Setting Up Hive</h3>

<p>With the data uploaded to Amazon, the next step is setting up and using Hive. First, return to the AWS Management Console and
select the Elastic MapReduce (EMR) service. Once in the EMR console, select "Create New Job Flow", seen in the
top left-hand corner of the EMR console. Once the job flow dialog appears, you should give the job a name, I chose <code>gdelt_query</code>,
and select the job type, which should be "Hive Program".</p>

<p><img src="/blog/downloads/hive_post/3_define_job.png"></p>

<p>On the next screen, you should simply choose the "Start an Interactive Hive Session" radio button.</p>

<p><img src="/blog/downloads/hive_post/4_parameters.png"></p>

<p>The next screen involves the selection of EC2 instance types of use as the computational power for the map/reduce tasks. For
the purposes of this post, I'll make use of 10 instances of the small server type for the core instance group and another small
instance for the master instance. These instances cost $0.06/hr to run, with Amazon rounding up to the nearest hour for any jobs.
The queries used in this tutorial should take less than an hour, which means that the total cost will come to roughly $0.70.</p>

<p><img src="/blog/downloads/hive_post/5_servers.png"></p>

<p>The only change needed on the next dialog screen is the selection of a key pair that is used to secure shell (SSH) into the master node.
Creating an EC2 Key Pair is beyond the scope of this tutorial, but Amazon has a <a href="http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/generating-a-keypair.html">tutorial</a>
that should provide all of the necessary information; the "Have Amazon EC2 generate it for you" is the
option I suggest. You should make sure to give the pair a distinctive name and download them to a memorable location on your computer.
The most common place to store ssh keys is in a <code>~/.ssh</code> directory. The next dialog box does not have any options that need to be changed;
no bootstrap actions are required for our purposes. The final screen shows a review of the job flow that will be created. Make sure
everything looks good and then click "Create Job Flow." Following this, the job flow should appear in the EMR console.
It will likely take a few minutes for everything to warm up.</p>

<p><img src="/blog/downloads/hive_post/6_overview.png"></p>

<h4>Elastic MapReduce Command-Line Interface</h4>

<p>In order to communicate with the EMR cluster you have created some additional tools are necessary, specifically, the EMR
command line interface. Amazon has a pretty good <a href="http://docs.aws.amazon.com/ElasticMapReduce/latest/DeveloperGuide/emr-cli-install.html">tutorial</a>
on this as well. The only additional comments I will provide are to set the "region" argument in the credentials file to "us-east-1", and that the
"log_uri" should be the bucket we created earlier such and should look like <code>"s3://&lt;YOURNAME&gt;/"</code> if you used your name to create the bucket.</p>

<p>I do know, however, that I had some issues installing the CLI since I have Ruby version 1.9.3 installed on my computer. I do not know if this
is still an issue, but if you have issues with the CLI and have a version of Ruby different than 1.8, a different implementation of the CLI
exists at <a href="https://github.com/tc/elastic-mapreduce-ruby">https://github.com/tc/elastic-mapreduce-ruby</a>. You should download this using the
ZIP download option towards the top of the screen. The installation and usage for this version of the CLI is the same as with the version
compatible with 1.8. The other option is to revert to Ruby 1.8.7.</p>

<p>With the CLI downloaded and set up, you should <code>cd</code> to the directory and issue the <code>./elastic-mapreduce --list</code> command. This should show
you the history of any job flows you created, along with the job IDs. The job ID is the first field for each job flow and looks something
like <code>j-ABC123DEFG45</code>. This ID is used to SSH into the master node for the job, using the
either the <code>./elastic-mapreduce -j j-ABC123DEFG45 --ssh</code> or the  <code>./elastic-mapreduce -ssh j-ABC123DEFG45</code> command, with the latter being
the command to use if you are using the CLI for Ruby 1.8 available from the Github link above.
With this, you should finally have a EMR instance up and running. The next section shows how to use this instance to query the GDELT
data using Hive.</p>

<h3>Querying The Data</h3>

<p>While at the instance prompt, assuming you have already SSH'ed into the instance, you should issue the command <code>hive</code>. As a note, I
will not cover the finer points of SQL in this post. SQL is easy to learn and  numerous tutorials exist that can help you learn. I
suggest Zed Shaw's <a href="sql.learncodethehardway.org">Learn SQL The Hard Way</a>. This section will only cover creating the table that stores
the GDELT data along with some basic queries, along with saving the output to a CSV file.</p>

<p>In order to create the table for GDELT, the following command should be issued in the Hive interpreter. Copy and paste should work fine,
excepting the final line; the last line should be modified to match the path to the S3 bucket to which the data was uploaded, e.g.,
the <code>&lt;YOURNAME&gt;/gdelt</code> format.</p>

<p><code>sql
CREATE EXTERNAL TABLE IF NOT EXISTS gdelt (
 GLOBALEVENTID BIGINT,
 SQLDATE INT,
 MonthYear INT,
 Year INT,
 FractionDate DOUBLE,
 Actor1Code STRING,
 Actor1Name STRING,
 Actor1CountryCode STRING,
 Actor1KnownGroupCode STRING,
 Actor1EthnicCode STRING,
 Actor1Religion1Code STRING,
 Actor1Religion2Code STRING,
 Actor1Type1Code STRING,
 Actor1Type2Code STRING,
 Actor1Type3Code STRING,
 Actor2Code STRING,
 Actor2Name STRING,
 Actor2CountryCode STRING,
 Actor2KnownGroupCode STRING,
 Actor2EthnicCode STRING,
 Actor2Religion1Code STRING,
 Actor2Religion2Code STRING,
 Actor2Type1Code STRING,
 Actor2Type2Code STRING,
 Actor2Type3Code STRING,
 IsRootEvent INT,
 EventCode STRING,
 EventBaseCode STRING,
 EventRootCode STRING,
 QuadClass INT,
 GoldsteinScale DOUBLE,
 NumMentions INT,
 NumSources INT,
 NumArticles INT,
 AvgTone DOUBLE,
 Actor1Geo_Type INT,
 Actor1Geo_FullName STRING,
 Actor1Geo_CountryCode INT,
 Actor1Geo_ADM1Code STRING,
 Actor1Geo_Lat FLOAT,
 Actor1Geo_Long FLOAT,
 Actor1Geo_FeatureID INT,
 Actor2Geo_Type INT,
 Actor2Geo_FullName STRING,
 Actor2Geo_CountryCode STRING,
 Actor2Geo_ADM1Code STRING,
 Actor2Geo_Lat FLOAT,
 Actor2Geo_Long FLOAT,
 Actor2Geo_FeatureID INT,
 ActionGeo_Type INT,
 ActionGeo_FullName STRING,
 ActionGeo_CountryCode STRING,
 ActionGeo_ADM1Code STRING,
 ActionGeo_Lat FLOAT,
 ActionGeo_Long FLOAT,
 ActionGeo_FeatureID INT,
 DATEADDED INT,
 SOURCEURL STRING )
ROW FORMAT DELIMITED
FIELDS TERMINATED BY '\t'
STORED AS TEXTFILE
LOCATION 's3n://&lt;YOURNAME&gt;/gdelt' ;
</code></p>

<p>This creates a table, named <code>gdelt</code>, with fields for each of the columns available in the GDELT dataset. It is now possible to run some simple queries against
the data. Before doing so, however, it is useful to adjust the number of <code>reducers</code> used in the map/reduce jobs since these are automatically
determined from the size of the input; upping the number of <code>reducers</code> can speed the queries. To do this, enter
<code>set hive.exec.reducers.max=200; set mapred.reduce.tasks=200;</code> into the Hive prompt.</p>

<p>The first example query used in this post is the same as in my <a href="http://johnbeieler.org/blog/2013/06/06/using-sql/">previous post</a>:
select all events that occurred within Syria, show the dyadic interactions, and create a sum of the Goldstein values for each dyad.
The data is drawn from the <code>gdelt</code> table created using the previous command.
The code for the query is the exact same as in the previous post:</p>

<p><code>sql
SELECT
Year, Actor1Code, Actor2Code, sum(GoldsteinScale), count(Actor1Code)
FROM
gdelt
WHERE
Actor1CountryCode == 'SYR'
AND
Actor2CountryCode == 'SYR'
GROUP BY
Year, Actor1Code, Actor2Code;
</code></p>

<p>After roughly 10 minutes you should see the results start to stream across the terminal showing the actors in the dyad, the sum of the Goldstein values,
and the count of events within the dyad. While this is interesting and useful, the ultimate goal is to save this subset to a file so it can
serve as the basis for future analysis. To do so, a temporary holding table is necessary as an intermediate step. The following command creates
such a table, named <code>temp</code>.</p>

<p><code>sql
CREATE TABLE temp (
 Actor1Code STRING,
 Actor2Code STRING,
 SumGoldstein FLOAT,
 EventCount INT
 );
</code></p>

<p>You can then issue the same command as above, with the slight modification of inserting the results into the new, <code>temp</code> table.</p>

<p><code>sql
INSERT OVERWRITE TABLE temp
SELECT
Actor1Code, Actor2Code, sum(GoldsteinScale), count(Actor1Code)
FROM
gdelt
WHERE
Actor1CountryCode == 'SYR'
AND
Actor2CountryCode == 'SYR'
GROUP BY
Actor1Code, Actor2Code;
</code></p>

<p>Now, in order to save the results into a single CSV file, issue the following command. This command creates a
table, <code>csvexport</code>, that is stored in your S3 bucket, with fields separated by commas and lines ended by <code>\n</code>
characters. Again, make sure to modify the <code>LOCATION</code> statement to match a path to your S3 bucket. The second
command writes all of the data from <code>temp</code> into the <code>csvexport</code> table.</p>

<p>```sql
CREATE TABLE csvexport (
 Actor1Code STRING,
 Actor2Code STRING,
 SumGoldstein FLOAT,
 EventCount FLOAT
 )
ROW FORMAT DELIMITED FIELDS TERMINATED BY ','
LINES TERMINATED BY '\n'
STORED AS TEXTFILE
LOCATION 's3n://<YOURNAME>/output';</p>

<p>INSERT OVERWRITE TABLE csvexport
SELECT
*
FROM
temp;
```</p>

<p>There should now be a file in your S3 bucket, within the folder <code>output</code>, with a strange name. This file contains the output of the query and can be
downloaded to your local computer. This multi-step process is necessary due to the nature of the map/reduce paradigm. Since the table <code>temp</code> is
created by using multiple reducers, the output would be written to a number of files that reflects the number of reducers used in a query. In the case
of this query, that would be 200 separate files. The creation of the <code>csvexport</code> table allows for the creation of a single text file since no reduce
jobs are needed for the <code>SELECT *</code> operation.</p>

<h3>Wrapping Up</h3>

<p>This tutorial has only scratched the surface of the power of Hive and SQL. It is important to remember that SQL, and by extension
HiveQL, is typically used in business-analytical environments. This means that it is very powerful for the type of quick, informative
queries that are also useful for social science data such as GDELT. Running a few queries to see what type of data you are working with
is very important when working with data on this scale; it is a waste of time to create a subset of the data and run some complicated
analysis, only to realize that you have the wrong subset. Thus, Hive should not be treated just as a tool to quickly subset the data,
but instead it should become another piece of the analytical workflow to aid in determining what the subset of the data should be in
the first place.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Using SQL, pandas, and Python to Work With Data]]></title>
    <link href="http://johnb30.github.com/blog/2013/06/06/using-sql/"/>
    <updated>2013-06-06T13:11:00-05:00</updated>
    <id>http://johnb30.github.com/blog/2013/06/06/using-sql</id>
    <content type="html"><![CDATA[<h3>Easier subsetting of data</h3>

<p>Yes, another post about GDELT. But this one can apply to other datasets, too.</p>

<p>In an earlier <a href="http://johnbeieler.org/blog/2013/04/04/how-do-i-gdelt-subsetting-and-aggregating-the-gdelt-dataset/">post</a>,
I wrote about how to start subsetting the <a href="http://gdelt.utdallas.edu">GDELT data</a>
using Python. Others also wrote <a href="http://nbviewer.ipython.org/urls/raw.github.com/dmasad/GDELT_Intro/master/Getting_Started_with_GDELT.ipynb">similar</a>
<a href="http://quantifyingmemory.blogspot.com/2013/04/mapping-gdelt-data-in-r-and-some.html">pieces</a>. Each of these posts used the
same basic idea: iterate over each line of the dataset, split the line based on tabs,
and select the lines that have fields that match some criteria. This was all well and
good, especially when working with the reduced dataset. The release of the full GDELT
data, however, complicates matters somewhat. Whereas the reduced dataset only has 11
fields of data, the full dataset contains 56 or 57 fields, depending on which set
of the full data is under examination. On top of this, I have noticed that when writing
more complex subsetting scripts it is often easy to lose track of the rules for
selection. These rules are also obfuscated in the Python code for splitting and
selecting. What was field 35 again? Suffice to say that I have become tired of writing
subsetting scripts. A second development is my growing using of SQL resources, including
those such as SQLite and Hive for Hadoop. I have found that these resources make parsing data
<em>much</em> easier, and I will have more to say about these technologies, specifically Hive and
Hadoop, in a later post as some projects I am working on develop further. But, currently,
it is possible to make use of SQL queries while still remaining in the Python ecosystem
and making use of fantastic libraries such as <code>pandas</code>. All while avoiding the actual
setup of a SQL database.</p>

<!-- more -->


<h3><code>pandasql</code></h3>

<p><a href="http://pandas.pydata.org/"><code>pandas</code></a> is, in my opinion, one of the best, and most important,
libraries for data analysis in Python. If you haven't taken a look at it yet, you are really
doing yourself a disfavor. At its core, <code>pandas</code> is a "library providing high-performance,
easy-to-use data structures and data analysis tools for the Python programming language."
One of the key features is <code>R</code>-style dataframes in Python. In addition to <code>pandas</code>, the
individuals at <a href="http://blog.yhathq.com/posts/pandasql-sql-for-pandas-dataframes.html">y-hat</a>
built a SQL interface for <code>pandas</code> dataframes in a library called <code>pandasql</code>. Using <code>pandasql</code>
is rather easy, and for those who have never used SQL before, the syntax is easy to learn
and comprehend.</p>

<p>Installation of <code>pandasql</code> follows the usual method with Python packages:</p>

<p><code>pip install pandasql</code></p>

<p>The example I'll provide in this post makes use of the GDELT daily update for
<a href="http://gdelt.utdallas.edu/data/dailyupdates/20130604.export.CSV.zip">June 4th</a>. The first
step is to take care of the library imports and data loading.</p>

<p>```python</p>

<p>import pandas as pd
from pandasql import sqldf</p>

<p>names = ['GLOBALEVENTID', 'SQLDATE', 'MonthYear', 'Year', 'FractionDate',
'Actor1Code', 'Actor1Name', 'Actor1CountryCode', 'Actor1KnownGroupCode',
'Actor1EthnicCode', 'Actor1Religion1Code', 'Actor1Religion2Code', 'Actor1Type1Code',
'Actor1Type2Code','Actor1Type3Code','Actor2Code','Actor2Name','Actor2CountryCode',
'Actor2KnownGroupCode','Actor2EthnicCode','Actor2Religion1Code','Actor2Religion2Code',
'Actor2Type1Code','Actor2Type2Code','Actor2Type3Code','IsRootEvent','EventCode',
'EventBaseCode','EventRootCode','QuadClass','GoldsteinScale','NumMentions',
'NumSources','NumArticles','AvgTone','Actor1Geo_Type','Actor1Geo_FullName',
'Actor1Geo_CountryCode','Actor1Geo_ADM1Code','Actor1Geo_Lat','Actor1Geo_Long',
'Actor1Geo_FeatureID','Actor2Geo_Type','Actor2Geo_FullName','Actor2Geo_CountryCode',
'Actor2Geo_ADM1Code','Actor2Geo_Lat','Actor2Geo_Long','Actor2Geo_FeatureID',
'ActionGeo_Type','ActionGeo_FullName','ActionGeo_CountryCode','ActionGeo_ADM1Code',
'ActionGeo_Lat','ActionGeo_Long','ActionGeo_FeatureID','DATEADDED','SOURCEURL']</p>

<p>gdelt = pd.read_csv('20130604.export.CSV', sep='\t', header=None, names=names, encoding="utf-8")
```</p>

<p>I found that is necessary to indicate the text encoding when importing the data, else an error is
thrown when trying to use <code>pandasql</code>. The resulting dataframe contains 41,569 events. For this
example, I'm going to take a look at the ongoing crisis in Syria and gather a subset of the
data that reflects this. To do this, I choose only events that have both country codes
as 'SYR'.</p>

<p>```python</p>

<p>q = """
SELECT
*
FROM
gdelt
WHERE
Actor1CountryCode == 'SYR'
AND
Actor2CountryCode == 'SYR';
"""</p>

<p>syr_subset = sqldf(q, globals())
```</p>

<p>This query results in a dataset with 91 events. The query also took roughly 5 seconds to
complete on my laptop. Not too bad. While this subset can now be used to perform some analyses,
it's also possible to perform a simple analysis using SQL queries. For instance, I might wish
to know who the different source actors are, along with the sum of the <code>GoldsteinScale</code>
values for each different source actor. SQL queries make this extremely easy and elegant.</p>

<p>```python</p>

<p>q = """
SELECT
Actor1Code, sum(GoldsteinScale)
FROM
gdelt
WHERE
Actor1CountryCode == 'SYR'
AND
Actor2CountryCode == 'SYR'
GROUP BY
Actor1Code;
"""</p>

<p>sqldf(q, globals())
```</p>

<p>This gives the result of:</p>

<p>```</p>

<pre><code>Actor1Code  sum(GoldsteinScale)
</code></pre>

<p>0        SYR                -74.8
1     SYRCVL                 16.6
2     SYRGOV                -81.1
3  SYRGOVMED                -50.0
4     SYRMIL                -29.5
5  SYROPPUAF                -20.0
6     SYRREB                -24.6
```</p>

<p>  Additionally, we might want to see the same information for dyadic interactions between actors
  along with information regarding how many events occur within the dyad,
  which only requires a small modification of the previous query.</p>

<p>```python</p>

<p>q = """
SELECT
Actor1Code, Actor2Code, sum(GoldsteinScale), count(Actor1Code)
FROM
gdelt
WHERE
Actor1CountryCode == 'SYR'
AND
Actor2CountryCode == 'SYR'
GROUP BY
Actor1Code, Actor2Code;
"""</p>

<p>sqldf(q, globals())</p>

<pre><code>Actor1Code Actor2Code  sum(GoldsteinScale)  count(Actor1Code)
</code></pre>

<p>0         SYR     SYRCVL                 -3.4                  3
1         SYR     SYRGOV                -50.0                 15
2         SYR     SYRLEG                  7.0                  3
3         SYR     SYRMIL                -24.0                  3
4         SYR     SYRREB                 -4.0                  2
5         SYR     SYRREF                 -0.4                  1
6      SYRCVL        SYR                 16.6                  4
7      SYRGOV        SYR                -70.9                 30
8      SYRGOV     SYRGOV                -10.2                 14
9   SYRGOVMED        SYR                -40.0                  4
10  SYRGOVMED     SYRCVL                -10.0                  1
11     SYRMIL        SYR                -29.5                  3
12  SYROPPUAF     SYRGOV                -20.0                  2
13     SYRREB        SYR                -24.6                  6
```</p>

<p>It is also possible to perform more complicated groupings and analyses using only SQL but, as
these examples show, even with a very limited knowledge of SQL one is able to draw subsets of
the data and perform some simple, but interesting, analyses. Additionally, since the data is stored
in a <code>pandas</code> dataframe, any analytical methods that are contained in <code>pandas</code> can be used, which
provides another set of powerful options for analysis. The final benefit of <code>pandasql</code> is that writing
these queries is <em>much</em> faster than writing a subsetting script, and the queries are also
remarkably fast; none of the queries used in this post took more than 10 seconds to run.</p>

<h3>Notes</h3>

<p>While <code>pandasql</code>, and SQL in general, is very useful and far more elegant than hacked together
scripts, there are some limitations. The primary limitation is that this method likely will not
scale well to iterating over the entire GDELT dataset, i.e., all the data in the historical backfile
downloads. The default behavior of <code>sqldf</code> is to hold the data in memory. There is an option to
temporarily write the data to disk, but this still does not address the issue of reading in the
yearly or monthly data into the initial dataframe. There is likely a workaround using the
chunk iterator for <code>pd.read_csv</code>, but in my mind this defeats part of the appeal for using
<code>pandasql</code> in the first place: quickness and ease of use. For working with the entire dataset,
a technology such as Hive, or a full SQL database, is likely a more viable option. I will have
more to say on the use of Hive in a future post. With all that said, however, <code>pandasql</code> is
a very useful tool when analyzing more atomic chunks of the GDELT data; daily, monthly, and perhaps
even a single year for the earlier years is not outside the realm of possibility. Finally, if you
need a resource to learn SQL, I recommend Zed Shaw's <a href="http://sql.learncodethehardway.org/">Learn SQL The Hard Way</a>,
which will provide as much SQL as you need to perform data subsetting tasks.</p>
]]></content>
  </entry>
  
</feed>
