
<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html class="no-js" lang="en"><!--<![endif]-->
<head>
  <meta charset="utf-8">
  <title>John Beieler</title>
  <meta name="author" content="John Beieler">

  
  <meta name="description" content="Processing Data With Hive Working with big data is not a particularly easy task. There is a lot of commentary on the web about what constitutes &# &hellip;">
  

  <!-- http://t.co/dKP3o1e -->
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  
  <link rel="canonical" href="http://johnb30.github.com/">
  <link href="/favicon.png" rel="icon">
  <link href="/stylesheets/screen.css" media="screen, projection" rel="stylesheet" type="text/css">
  <script src="/javascripts/modernizr-2.0.js"></script>
  <script src="/javascripts/ender.js"></script>
  <script src="/javascripts/octopress.js" type="text/javascript"></script>
  <link href="/atom.xml" rel="alternate" title="John Beieler" type="application/atom+xml">
  <!--Fonts from Google"s Web font directory at http://google.com/webfonts -->
<link href="http://fonts.googleapis.com/css?family=PT+Serif:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">
<link href="http://fonts.googleapis.com/css?family=PT+Sans:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">

  
  <script type="text/javascript">
    var _gaq = _gaq || [];
    _gaq.push(['_setAccount', 'UA-40125913-1']);
    _gaq.push(['_trackPageview']);

    (function() {
      var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
      ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
      var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
    })();
  </script>


</head>

<body   >
  <header role="banner"><hgroup>
  <h1><a href="/">John Beieler</a></h1>
  
    <h2>PhD Student in Political Science</h2>
  
</hgroup>

</header>
  <nav role="navigation"><ul class="subscription" data-subscription="rss">
  <li><a href="/atom.xml" rel="subscribe-rss" title="subscribe via RSS">RSS</a></li>
  
</ul>
  
<form action="http://google.com/search" method="get">
  <fieldset role="search">
    <input type="hidden" name="q" value="site:johnb30.github.com" />
    <input class="search" type="text" name="q" results="0" placeholder="Search"/>
  </fieldset>
</form>
  
<ul class="main-navigation">
  <li><a href="/">Blog</a></li>
  <li><a href="/about">About</a></li>
  <li><a href="/cv">CV</a></li>
  <li><a href="/papers">Papers</a></li>
  <li><a href="/code">Code</a></li>
  <li><a href="/blog/archives">Archives</a></li>
</ul>

</nav>
  <div id="main">
    <div id="content">
      <div class="blog-index">
  
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2013/06/16/using-hive-with-social-science-data/">Using Hive With Social Science Data</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2013-06-16T16:05:00-05:00" pubdate data-updated="true">Jun 16<span>th</span>, 2013</time>
        
      </p>
    
  </header>


  <div class="entry-content"><h3>Processing Data With Hive</h3>

<p>Working with big data is not a particularly easy task. There is a lot of commentary on the web about what constitutes &#8220;big&#8221;
data. The <a href="http://gdelt.utdallas.edu">GDELT</a> dataset, which is the focus of this post, is over 40 gigabytes uncompressed, so this
is not a discussion of &#8220;Google-size&#8221; data. It is, however, more than most social scientists are used to dealing with in one
pass. I&#8217;ve attempted to chronicle my history of working with the GDELT dataset to draw interesting conclusions about the world
using event data. I&#8217;ve been relatively successful so far, but I felt that it was possible
to make the data easier to work with. Towards this end, I began to explore SQL and database technologies to use as a subsetting
method. I finally landed on <a href="https://hive.apache.org/">Hive</a>, which is part of the <a href="https://hadoop.apache.org/">Hadoop</a> ecosystem.
Hive allows you to run SQL queries (Hive&#8217;s language is actually called HiveQL) on top
of the map/reduce framework for computation. The data is distributed (mapped) across multiple nodes in a server cluster and queries
are run atomically on this set of the data. This distributed data is then recombined (reduced) back into a single output form.</p>

<p>Using Hive, it is possible to run fairly complex queries across the entirety of the GDELT dataset in roughly five minutes.
This speed is possible thanks to Amazon&#8217;s <a href="https://aws.amazon.com/elasticmapreduce/">Elastic MapReduce</a> environment, which makes
use of Elastic Cloud Compute (EC2) resources as the computational backend. EC2 makes it cheap and easy to rent a large cluster of
servers for cheap; as an example, I have used 40 servers at ~$0.10/hr per server. Thus, the combination of Hive and Amazon Web Services makes it
remarkably easy to get up and running with quick queries over this very interesting dataset. The rest of this post shows you how.</p>

</div>
  
  
    <footer>
      <a rel="full-article" href="/blog/2013/06/16/using-hive-with-social-science-data/">Read on &rarr;</a>
    </footer>
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2013/06/06/using-sql/">Using SQL, Pandas, and Python to Work With Data</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2013-06-06T13:11:00-05:00" pubdate data-updated="true">Jun 6<span>th</span>, 2013</time>
        
      </p>
    
  </header>


  <div class="entry-content"><h3>Easier subsetting of data</h3>

<p>Yes, another post about GDELT. But this one can apply to other datasets, too.</p>

<p>In an earlier <a href="http://johnbeieler.org/blog/2013/04/04/how-do-i-gdelt-subsetting-and-aggregating-the-gdelt-dataset/">post</a>,
I wrote about how to start subsetting the <a href="http://gdelt.utdallas.edu">GDELT data</a>
using Python. Others also wrote <a href="http://nbviewer.ipython.org/urls/raw.github.com/dmasad/GDELT_Intro/master/Getting_Started_with_GDELT.ipynb">similar</a>
<a href="http://quantifyingmemory.blogspot.com/2013/04/mapping-gdelt-data-in-r-and-some.html">pieces</a>. Each of these posts used the
same basic idea: iterate over each line of the dataset, split the line based on tabs,
and select the lines that have fields that match some criteria. This was all well and
good, especially when working with the reduced dataset. The release of the full GDELT
data, however, complicates matters somewhat. Whereas the reduced dataset only has 11
fields of data, the full dataset contains 56 or 57 fields, depending on which set
of the full data is under examination. On top of this, I have noticed that when writing
more complex subsetting scripts it is often easy to lose track of the rules for
selection. These rules are also obfuscated in the Python code for splitting and
selecting. What was field 35 again? Suffice to say that I have become tired of writing
subsetting scripts. A second development is my growing using of SQL resources, including
those such as SQLite and Hive for Hadoop. I have found that these resources make parsing data
<em>much</em> easier, and I will have more to say about these technologies, specifically Hive and
Hadoop, in a later post as some projects I am working on develop further. But, currently,
it is possible to make use of SQL queries while still remaining in the Python ecosystem
and making use of fantastic libraries such as <code>pandas</code>. All while avoiding the actual
setup of a SQL database.</p>

</div>
  
  
    <footer>
      <a rel="full-article" href="/blog/2013/06/06/using-sql/">Read on &rarr;</a>
    </footer>
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2013/06/01/making-gdelt-downloads-easy/">Making GDELT Downloads Easy</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2013-06-01T00:06:00-05:00" pubdate data-updated="true">Jun 1<span>st</span>, 2013</time>
        
      </p>
    
  </header>


  <div class="entry-content"><h2>Downloading GDELT</h2>

<p>Yesterday, I started downloading the GDELT data from the <a href="gdelt.utdallas.edu">website</a>,
having previously pulled the data from the servers at Penn State.
Having to navigate the website and download each file individually
caused me far more frustration that it should have, not due to the
design of the website but due to my own impatience. I&#8217;ll have more
to say about the general state of data distribution in the social
sciences in another post, but for now it&#8217;s enough to say that I&#8217;m
not a fan of downloading data by hand. Because of this, I wrote some
scripts in Python to help with downloading the data. There are two
scripts, <code>download_historical.py</code> and <code>download_daily.py</code> to
download either the historical files or the continuously-updated
daily files. The code is on <a href="https://github.com/johnb30/gdelt_download">github</a>.
I&#8217;ve copied the contents of the README file below so you can determine
if the scripts would be useful for you.</p>

</div>
  
  
    <footer>
      <a rel="full-article" href="/blog/2013/06/01/making-gdelt-downloads-easy/">Read on &rarr;</a>
    </footer>
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2013/04/29/terrorism-studies-and-prediction/">Terrorism Studies and Prediction</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2013-04-29T13:44:00-05:00" pubdate data-updated="true">Apr 29<span>th</span>, 2013</time>
        
      </p>
    
  </header>


  <div class="entry-content"><p>This semester I&#8217;m taking a class on terrorism. Overall I&#8217;ve found the class
very enjoyable; the topic of political violence is one that is always
fascinating. With that said, I found one issue that would repeatedly pop up
in the readings for the seminar. Almost every paper would demarcate some type
of theory, discuss the data used, and run some statistical tests, which is
pretty standard social science research. The issue arose in the &#8220;Discussion&#8221;
or &#8220;Conclusion&#8221; sections. Almost invariably the authors would discuss the
practical implications of their research, which is fine until the dreaded
<strong>prediction</strong> word appears. Then claims about the predictive accuracy of
the models used in the paper would rear their ugly heads. These models were
explicitly <em>not</em> predictive models. This became the soap box that I would
drag out repeatedly throughout the semester. Finally, I decided to put my own
assertions to the test and see how some models performed on out-of-sample
predictive tests.</p>

</div>
  
  
    <footer>
      <a rel="full-article" href="/blog/2013/04/29/terrorism-studies-and-prediction/">Read on &rarr;</a>
    </footer>
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2013/04/23/actor-codes-in-gdelt/">Actor Codes in GDELT</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2013-04-23T18:40:00-05:00" pubdate data-updated="true">Apr 23<span>rd</span>, 2013</time>
        
      </p>
    
  </header>


  <div class="entry-content"><p>As I&#8217;m sure anyone who has begun exploring the GDELT data has noticed, there
are a large number of unique actors coded in the data. While working on a
project for a colleague, I began to wonder exactly how many unique actor
codes exist in the dataset, and what the maximum level of actor code complexity
is within the dataset. For those who don&#8217;t know, the actor codes are created
by chaining together three character CAMEO actor codes, which results in actor
codes that look like <code>USAMIL</code>, which indicates the United States military. The complexity
of the actor code, then, refers to how many of these three character codes are
chained together.</p>

<h4>Results</h4>

<p>Using the Python code available <a href="https://gist.github.com/johnb30/5448269">here</a>, I iterated over the dataset and
identified each unique actor code in the data, along with how many times the
unique code appeared. The resulting data is available
<a href="/blog/downloads/unique_actors.csv">here</a>. The results indicate that there are <strong>22,857</strong>
unique actors in the dataset, with a maximum of <strong>6</strong> three character codes chained
together. The following two plots show the top twenty actor codes overall,
along with the top twenty USA actor codes.</p>

<p><img src="/blog/downloads/top20.png"></p>

<p><img src="/blog/downloads/usa_top20.png"></p>

<p>This analysis shows the incredible complexity that is present in the actor codes,
which I would argue is the most important, fascinating, and challenging part
of working with event data on this scale.</p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2013/04/12/gdelt/">GDELT, Big Data, and Theory</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2013-04-12T14:56:00-05:00" pubdate data-updated="true">Apr 12<span>th</span>, 2013</time>
        
      </p>
    
  </header>


  <div class="entry-content"><p>I made the remark on Twitter that it seemed like GDELT week due to a
<a href="http://ideas.foreignpolicy.com/posts/2013/04/10%20what_can_we_learn_from_the_last_200_million_things_that_happened_in_the_world">Foreign Policy</a>
piece about the dataset, Phil and Kalev&#8217;s <a href="http://eventdata.psu.edu/papers.dir/ISA.2013.GDELT.pdf">paper</a>
for the ISA 2013 meeting, and a <a href="https://dartthrowingchimp.wordpress.com/2013/04/10/the-future-of-political-science-just-showed-up/">host</a>
of <a href="http://badhessian.org/gdelt-and-social-movements/">blog</a>
<a href="https://willopines.wordpress.com/2013/04/11/excitement-about-gdelt-and-some-personal-intellectual-history/">posts</a>
about the data. So, in the spirit of GDELT week, I thought I would throw my hat into
the ring. But instead of taking the approach of lauding the new age that is approaching
for political and social research due to the monstrous scale of the data now available, I thought
I would write a little about the issues that come along with dealing with such massive data.</p>

<h3>Dealing with GDELT</h3>

<p>As someone who has spent the better part of the past 8 months dealing with the GDELT dataset,
including <a href="http://johnbeieler.org/blog/2013/04/04/how-do-i-gdelt-subsetting-and-aggregating-the-gdelt-dataset/">writing</a> a little about
working with the data, I feel that I have a somewhat unique perspective. The long and the
short of my experience is: working with data on this scale is hard. This may strike some
as obvious, especially given the cottage industry that has sprung up around Hadoop and
and other services for processing data. GDELT is 200+ million events spread across several
years. Each year of the <a href="http://eventdata.psu.edu/data.dir/GDELT.html">reduced data</a> is in
a separate file and contains information about many, many different actors. This is part of
what makes the data so intriguing and useful, but the data is also unlike data such as the
ever-popular <a href="http://www.correlatesofwar.org/COW2%20Data/MIDs/MID310.html">MID data</a> in
political science that is easily managed in a program like Stata or <code>R</code>. The data requires
subsetting, massaging, and aggregating; having so much data can, at some points, become
overwhelming. What states do I want to look at? What type of actors? What type of actions?
What about substate actors? Oh, what about the dyadic interactions? These questions and
more quickly come to the fore when dealing with data on this scale. So while the GDELT
data offers an avenue to answer some existing questions, it also brings with it many
potential problems.</p>

<h3>Careful Research</h3>

<p>So, that all sounds kind of depressing. We have this new, cool dataset that could
be tremendously useful, but it also presents many hurdles. What, then, should we
as social science researchers do about it? My answer is careful theorizing and
thinking about the processes under examination. This might be a &#8220;well, duh&#8221;
moment to those in the social sciences, but I think it is worth saying when
there are some heralding <a href="http://www.wired.com/science/discoveries/magazine/16-07/pb_theory">&#8220;The End of Theory&#8221;</a>.
This type of large-scale data does not reduce theory and the scientific
method to irrelevance. Instead, theory is elevated to a position of
higher importance. What states do I want to look at? What type of
actions? Well, what does the theory say? As Hilary Mason noted in
a <a href="https://twitter.com/hmason/status/294930646122504192">tweet</a>:</p>

<blockquote><p>Data tells you whether to use A or B. Science tells you what A and B should be in the first place.</p></blockquote>

<p>Put into more social-scientific language, data tells us the relationship
between A and B, while science tells us what A and B should be and what type
of observations should be used. The data under examination in a given study
should be driven by careful consideration of the processes of interest.
This idea should not, however, be construed as a rejection of &#8220;big data&#8221; in the
social sciences. I personally believe the exact opposite; give me as many features,
measures, and observations as possible and let algorithms sort out what is important.
Instead, I think the social sciences, and science in general, is about asking
interesting questions of the data that will often require more finesse than taking an
&#8220;ANALYZE ALL THE DATA&#8221; approach. Thus, while datasets like GDELT provide new opportunities,
they are not opportunities to relax and let the data do the talking. If anything, big data
generating processes will require more work on the part of the researcher than previous
data sources.</p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2013/04/04/how-do-i-gdelt-subsetting-and-aggregating-the-gdelt-dataset/">How Do I GDELT?: Subsetting and Aggregating the GDELT Dataset</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2013-04-04T19:15:00-05:00" pubdate data-updated="true">Apr 4<span>th</span>, 2013</time>
        
      </p>
    
  </header>


  <div class="entry-content"><h3>GDELT</h3>

<p>Over the past week, the Global Data on Events, Location and Tone (GDELT)
dataset was finally released to the general public. The data is available
at the Penn State event data <a href="http://eventdata.psu.edu/data.dir/GDELT.html">website</a>.
We at Penn State had the
good fortune to have access to this dataset for many months before its public
release. This allowed us to gain some experience working with this massive
collection of data. As a brief background, GDELT is comprised of event
data records spanning 1979 - mid 2012. The events are coded according to the
<a href="http://eventdata.psu.edu/data.dir/cameo.html">CAMEO</a> coding scheme, with the
addition of a &#8220;QuadCategory,&#8221; which separates the events into the material conflict,
material cooperation, verbal conflict, and verbal conflict categories. The data is
spread across 33 different files, each of which is substantially large on its own.
This makes it fairly difficult to work with, and almost guarantees that some
subset of the data is necessary in order to perform analysis. Phil Schrodt has
included some programs with the data to aid in this subsetting, but I thought there
might be some who would prefer to get their hands dirty and write some of their
own code. Given this, I thought I would share some of the knowledge I gained
while working with the GDELT dataset.</p>

<p>For the purposes of this brief introduction, I will work under the assumption
that the desired events are those that originate from the United States, are
directed at some type of state actor, and are either verbal cooperation or
conflict. The following code, written in Python, demonstrates how this subset
might be selected from the GDELT data. The code also assumes the reader
has the <code>pandas</code> and <code>path</code> Python modules installed. Both can be installed using
the normal <code>pip install</code> method. Finally, the complete code is available as
a gist on <a href="https://gist.github.com/johnb30/5316196">github</a>.</p>

<p>Before starting, it is always useful to take a peek at the data. This is as
simple as opening up the terminal and using <code>head 1979.reduced.txt</code>. Doing this
shows the various columns in the reduced data and how they are arranged. We
can see that the date is in the 0 index, with <code>Actor1Code</code> and <code>Actor2Code</code>
in spots 1 and 2, respectively. Additionally, <code>EventCode</code> is located in spot 3,
while the <code>QuadCategory</code> variable is, fittingly, in position 4. These indices
will prove crucial when it comes time to split and subset the data.</p>

</div>
  
  
    <footer>
      <a rel="full-article" href="/blog/2013/04/04/how-do-i-gdelt-subsetting-and-aggregating-the-gdelt-dataset/">Read on &rarr;</a>
    </footer>
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2013/02/08/web-scraping-tutorial/">Web Scraping Tutorial</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2013-02-08T20:39:00-06:00" pubdate data-updated="true">Feb 8<span>th</span>, 2013</time>
        
      </p>
    
  </header>


  <div class="entry-content"><p>I had the opportunity to give a short tutorial on web scraping for the Event
Data class here at Penn State. I used an IPython notebook to give the
presentation and I&#8217;ve put the code in a gist. The link to the IPython notebook
is <a href="http://nbviewer.ipython.org/4743272">http://nbviewer.ipython.org/4743272/</a>.</p>

<p>The PITF project I make reference to is hosted on
<a href="https://github.com/johnb30/atrocitiesProject">github</a>.</p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2012/12/07/parallel-data-subsetting/">Parallel Data Subsetting</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2012-12-07T18:26:00-06:00" pubdate data-updated="true">Dec 7<span>th</span>, 2012</time>
        
      </p>
    
  </header>


  <div class="entry-content"><h3>The Challenge</h3>

<p>I&#8217;ve been working with some data that is spread over multiple
tab-delimited files, and is rather large (on the order of 20-30gb). The task has
been to comb through the data, and extract observations (rows) if they match
certain characteristics. Specifically, each file is iterated over and if a field
within a line matches a given value then the line should be extracted and
appended to the final output. This task is relatively straightforward in Python,
but to iterate over all of the files takes around 45 minutes. While this isn&#8217;t
an exorbiant amount of time, fast is always better. This problem is also
embarassingly parallel; the different files do not need to communicate their
results to each other, the results simply need to be stacked into a final array
in order to be saved to a file. Thus began my saga to implement a parallel
version of a script I wrote to iterate over the files and select the lines.</p>

<h3>The Code</h3>

<div><script src='https://gist.github.com/4237347.js?file='></script>
<noscript><pre><code>import numpy as np
from joblib import Parallel, delayed

def subset(file):
    dataOut = []
    data = open(file, 'r')
    data.readline()
    for line in data:
        splitLine = line.split('\t')
        if splitLine[3] == '57':
            dataOut.append(splitLine)
    return dataOut

def stack(list_of_data, hold_data):
    for i in xrange(len(list_of_data)):
        current = np.array(data[i])
        hold = np.vstack((hold_data, current))
    return hold

if __name__ == &quot;__main__&quot;:
    filepath = ['testData1.txt', 'testData2.txt']

    hold  = []
    temp = open(filepath[0], 'r')
    hold.append(temp.readline().split('\t'))
    data = Parallel(n_jobs=-1)(delayed(subset)(x) for x in filepath)
    finalData = stack(data, hold)
</code></pre></noscript></div>


<p>As with my other posts I&#8217;ll walk through the code line by line.</p>

<ul>
<li><p>1-2 Just imports. I&#8217;ll be using the <code>Parallel</code> and
<code>delayed</code> functions from the joblib module.</p></li>
<li><p>4-13 Defining the function to subset out the data. The code is fairly easy to
understand here. The file is opened, the first line is read since this
contains the column names and should not be appended to the result. Then each
line is iterated over, split on the basis of the tabs, and appended if it
meets a certain criteria.</p></li>
<li><p>15-19 Function to stack the data. The joblib <code>Parallel</code> function will return a
list of lists, with each list within the list being the results from the
individual files. The <code>stack</code> function iterates over the list, converts the
inner lists to numpy arrays, and stacks the current data with the previous
data.</p></li>
<li><p>22-28 Running the script. The main focus here is on line 27. The first
argument that <code>Parallel</code> takes is the number of jobs to be used. Setting
<code>n_jobs</code> to -1 says to use all possible cores. The second argument is the
function to be run in parallel. The joblib docs indicate that &#8220;The delayed
function is a simple trick to be able to create a tuple
(function, args, kwargs) with a function-call syntax.&#8221; So delayed is passed
the <code>subset</code> function with the arg <code>x</code>, which represents the file to be opened
as held in <code>filepath</code>. This data is then stored as a list of lists, and
stacked using the <code>stack</code> function.</p></li>
</ul>


<p>Some quick prelimenary examinations shows that this parallel implementation is
much, much faster than running in serial. Running on two files is almost
instananeous, which is a drastic improvement.</p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2012/11/29/r-magic-and-bootstrapped-t-test/">R Magic and Bootstrapped T-test</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2012-11-29T21:53:00-06:00" pubdate data-updated="true">Nov 29<span>th</span>, 2012</time>
        
      </p>
    
  </header>


  <div class="entry-content"><p>Following up on my last post, I wanted a way to test my bootstrapped t-test
function against the regular t-test function in R. While I was able to do this
by copy-pasting between R and a Python shell, this was less than ideal. I then
saw, however, a <a href="http://nbviewer.ipython.org/4166681/">post</a> by Christopher
Fonnesbeck that discussed the use of the rmagic function in ipython, which can
be loaded using the %load_ext magic function. So, with this in mind, I decided
to test it out using a comparison between my bootstrap function and the
<code>t.test</code> function in R. As a note, the rmagic extension requires
<a href="http://rpy.sourceforge.net/rpy2.html">rpy2</a>, so just <code>pip install rpy2</code> and
you should be good to go.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>import bootFunction
</span><span class='line'>
</span><span class='line'>%load_ext rmagic
</span><span class='line'>%R treatment = c(24,25,28,28,28,29,29,31,31,35,35,35)
</span><span class='line'>%R control = c(21,22,24,27,27,28,29,32,32)
</span><span class='line'>%Rpull treatment control
</span><span class='line'>
</span><span class='line'>bootFunction.bootstrap_t_test(treatment, control, direction = "greater")
</span><span class='line'>
</span><span class='line'>%R print(t.test(treatment, control, alternative = "greater"))</span></code></pre></td></tr></table></div></figure>


<p>I first import the set of functions from the bootFunction. I then load the
rmagic extension using the <code>%load_ext</code> magic function. Using the <code>%R</code> magic
function I then defined two vectors of data, treatment and control, in the R
space. I then used <code>%Rpull</code> to pull the two vectors from the R space into the
Python shell. The two variables become structured numpy arrays.
I then perform the bootstrapped t-test as described in the earlier post.
Finally, using the <code>%R</code> magic function again, I print out the results of the
<code>t.test</code> function in R using the same data. The p-values aren&#8217;t exactly the
same, as is to be expected, but are at least within the same ballpark (the R
t-test gives .05, while the boostrap function has returned a range between .05
and .03).</p>
</div>
  
  


    </article>
  
  <div class="pagination">
    
      <a class="prev" href="/blog/page/2/">&larr; Older</a>
    
    <a href="/blog/archives">Blog Archives</a>
    
  </div>
</div>
<aside class="sidebar">
  
    <section>
  <h1>Recent Posts</h1>
  <ul id="recent_posts">
    
      <li class="post">
        <a href="/blog/2013/06/16/using-hive-with-social-science-data/">Using Hive with Social Science Data</a>
      </li>
    
      <li class="post">
        <a href="/blog/2013/06/06/using-sql/">Using SQL, pandas, and Python to Work With Data</a>
      </li>
    
      <li class="post">
        <a href="/blog/2013/06/01/making-gdelt-downloads-easy/">Making GDELT Downloads Easy</a>
      </li>
    
      <li class="post">
        <a href="/blog/2013/04/29/terrorism-studies-and-prediction/">Terrorism Studies and Prediction</a>
      </li>
    
      <li class="post">
        <a href="/blog/2013/04/23/actor-codes-in-gdelt/">Actor Codes in GDELT</a>
      </li>
    
  </ul>
</section>

<section>
  <h1>GitHub Repos</h1>
  <ul id="gh_repos">
    <li class="loading">Status updating&#8230;</li>
  </ul>
  
  <a href="https://github.com/johnb30">@johnb30</a> on GitHub
  
  <script type="text/javascript">
    $.domReady(function(){
        if (!window.jXHR){
            var jxhr = document.createElement('script');
            jxhr.type = 'text/javascript';
            jxhr.src = '/javascripts/libs/jXHR.js';
            var s = document.getElementsByTagName('script')[0];
            s.parentNode.insertBefore(jxhr, s);
        }

        github.showRepos({
            user: 'johnb30',
            count: 0,
            skip_forks: true,
            target: '#gh_repos'
        });
    });
  </script>
  <script src="/javascripts/github.js" type="text/javascript"> </script>
</section>


<section>
  <h1>Latest Tweets</h1>
  <ul id="tweets">
    <li class="loading">Status updating&#8230;</li>
  </ul>
  <script type="text/javascript">
    $.domReady(function(){
      getTwitterFeed("johnb30", 4, false);
    });
  </script>
  <script src="/javascripts/twitter.js" type="text/javascript"> </script>
  
    <a href="http://twitter.com/johnb30" class="twitter-follow-button" data-show-count="false">Follow @johnb30</a>
  
</section>





  
</aside>

    </div>
  </div>
  <footer role="contentinfo"><p>
  Copyright &copy; 2013 - John Beieler -
  <span class="credit">Powered by <a href="http://octopress.org">Octopress</a></span>
</p>

</footer>
  







  <script type="text/javascript">
    (function(){
      var twitterWidgets = document.createElement('script');
      twitterWidgets.type = 'text/javascript';
      twitterWidgets.async = true;
      twitterWidgets.src = 'http://platform.twitter.com/widgets.js';
      document.getElementsByTagName('head')[0].appendChild(twitterWidgets);
    })();
  </script>





</body>
</html>
