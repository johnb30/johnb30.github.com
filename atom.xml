<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[John Beieler]]></title>
  <link href="http://johnb30.github.com/atom.xml" rel="self"/>
  <link href="http://johnb30.github.com/"/>
  <updated>2013-09-26T15:05:51-04:00</updated>
  <id>http://johnb30.github.com/</id>
  <author>
    <name><![CDATA[John Beieler]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Week of Egyptian Protests]]></title>
    <link href="http://johnb30.github.com/blog/2013/08/19/week-of-egyptian-protests/"/>
    <updated>2013-08-19T11:00:00-04:00</updated>
    <id>http://johnb30.github.com/blog/2013/08/19/week-of-egyptian-protests</id>
    <content type="html"><![CDATA[<h3>Egypt Is Burning</h3>

<p><a href="http://www.kalevleetaru.com/">Kalev Leetaru</a> pointed out to me that the
recent events in Egypt would provide another opportunity to visualize the
<a href="http://gdelt.utdallas/edu">GDELT</a> data and how it tracks the protests.
In addition, GDELT also provides the opportunity to track the government
response to the protests. My <a href="http://johnbeieler.org/protest_mapping">previous</a>
<a href="http://http://johnbeieler.org/blog/2013/07/03/mapping-protest-data/">maps</a>
only examined the presence of protest behavior across the world. For this set
of maps, however, I focus instead on Egypt alone and include various
government responses to the protest activity.</p>

<h3>The Data</h3>

<p>As always, the data is pulled from the GDELT dataset. The data runs from
August 9th until the 17th, which I&#8217;ve used to create two separate maps. With each of
these maps, in the upper right corner you can choose the layers that appear
on the map. When both layers appear on the map, the clickable points show the
information <strong>only</strong> for the protest events. If you disable the protest layer,
you can see information for the additional violence or posture levels. Each
point includes a record of number of events, location name, and the source
URLs used to generate the events. As a note, CartoDB doesn&#8217;t play nicely with
the long string of source URLs, so each URL is only separated by a space and
must be copied and pasted into a browser. Finally, it is important to note
that zooming in on the map is highly suggested; the GDELT dataset provides
a fair amount of city-level detail in the geolocation of events and it is
very interesting to see the spread of protests and government responses.</p>

<p>The first map displays protest behavior, subsetted from GDELT using root CAMEO
codes of <code>14</code>, with an overlay of violent events as red circles. The violent
events are pulled using the root CAMEO code of <code>18</code>, with the additional
stipulation that the target of the action had to be civilian in some way,
as indicated by a <code>CVL</code> label in one of the <code>Actor2Type</code> codes. This generates
the following map:</p>

<iframe width='100%' height='400' frameborder='0' src='http://johnb30.cartodb.com/viz/1c551484-0868-11e3-bae6-3085a9a956e8/embed_map?title=true&description=true&search=false&shareable=false&cartodb_logo=true&layer_selector=true&legends=false&scrollwheel=true&sublayer_options=1%7C1&sql=&sw_lat=23.65349807460049&sw_lon=18.194046020507812&ne_lat=33.09054478515459&ne_lon=46.31904602050781'></iframe>


<p>The second map uses the same protest data, but instead overlays CAMEO root
codes of <code>15</code>, which indicates a change in military or police posture.
This results in</p>

<iframe width='100%' height='400' frameborder='0' src='http://johnb30.cartodb.com/viz/ec0402a2-0869-11e3-86d4-3085a9a956e8/embed_map?title=true&description=true&search=false&shareable=false&cartodb_logo=true&layer_selector=true&legends=true&scrollwheel=true&sublayer_options=1%7C1&sql=&sw_lat=23.80544961231464&sw_lon=19.1162109375&ne_lat=33.22949814144951&ne_lon=47.2412109375'></iframe>


<p>I leave the substantive interpretation up to the reader, but it is worthwhile
to note that the change in military/police posture events are more widespread
than the violent events. As expected, there is also a high concentration of
violent events within Cairo.</p>

<h3>Tools</h3>

<p>As with the previous maps, I made use of the great tools developed by
<a href="http://cartodb.com/">CartoDB</a> and CSS by
<a href="http://www.joshuastevens.net/">Josh Stevens</a>.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Animated Protest Mapping]]></title>
    <link href="http://johnb30.github.com/blog/2013/07/31/animated-protest-mapping/"/>
    <updated>2013-07-31T21:20:00-04:00</updated>
    <id>http://johnb30.github.com/blog/2013/07/31/animated-protest-mapping</id>
    <content type="html"><![CDATA[<p>One of the primary shortcomings of the original
<a href="http://johnb30.cartodb.com/viz/ffd5eeb6-ee20-11e2-8fd0-3085a9a956e8/embed_map?title=false&amp;description=true&amp;search=false&amp;shareable=false&amp;cartodb_logo=true&amp;layer_selector=false&amp;scrollwheel=true&amp;sublayer_options=1&amp;sql=&amp;zoom=2&amp;center_lat=-0.004273915238222043&amp;center_lon=-8.0859375">protest map</a>
I posted was that it only captured a static picture of protest activity over
a 6 month span. While this was interesting in its own right, many people,
including myself, were interested in how protest behavior changes over time.
Given this, I decided to explore creating an animated version of the original
protest map.</p>

<h3>Data</h3>

<p>As I&#8217;ve mentioned before, the <a href="http://gdelt.utdallas.edu">GDELT data</a> covers
from 1979 until present day, with continuous daily updates. I&#8217;m making use of
a subset that runs through June of 2013. The same caveats of the data
that I noted in the
<a href="http://johnbeieler.org/blog/2013/07/03/mapping-protest-data/">previous post</a>
about the protest map still apply. When dealing with the time-series of data,
however, one additional, and very important, point also applies. The number
of events recorded in GDELT grows exponentially over time, as noted in the
<a href="http://gdelt.utdallas.edu/data/documentation/ISA.2013.GDELT.pdf">paper</a>
introducing the dataset. This means that over time there appears to be a
steady increase in events, but this should not be mistaken as a rise in the
actual amount of behavior <code>X</code> (protest behavior in this case). Instead, due
to changes in reporting and the digital recording of news stories, it is
simply the case that there are more events of every type over time. In some
preliminary work that is not yet publicly released, protest behavior seems to
remain relatively constant over time as a <strong>percentage</strong> of the total number
of events. This means that while there was an explosion of protest activity
in the Middle East, and elsewhere, during the past few years, identifying
visible patterns is a tricky endeavor due to the nature of the underlying data.</p>

<p>Finally, the data used to create the map can be viewed
<a href="https://johnb30.cartodb.com/tables/protest_data/">here</a>. In order to reduce
the data to a manageable size, only locations where 10 or more events occurred
within a specific month are included. 10 events is an admittedly arbitrary cutoff,
but given that the highest number of events for one location in a
given month is 3,746 (Cairo in February of 2011) I feel that it is reasonable.</p>

<h3>The Map</h3>

<p>With these notes out of the way, the map can be viewed at
<a href="http://johnbeieler.org/protest_mapping">johnbeieler.org/protest_mapping</a>.
The color of each point is scaled by the number of events, with darker circles
indicating more events.</p>

<h3>Tools, etc.</h3>

<p>Once again this map was made using the wonderful tools provided by
<a href="http://cartodb.com">CartoDB</a>. For the animation specifically, I made use of
the <a href="https://github.com/CartoDB/torque">Torque</a> library provided by CartoDB.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Protest Behavior and Ramadan]]></title>
    <link href="http://johnb30.github.com/blog/2013/07/16/protest-behavior-and-ramadan/"/>
    <updated>2013-07-16T18:54:00-04:00</updated>
    <id>http://johnb30.github.com/blog/2013/07/16/protest-behavior-and-ramadan</id>
    <content type="html"><![CDATA[<h3>Plotting Protests</h3>

<p>There&#8217;s been a fair amount of <a href="http://worldnews.nbcnews.com/_news/2013/07/13/19439120-fasting-makes-us-stronger-cairo-protesters-carry-on-despite-ramadan?lite">discussion</a>
 in the <a href="http://www.dw.de/protests-in-turkey-continue-despite-ramadan/a-16943978">news</a>
<a href="http://world.time.com/2013/07/12/egypts-crisis-ramadan-heat-cools-tensions-in-cairo-for-now/">media</a>
recently about whether the start of Ramadan will lead to a decrease in protest
behavior in locations such as Egypt and Turkey. While I don&#8217;t
have a forecast to offer, I do have data from <a href="http://gdelt.utdallas.edu">GDELT</a>
that tracks protest activity dating back to 1979. With the help of <a href="https://en.wikipedia.org/wiki/File:Ramadan100years1938-2037.png">Wikipedia</a>
I was able to identify the beginning of Ramadan going back to 1979. Using
these dates, it is then possible to plot the time series of protest behavior
with the Ramadan time period layered on top. It is important to note that
this is a rather quick and dirty analysis. For example, in 1981 Ramadan began
on July 3 so I marked July and August as Ramadan months. It shouldn&#8217;t affect
the visualization all that much, but I thought it is important to note. Taken
together, this data produces the following plot (larger version available
<a href="http://johnb30.github.com/blog/downloads/protest_plot.png">here</a>):</p>

<p><img src="http://johnb30.github.com/blog/downloads/protest_plot.png"></p>

<p>A quick eyeball test seems to indicate that Ramadan often coincides with a
slightly lower level of protest activity over time. In order to shore
this up, a quick correlation of &#8220;Ramadan&#8221; with the protest data shows a
coefficient of about <code>-0.025</code>, which indicates a very weak, negative
relationship between Ramadan and protest behavior.</p>

<h3>Caveats, notes, etc.</h3>

<p>With this said, it is important to note a few things about the data. First,
this includes <strong>all</strong> protest activity over time. Events such
as the US protesting the behavior of another state are included along with
civilian protests. This data also includes protest data
from the entire globe; it is quite possible that the effects of Ramadan are
more pronounced in countries with a higher Muslim population.
A final issue is the possible presence of seasonality in the data. Ramadan
just might happen to coincide with time periods that
experience lower protest activity, e.g., the summer months.</p>

<h3>Data</h3>

<p>Since I complained on Twitter that the creators of time-series plots often fail
to upload the data used to create the time series, the
protest data is available <a href="http://johnb30.github.com/blog/downloads/protest_data.csv">here</a>, while the
dates for Ramadan I used are <a href="http://johnb30.github.com/blog/downloads/ramadan_dates.csv">here</a>.
As a final note on the data, the values for the protests are percentages of
total events that occurred in a given month. This controls for
fact that later years, particularly those after 2005 or so, have more events
due to changes in reporting, etc.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Mapping Protest Data]]></title>
    <link href="http://johnb30.github.com/blog/2013/07/03/mapping-protest-data/"/>
    <updated>2013-07-03T16:03:00-04:00</updated>
    <id>http://johnb30.github.com/blog/2013/07/03/mapping-protest-data</id>
    <content type="html"><![CDATA[<p><em>Edit July 17, 2013:</em></p>

<p>Since this map got picked up by the <a href="http://www.guardian.co.uk/news/datablog/interactive/2013/jul/17/global-map-protests-2013">Guardian</a>,
I thought I would clarify some points further for those who aren&#8217;t familiar with the data.</p>

<p>First, the GDELT data is based on news reports
from a variety of sources (a list of sources used can be found <a href="http://gdelt.utdallas.edu/about.html">here</a> under &#8220;Data Sources&#8221;). For
better or for worse, journalistic accounts of events are about the best we can do for large-scale, global projects such as this. Second,
if an event occurs but does not have a specific location within a country, e.g., &#8220;Protestors in Syria…&#8221;, the event is geolocated to the
centroid of the country. This means that there may be some odd events at some locations, and with a high number of events. Third, the
&#8220;Event Count&#8221; featured on the map is the number of protest events that occurred at that location for the entire first half of 2013. This
means that if the &#8220;Event Count&#8221; variable shows 60, then there were 60 unique protest events at that location. This is not a measure of
scale or intensity of a given protest, or even how many times a certain protest was mentioned in the news media, though GDELT does record
this, it is simply a measure of unique events. Next, geolocation is hard, especially on the scale GDELT works on (300+ million events spanning
over 30 years), so some of the points may not be perfect. Even if 10 million events are located in the wrong place, however, that&#8217;s still an error
rate of about 3%. Finally, and this is mentioned in the post further below, GDELT uses the <a href="http://gdelt.utdallas.edu/data/documentation/CAMEO.Manual.1.1b3.pdf">CAMEO</a>
coding scheme to classify events. This means that many different types of protest behavior are recorded, not just the protests or riots that come
to mind when one thinks of Egypt or Turkey. Russia verbally protesting actions of the United States is a protest event. This means that there
are both a higher number of events, and events that occur in locations that a person might not tie to a protest or riot.</p>

<h3>GDELT and Protests</h3>

<p>Given the recent spate of protests around the world, there was some discussion
between <a href="http://dartthrowingchimp.wordpress.com/">Jay Ulfelder</a>,
<a href="https://www.utdallas.edu/~pxb054000/pbrandt/Home.html">Patrick Brandt</a>,
<a href="http://www.kalevleetaru.com/">Kalev Leetaru</a>,
<a href="http://eventdata.psu.edu/">Phil Schrodt</a>, and myself about the possibility of using
<a href="http://gdelt.utdallas.edu/">GDELT</a> to examine some of
the protest activity. Much of this still remains in the discussion stage, but
some data was pulled from GDELT, and I decided to venture into the world of
map making. As a caveat, I&#8217;ve never really worked with geographic visualization
of data, and this is my first cut at this type of work. So, without further
ado, the map is located at <a href="http://cdb.io/14RHla0">http://cdb.io/14RHla0</a>.</p>

<h3>Data</h3>

<p>The data used to create the map contains all CAMEO codes that begin with <code>14</code>,
which is the general category for &#8220;protest&#8221; events, for the year 2013. Including
data from earlier than 2013 made the map much too cluttered.
A potential issue with the use of this CAMEO category is that it picks up governments protesting other governments, politicians protesting
policies, etc. Thus why the U.S. is blank; it was a shining beacon of protest
activity that distracted from the other parts of the map. If anyone is
interested I can put the U.S. data back in and regenerate the map. The data
was grouped by the latitude and longitude coordinates, and a count of protest
events at each location is included. If you zoom in, you are able to see the
individuals points, which when clicked provide information about the location
and number of events.</p>

<p>The main takeaway from this map seems to be that GDELT does a pretty good job
of capturing the broad trends of protest activity; the areas that are &#8220;bright&#8221;
are those that would generally be expected to be so.</p>

<h3>Note on Tools</h3>

<p>The map was created using the fantastic <a href="http://cartodb.com/">CartoDB</a> along with CSS provided
by <a href="http://www.joshuastevens.net/">Josh Stevens</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Using Hive with Social Science Data]]></title>
    <link href="http://johnb30.github.com/blog/2013/06/16/using-hive-with-social-science-data/"/>
    <updated>2013-06-16T16:05:00-04:00</updated>
    <id>http://johnb30.github.com/blog/2013/06/16/using-hive-with-social-science-data</id>
    <content type="html"><![CDATA[<h3>Processing Data With Hive</h3>

<p>Working with big data is not a particularly easy task. There is a lot of commentary on the web about what constitutes &#8220;big&#8221;
data. The <a href="http://gdelt.utdallas.edu">GDELT</a> dataset, which is the focus of this post, is over 40 gigabytes uncompressed, so this
is not a discussion of &#8220;Google-size&#8221; data. It is, however, more than most social scientists are used to dealing with in one
pass. I&#8217;ve attempted to chronicle my history of working with the GDELT dataset to draw interesting conclusions about the world
using event data. I&#8217;ve been relatively successful so far, but I felt that it was possible
to make the data easier to work with. Towards this end, I began to explore SQL and database technologies to use as a subsetting
method. I finally landed on <a href="https://hive.apache.org/">Hive</a>, which is part of the <a href="https://hadoop.apache.org/">Hadoop</a> ecosystem.
Hive allows you to run SQL queries (Hive&#8217;s language is actually called HiveQL) on top
of the map/reduce framework for computation. The data is distributed (mapped) across multiple nodes in a server cluster and queries
are run atomically on this set of the data. This distributed data is then recombined (reduced) back into a single output form.</p>

<p>Using Hive, it is possible to run fairly complex queries across the entirety of the GDELT dataset in roughly five minutes.
This speed is possible thanks to Amazon&#8217;s <a href="https://aws.amazon.com/elasticmapreduce/">Elastic MapReduce</a> environment, which makes
use of Elastic Cloud Compute (EC2) resources as the computational backend. EC2 makes it cheap and easy to rent a large cluster of
servers for cheap; as an example, I have used 40 servers at ~$0.10/hr per server. Thus, the combination of Hive and Amazon Web Services makes it
remarkably easy to get up and running with quick queries over this very interesting dataset. The rest of this post shows you how.</p>

<!-- more -->


<h3>Preparing The Data</h3>

<p>Before doing any actual analyses with the data it must be loaded into the Amazon Web Services (AWS) environment. The primary
service for storing data on AWS is the &#8220;Simple Storage Solution&#8221; (S3). To upload data to S3, first sign in to your AWS account
and head to the AWS Management Console. Once at the console, you should select the S3 services, as seen below.</p>

<p><img src="http://johnb30.github.com/blog/downloads/hive_post/1_aws_console.png"></p>

<p>Within the S3 console, you should create a new bucket by clicking the &#8220;Create Bucket&#8221; button at the top left of the screen.
The bucket name should be globally unique, i.e., it does not appear anywhere else in the S3 system. Your name is likely a good
choice. Once you have selected a bucket name, select the &#8220;Create&#8221; option rather than &#8220;Set Up Logging >&#8221;. The next step is to
navigate to the created bucket and create a new folder; I chose &#8220;gdelt&#8221; as the folder name.</p>

<p><img src="http://johnb30.github.com/blog/downloads/hive_post/2_s3_setup.png"></p>

<p>You should navigate to this new folder and begin uploading the GDELT data. For the purposes of this tutorial only a couple
years are necessary. I recommend downloading the 1979 and 1980 data and uploading only these years to the <code>&lt;YOURNAME&gt;/gdelt</code>
folder on S3 as a start. As a recap, you should download the desired <a href="http://gdelt.utdallas.edu/data.html">GDELT data</a> and
upload these files to the S3 bucket and folder that you created. The Hive queries used later in this post will load all of
the data within a folder into the table, so make sure that only the GDELT data is contained in the bucket/folder path.</p>

<p>Loading the entire dataset onto S3 is a lengthy process. I have the entire dataset uploaded, along with
continual, daily updates, and I am planning on making this bucket public, but a lot depends on the potential costs.
I will have more to say about this as I find out more information.</p>

<h3>Setting Up Hive</h3>

<p>With the data uploaded to Amazon, the next step is setting up and using Hive. First, return to the AWS Management Console and
select the Elastic MapReduce (EMR) service. Once in the EMR console, select &#8220;Create New Job Flow&#8221;, seen in the
top left-hand corner of the EMR console. Once the job flow dialog appears, you should give the job a name, I chose <code>gdelt_query</code>,
and select the job type, which should be &#8220;Hive Program&#8221;.</p>

<p><img src="http://johnb30.github.com/blog/downloads/hive_post/3_define_job.png"></p>

<p>On the next screen, you should simply choose the &#8220;Start an Interactive Hive Session&#8221; radio button.</p>

<p><img src="http://johnb30.github.com/blog/downloads/hive_post/4_parameters.png"></p>

<p>The next screen involves the selection of EC2 instance types of use as the computational power for the map/reduce tasks. For
the purposes of this post, I&#8217;ll make use of 10 instances of the small server type for the core instance group and another small
instance for the master instance. These instances cost $0.06/hr to run, with Amazon rounding up to the nearest hour for any jobs.
The queries used in this tutorial should take less than an hour, which means that the total cost will come to roughly $0.70.</p>

<p><img src="http://johnb30.github.com/blog/downloads/hive_post/5_servers.png"></p>

<p>The only change needed on the next dialog screen is the selection of a key pair that is used to secure shell (SSH) into the master node.
Creating an EC2 Key Pair is beyond the scope of this tutorial, but Amazon has a <a href="http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/generating-a-keypair.html">tutorial</a>
that should provide all of the necessary information; the &#8220;Have Amazon EC2 generate it for you&#8221; is the
option I suggest. You should make sure to give the pair a distinctive name and download them to a memorable location on your computer.
The most common place to store ssh keys is in a <code>~/.ssh</code> directory. The next dialog box does not have any options that need to be changed;
no bootstrap actions are required for our purposes. The final screen shows a review of the job flow that will be created. Make sure
everything looks good and then click &#8220;Create Job Flow.&#8221; Following this, the job flow should appear in the EMR console.
It will likely take a few minutes for everything to warm up.</p>

<p><img src="http://johnb30.github.com/blog/downloads/hive_post/6_overview.png"></p>

<h4>Elastic MapReduce Command-Line Interface</h4>

<p>In order to communicate with the EMR cluster you have created some additional tools are necessary, specifically, the EMR
command line interface. Amazon has a pretty good <a href="http://docs.aws.amazon.com/ElasticMapReduce/latest/DeveloperGuide/emr-cli-install.html">tutorial</a>
on this as well. The only additional comments I will provide are to set the &#8220;region&#8221; argument in the credentials file to &#8220;us-east-1&#8221;, and that the
&#8220;log_uri&#8221; should be the bucket we created earlier such and should look like <code>"s3://&lt;YOURNAME&gt;/"</code> if you used your name to create the bucket.</p>

<p>I do know, however, that I had some issues installing the CLI since I have Ruby version 1.9.3 installed on my computer. I do not know if this
is still an issue, but if you have issues with the CLI and have a version of Ruby different than 1.8, a different implementation of the CLI
exists at <a href="https://github.com/tc/elastic-mapreduce-ruby">https://github.com/tc/elastic-mapreduce-ruby</a>. You should download this using the
ZIP download option towards the top of the screen. The installation and usage for this version of the CLI is the same as with the version
compatible with 1.8. The other option is to revert to Ruby 1.8.7.</p>

<p>With the CLI downloaded and set up, you should <code>cd</code> to the directory and issue the <code>./elastic-mapreduce --list</code> command. This should show
you the history of any job flows you created, along with the job IDs. The job ID is the first field for each job flow and looks something
like <code>j-ABC123DEFG45</code>. This ID is used to SSH into the master node for the job, using the
either the <code>./elastic-mapreduce -j j-ABC123DEFG45 --ssh</code> or the  <code>./elastic-mapreduce -ssh j-ABC123DEFG45</code> command, with the latter being
the command to use if you are using the CLI for Ruby 1.8 available from the Github link above.
With this, you should finally have a EMR instance up and running. The next section shows how to use this instance to query the GDELT
data using Hive.</p>

<h3>Querying The Data</h3>

<p>While at the instance prompt, assuming you have already SSH&#8217;ed into the instance, you should issue the command <code>hive</code>. As a note, I
will not cover the finer points of SQL in this post. SQL is easy to learn and  numerous tutorials exist that can help you learn. I
suggest Zed Shaw&#8217;s <a href="sql.learncodethehardway.org">Learn SQL The Hard Way</a>. This section will only cover creating the table that stores
the GDELT data along with some basic queries, along with saving the output to a CSV file.</p>

<p>In order to create the table for GDELT, the following command should be issued in the Hive interpreter. Copy and paste should work fine,
excepting the final line; the last line should be modified to match the path to the S3 bucket to which the data was uploaded, e.g.,
the <code>&lt;YOURNAME&gt;/gdelt</code> format.</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
<span class='line-number'>41</span>
<span class='line-number'>42</span>
<span class='line-number'>43</span>
<span class='line-number'>44</span>
<span class='line-number'>45</span>
<span class='line-number'>46</span>
<span class='line-number'>47</span>
<span class='line-number'>48</span>
<span class='line-number'>49</span>
<span class='line-number'>50</span>
<span class='line-number'>51</span>
<span class='line-number'>52</span>
<span class='line-number'>53</span>
<span class='line-number'>54</span>
<span class='line-number'>55</span>
<span class='line-number'>56</span>
<span class='line-number'>57</span>
<span class='line-number'>58</span>
<span class='line-number'>59</span>
<span class='line-number'>60</span>
<span class='line-number'>61</span>
<span class='line-number'>62</span>
<span class='line-number'>63</span>
</pre></td><td class='code'><pre><code class='sql'><span class='line'><span class="k">CREATE</span> <span class="k">EXTERNAL</span> <span class="k">TABLE</span> <span class="n">IF</span> <span class="k">NOT</span> <span class="k">EXISTS</span> <span class="n">gdelt</span> <span class="p">(</span>
</span><span class='line'> <span class="n">GLOBALEVENTID</span> <span class="nb">BIGINT</span><span class="p">,</span>
</span><span class='line'> <span class="n">SQLDATE</span> <span class="nb">INT</span><span class="p">,</span>
</span><span class='line'> <span class="n">MonthYear</span> <span class="nb">INT</span><span class="p">,</span>
</span><span class='line'> <span class="k">Year</span> <span class="nb">INT</span><span class="p">,</span>
</span><span class='line'> <span class="n">FractionDate</span> <span class="n">DOUBLE</span><span class="p">,</span>
</span><span class='line'> <span class="n">Actor1Code</span> <span class="n">STRING</span><span class="p">,</span>
</span><span class='line'> <span class="n">Actor1Name</span> <span class="n">STRING</span><span class="p">,</span>
</span><span class='line'> <span class="n">Actor1CountryCode</span> <span class="n">STRING</span><span class="p">,</span>
</span><span class='line'> <span class="n">Actor1KnownGroupCode</span> <span class="n">STRING</span><span class="p">,</span>
</span><span class='line'> <span class="n">Actor1EthnicCode</span> <span class="n">STRING</span><span class="p">,</span>
</span><span class='line'> <span class="n">Actor1Religion1Code</span> <span class="n">STRING</span><span class="p">,</span>
</span><span class='line'> <span class="n">Actor1Religion2Code</span> <span class="n">STRING</span><span class="p">,</span>
</span><span class='line'> <span class="n">Actor1Type1Code</span> <span class="n">STRING</span><span class="p">,</span>
</span><span class='line'> <span class="n">Actor1Type2Code</span> <span class="n">STRING</span><span class="p">,</span>
</span><span class='line'> <span class="n">Actor1Type3Code</span> <span class="n">STRING</span><span class="p">,</span>
</span><span class='line'> <span class="n">Actor2Code</span> <span class="n">STRING</span><span class="p">,</span>
</span><span class='line'> <span class="n">Actor2Name</span> <span class="n">STRING</span><span class="p">,</span>
</span><span class='line'> <span class="n">Actor2CountryCode</span> <span class="n">STRING</span><span class="p">,</span>
</span><span class='line'> <span class="n">Actor2KnownGroupCode</span> <span class="n">STRING</span><span class="p">,</span>
</span><span class='line'> <span class="n">Actor2EthnicCode</span> <span class="n">STRING</span><span class="p">,</span>
</span><span class='line'> <span class="n">Actor2Religion1Code</span> <span class="n">STRING</span><span class="p">,</span>
</span><span class='line'> <span class="n">Actor2Religion2Code</span> <span class="n">STRING</span><span class="p">,</span>
</span><span class='line'> <span class="n">Actor2Type1Code</span> <span class="n">STRING</span><span class="p">,</span>
</span><span class='line'> <span class="n">Actor2Type2Code</span> <span class="n">STRING</span><span class="p">,</span>
</span><span class='line'> <span class="n">Actor2Type3Code</span> <span class="n">STRING</span><span class="p">,</span>
</span><span class='line'> <span class="n">IsRootEvent</span> <span class="nb">INT</span><span class="p">,</span>
</span><span class='line'> <span class="n">EventCode</span> <span class="n">STRING</span><span class="p">,</span>
</span><span class='line'> <span class="n">EventBaseCode</span> <span class="n">STRING</span><span class="p">,</span>
</span><span class='line'> <span class="n">EventRootCode</span> <span class="n">STRING</span><span class="p">,</span>
</span><span class='line'> <span class="n">QuadClass</span> <span class="nb">INT</span><span class="p">,</span>
</span><span class='line'> <span class="n">GoldsteinScale</span> <span class="n">DOUBLE</span><span class="p">,</span>
</span><span class='line'> <span class="n">NumMentions</span> <span class="nb">INT</span><span class="p">,</span>
</span><span class='line'> <span class="n">NumSources</span> <span class="nb">INT</span><span class="p">,</span>
</span><span class='line'> <span class="n">NumArticles</span> <span class="nb">INT</span><span class="p">,</span>
</span><span class='line'> <span class="n">AvgTone</span> <span class="n">DOUBLE</span><span class="p">,</span>
</span><span class='line'> <span class="n">Actor1Geo_Type</span> <span class="nb">INT</span><span class="p">,</span>
</span><span class='line'> <span class="n">Actor1Geo_FullName</span> <span class="n">STRING</span><span class="p">,</span>
</span><span class='line'> <span class="n">Actor1Geo_CountryCode</span> <span class="n">STRING</span><span class="p">,</span>
</span><span class='line'> <span class="n">Actor1Geo_ADM1Code</span> <span class="n">STRING</span><span class="p">,</span>
</span><span class='line'> <span class="n">Actor1Geo_Lat</span> <span class="nb">FLOAT</span><span class="p">,</span>
</span><span class='line'> <span class="n">Actor1Geo_Long</span> <span class="nb">FLOAT</span><span class="p">,</span>
</span><span class='line'> <span class="n">Actor1Geo_FeatureID</span> <span class="nb">INT</span><span class="p">,</span>
</span><span class='line'> <span class="n">Actor2Geo_Type</span> <span class="nb">INT</span><span class="p">,</span>
</span><span class='line'> <span class="n">Actor2Geo_FullName</span> <span class="n">STRING</span><span class="p">,</span>
</span><span class='line'> <span class="n">Actor2Geo_CountryCode</span> <span class="n">STRING</span><span class="p">,</span>
</span><span class='line'> <span class="n">Actor2Geo_ADM1Code</span> <span class="n">STRING</span><span class="p">,</span>
</span><span class='line'> <span class="n">Actor2Geo_Lat</span> <span class="nb">FLOAT</span><span class="p">,</span>
</span><span class='line'> <span class="n">Actor2Geo_Long</span> <span class="nb">FLOAT</span><span class="p">,</span>
</span><span class='line'> <span class="n">Actor2Geo_FeatureID</span> <span class="nb">INT</span><span class="p">,</span>
</span><span class='line'> <span class="n">ActionGeo_Type</span> <span class="nb">INT</span><span class="p">,</span>
</span><span class='line'> <span class="n">ActionGeo_FullName</span> <span class="n">STRING</span><span class="p">,</span>
</span><span class='line'> <span class="n">ActionGeo_CountryCode</span> <span class="n">STRING</span><span class="p">,</span>
</span><span class='line'> <span class="n">ActionGeo_ADM1Code</span> <span class="n">STRING</span><span class="p">,</span>
</span><span class='line'> <span class="n">ActionGeo_Lat</span> <span class="nb">FLOAT</span><span class="p">,</span>
</span><span class='line'> <span class="n">ActionGeo_Long</span> <span class="nb">FLOAT</span><span class="p">,</span>
</span><span class='line'> <span class="n">ActionGeo_FeatureID</span> <span class="nb">INT</span><span class="p">,</span>
</span><span class='line'> <span class="n">DATEADDED</span> <span class="nb">INT</span><span class="p">,</span>
</span><span class='line'> <span class="n">SOURCEURL</span> <span class="n">STRING</span> <span class="p">)</span>
</span><span class='line'><span class="k">ROW</span> <span class="n">FORMAT</span> <span class="n">DELIMITED</span>
</span><span class='line'><span class="n">FIELDS</span> <span class="n">TERMINATED</span> <span class="k">BY</span> <span class="s1">&#39;\t&#39;</span>
</span><span class='line'><span class="n">STORED</span> <span class="k">AS</span> <span class="n">TEXTFILE</span>
</span><span class='line'><span class="k">LOCATION</span> <span class="s1">&#39;s3n://&lt;YOURNAME&gt;/gdelt&#39;</span> <span class="p">;</span>
</span></code></pre></td></tr></table></div></figure>


<p>This creates a table, named <code>gdelt</code>, with fields for each of the columns available in the GDELT dataset. It is now possible to run some simple queries against
the data. Before doing so, however, it is useful to adjust the number of <code>reducers</code> used in the map/reduce jobs since these are automatically
determined from the size of the input; upping the number of <code>reducers</code> can speed the queries. To do this, enter
<code>set hive.exec.reducers.max=200; set mapred.reduce.tasks=200;</code> into the Hive prompt.</p>

<p>The first example query used in this post is the same as in my <a href="http://johnbeieler.org/blog/2013/06/06/using-sql/">previous post</a>:
select all events that occurred within Syria, show the dyadic interactions, and create a sum of the Goldstein values for each dyad.
The data is drawn from the <code>gdelt</code> table created using the previous command.
The code for the query is the exact same as in the previous post:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
</pre></td><td class='code'><pre><code class='sql'><span class='line'><span class="k">SELECT</span>
</span><span class='line'><span class="k">Year</span><span class="p">,</span> <span class="n">Actor1Code</span><span class="p">,</span> <span class="n">Actor2Code</span><span class="p">,</span> <span class="k">sum</span><span class="p">(</span><span class="n">GoldsteinScale</span><span class="p">),</span> <span class="k">count</span><span class="p">(</span><span class="n">Actor1Code</span><span class="p">)</span>
</span><span class='line'><span class="k">FROM</span>
</span><span class='line'><span class="n">gdelt</span>
</span><span class='line'><span class="k">WHERE</span>
</span><span class='line'><span class="n">Actor1CountryCode</span> <span class="o">==</span> <span class="s1">&#39;SYR&#39;</span>
</span><span class='line'><span class="k">AND</span>
</span><span class='line'><span class="n">Actor2CountryCode</span> <span class="o">==</span> <span class="s1">&#39;SYR&#39;</span>
</span><span class='line'><span class="k">GROUP</span> <span class="k">BY</span>
</span><span class='line'><span class="k">Year</span><span class="p">,</span> <span class="n">Actor1Code</span><span class="p">,</span> <span class="n">Actor2Code</span><span class="p">;</span>
</span></code></pre></td></tr></table></div></figure>


<p>After roughly 10 minutes you should see the results start to stream across the terminal showing the actors in the dyad, the sum of the Goldstein values,
and the count of events within the dyad. While this is interesting and useful, the ultimate goal is to save this subset to a file so it can
serve as the basis for future analysis. To do so, a temporary holding table is necessary as an intermediate step. The following command creates
such a table, named <code>temp</code>.</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
</pre></td><td class='code'><pre><code class='sql'><span class='line'><span class="k">CREATE</span> <span class="k">TABLE</span> <span class="n">temp</span> <span class="p">(</span>
</span><span class='line'> <span class="n">Actor1Code</span> <span class="n">STRING</span><span class="p">,</span>
</span><span class='line'> <span class="n">Actor2Code</span> <span class="n">STRING</span><span class="p">,</span>
</span><span class='line'> <span class="n">SumGoldstein</span> <span class="nb">FLOAT</span><span class="p">,</span>
</span><span class='line'> <span class="n">EventCount</span> <span class="nb">INT</span>
</span><span class='line'> <span class="p">);</span>
</span></code></pre></td></tr></table></div></figure>


<p>You can then issue the same command as above, with the slight modification of inserting the results into the new, <code>temp</code> table.</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
</pre></td><td class='code'><pre><code class='sql'><span class='line'><span class="k">INSERT</span> <span class="n">OVERWRITE</span> <span class="k">TABLE</span> <span class="n">temp</span>
</span><span class='line'><span class="k">SELECT</span>
</span><span class='line'><span class="n">Actor1Code</span><span class="p">,</span> <span class="n">Actor2Code</span><span class="p">,</span> <span class="k">sum</span><span class="p">(</span><span class="n">GoldsteinScale</span><span class="p">),</span> <span class="k">count</span><span class="p">(</span><span class="n">Actor1Code</span><span class="p">)</span>
</span><span class='line'><span class="k">FROM</span>
</span><span class='line'><span class="n">gdelt</span>
</span><span class='line'><span class="k">WHERE</span>
</span><span class='line'><span class="n">Actor1CountryCode</span> <span class="o">==</span> <span class="s1">&#39;SYR&#39;</span>
</span><span class='line'><span class="k">AND</span>
</span><span class='line'><span class="n">Actor2CountryCode</span> <span class="o">==</span> <span class="s1">&#39;SYR&#39;</span>
</span><span class='line'><span class="k">GROUP</span> <span class="k">BY</span>
</span><span class='line'><span class="n">Actor1Code</span><span class="p">,</span> <span class="n">Actor2Code</span><span class="p">;</span>
</span></code></pre></td></tr></table></div></figure>


<p>Now, in order to save the results into a single CSV file, issue the following command. This command creates a
table, <code>csvexport</code>, that is stored in your S3 bucket, with fields separated by commas and lines ended by <code>\n</code>
characters. Again, make sure to modify the <code>LOCATION</code> statement to match a path to your S3 bucket. The second
command writes all of the data from <code>temp</code> into the <code>csvexport</code> table.</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
</pre></td><td class='code'><pre><code class='sql'><span class='line'><span class="k">CREATE</span> <span class="k">TABLE</span> <span class="n">csvexport</span> <span class="p">(</span>
</span><span class='line'> <span class="n">Actor1Code</span> <span class="n">STRING</span><span class="p">,</span>
</span><span class='line'> <span class="n">Actor2Code</span> <span class="n">STRING</span><span class="p">,</span>
</span><span class='line'> <span class="n">SumGoldstein</span> <span class="nb">FLOAT</span><span class="p">,</span>
</span><span class='line'> <span class="n">EventCount</span> <span class="nb">FLOAT</span>
</span><span class='line'> <span class="p">)</span>
</span><span class='line'><span class="k">ROW</span> <span class="n">FORMAT</span> <span class="n">DELIMITED</span> <span class="n">FIELDS</span> <span class="n">TERMINATED</span> <span class="k">BY</span> <span class="s1">&#39;,&#39;</span>
</span><span class='line'><span class="n">LINES</span> <span class="n">TERMINATED</span> <span class="k">BY</span> <span class="s1">&#39;\n&#39;</span>
</span><span class='line'><span class="n">STORED</span> <span class="k">AS</span> <span class="n">TEXTFILE</span>
</span><span class='line'><span class="k">LOCATION</span> <span class="s1">&#39;s3n://&lt;YOURNAME&gt;/output&#39;</span><span class="p">;</span>
</span><span class='line'>
</span><span class='line'><span class="k">INSERT</span> <span class="n">OVERWRITE</span> <span class="k">TABLE</span> <span class="n">csvexport</span>
</span><span class='line'><span class="k">SELECT</span>
</span><span class='line'><span class="o">*</span>
</span><span class='line'><span class="k">FROM</span>
</span><span class='line'><span class="n">temp</span><span class="p">;</span>
</span></code></pre></td></tr></table></div></figure>


<p>There should now be a file in your S3 bucket, within the folder <code>output</code>, with a strange name. This file contains the output of the query and can be
downloaded to your local computer. This multi-step process is necessary due to the nature of the map/reduce paradigm. Since the table <code>temp</code> is
created by using multiple reducers, the output would be written to a number of files that reflects the number of reducers used in a query. In the case
of this query, that would be 200 separate files. The creation of the <code>csvexport</code> table allows for the creation of a single text file since no reduce
jobs are needed for the <code>SELECT *</code> operation.</p>

<h3>Wrapping Up</h3>

<p>This tutorial has only scratched the surface of the power of Hive and SQL. It is important to remember that SQL, and by extension
HiveQL, is typically used in business-analytical environments. This means that it is very powerful for the type of quick, informative
queries that are also useful for social science data such as GDELT. Running a few queries to see what type of data you are working with
is very important when working with data on this scale; it is a waste of time to create a subset of the data and run some complicated
analysis, only to realize that you have the wrong subset. Thus, Hive should not be treated just as a tool to quickly subset the data,
but instead it should become another piece of the analytical workflow to aid in determining what the subset of the data should be in
the first place.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Using SQL, pandas, and Python to Work With Data]]></title>
    <link href="http://johnb30.github.com/blog/2013/06/06/using-sql/"/>
    <updated>2013-06-06T13:11:00-04:00</updated>
    <id>http://johnb30.github.com/blog/2013/06/06/using-sql</id>
    <content type="html"><![CDATA[<h3>Easier subsetting of data</h3>

<p>Yes, another post about GDELT. But this one can apply to other datasets, too.</p>

<p>In an earlier <a href="http://johnbeieler.org/blog/2013/04/04/how-do-i-gdelt-subsetting-and-aggregating-the-gdelt-dataset/">post</a>,
I wrote about how to start subsetting the <a href="http://gdelt.utdallas.edu">GDELT data</a>
using Python. Others also wrote <a href="http://nbviewer.ipython.org/urls/raw.github.com/dmasad/GDELT_Intro/master/Getting_Started_with_GDELT.ipynb">similar</a>
<a href="http://quantifyingmemory.blogspot.com/2013/04/mapping-gdelt-data-in-r-and-some.html">pieces</a>. Each of these posts used the
same basic idea: iterate over each line of the dataset, split the line based on tabs,
and select the lines that have fields that match some criteria. This was all well and
good, especially when working with the reduced dataset. The release of the full GDELT
data, however, complicates matters somewhat. Whereas the reduced dataset only has 11
fields of data, the full dataset contains 56 or 57 fields, depending on which set
of the full data is under examination. On top of this, I have noticed that when writing
more complex subsetting scripts it is often easy to lose track of the rules for
selection. These rules are also obfuscated in the Python code for splitting and
selecting. What was field 35 again? Suffice to say that I have become tired of writing
subsetting scripts. A second development is my growing using of SQL resources, including
those such as SQLite and Hive for Hadoop. I have found that these resources make parsing data
<em>much</em> easier, and I will have more to say about these technologies, specifically Hive and
Hadoop, in a later post as some projects I am working on develop further. But, currently,
it is possible to make use of SQL queries while still remaining in the Python ecosystem
and making use of fantastic libraries such as <code>pandas</code>. All while avoiding the actual
setup of a SQL database.</p>

<!-- more -->


<h3><code>pandasql</code></h3>

<p><a href="http://pandas.pydata.org/"><code>pandas</code></a> is, in my opinion, one of the best, and most important,
libraries for data analysis in Python. If you haven&#8217;t taken a look at it yet, you are really
doing yourself a disfavor. At its core, <code>pandas</code> is a &#8220;library providing high-performance,
easy-to-use data structures and data analysis tools for the Python programming language.&#8221;
One of the key features is <code>R</code>-style dataframes in Python. In addition to <code>pandas</code>, the
individuals at <a href="http://blog.yhathq.com/posts/pandasql-sql-for-pandas-dataframes.html">y-hat</a>
built a SQL interface for <code>pandas</code> dataframes in a library called <code>pandasql</code>. Using <code>pandasql</code>
is rather easy, and for those who have never used SQL before, the syntax is easy to learn
and comprehend.</p>

<p>Installation of <code>pandasql</code> follows the usual method with Python packages:</p>

<p><code>pip install pandasql</code></p>

<p>The example I&#8217;ll provide in this post makes use of the GDELT daily update for
<a href="http://gdelt.utdallas.edu/data/dailyupdates/20130604.export.CSV.zip">June 4th</a>. The first
step is to take care of the library imports and data loading.</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
</pre></td><td class='code'><pre><code class='python'><span class='line'><span class="kn">import</span> <span class="nn">pandas</span> <span class="kn">as</span> <span class="nn">pd</span>
</span><span class='line'><span class="kn">from</span> <span class="nn">pandasql</span> <span class="kn">import</span> <span class="n">sqldf</span>
</span><span class='line'>
</span><span class='line'><span class="n">names</span> <span class="o">=</span> <span class="p">[</span><span class="s">&#39;GLOBALEVENTID&#39;</span><span class="p">,</span> <span class="s">&#39;SQLDATE&#39;</span><span class="p">,</span> <span class="s">&#39;MonthYear&#39;</span><span class="p">,</span> <span class="s">&#39;Year&#39;</span><span class="p">,</span> <span class="s">&#39;FractionDate&#39;</span><span class="p">,</span>
</span><span class='line'><span class="s">&#39;Actor1Code&#39;</span><span class="p">,</span> <span class="s">&#39;Actor1Name&#39;</span><span class="p">,</span> <span class="s">&#39;Actor1CountryCode&#39;</span><span class="p">,</span> <span class="s">&#39;Actor1KnownGroupCode&#39;</span><span class="p">,</span>
</span><span class='line'><span class="s">&#39;Actor1EthnicCode&#39;</span><span class="p">,</span> <span class="s">&#39;Actor1Religion1Code&#39;</span><span class="p">,</span> <span class="s">&#39;Actor1Religion2Code&#39;</span><span class="p">,</span> <span class="s">&#39;Actor1Type1Code&#39;</span><span class="p">,</span>
</span><span class='line'><span class="s">&#39;Actor1Type2Code&#39;</span><span class="p">,</span><span class="s">&#39;Actor1Type3Code&#39;</span><span class="p">,</span><span class="s">&#39;Actor2Code&#39;</span><span class="p">,</span><span class="s">&#39;Actor2Name&#39;</span><span class="p">,</span><span class="s">&#39;Actor2CountryCode&#39;</span><span class="p">,</span>
</span><span class='line'><span class="s">&#39;Actor2KnownGroupCode&#39;</span><span class="p">,</span><span class="s">&#39;Actor2EthnicCode&#39;</span><span class="p">,</span><span class="s">&#39;Actor2Religion1Code&#39;</span><span class="p">,</span><span class="s">&#39;Actor2Religion2Code&#39;</span><span class="p">,</span>
</span><span class='line'><span class="s">&#39;Actor2Type1Code&#39;</span><span class="p">,</span><span class="s">&#39;Actor2Type2Code&#39;</span><span class="p">,</span><span class="s">&#39;Actor2Type3Code&#39;</span><span class="p">,</span><span class="s">&#39;IsRootEvent&#39;</span><span class="p">,</span><span class="s">&#39;EventCode&#39;</span><span class="p">,</span>
</span><span class='line'><span class="s">&#39;EventBaseCode&#39;</span><span class="p">,</span><span class="s">&#39;EventRootCode&#39;</span><span class="p">,</span><span class="s">&#39;QuadClass&#39;</span><span class="p">,</span><span class="s">&#39;GoldsteinScale&#39;</span><span class="p">,</span><span class="s">&#39;NumMentions&#39;</span><span class="p">,</span>
</span><span class='line'><span class="s">&#39;NumSources&#39;</span><span class="p">,</span><span class="s">&#39;NumArticles&#39;</span><span class="p">,</span><span class="s">&#39;AvgTone&#39;</span><span class="p">,</span><span class="s">&#39;Actor1Geo_Type&#39;</span><span class="p">,</span><span class="s">&#39;Actor1Geo_FullName&#39;</span><span class="p">,</span>
</span><span class='line'><span class="s">&#39;Actor1Geo_CountryCode&#39;</span><span class="p">,</span><span class="s">&#39;Actor1Geo_ADM1Code&#39;</span><span class="p">,</span><span class="s">&#39;Actor1Geo_Lat&#39;</span><span class="p">,</span><span class="s">&#39;Actor1Geo_Long&#39;</span><span class="p">,</span>
</span><span class='line'><span class="s">&#39;Actor1Geo_FeatureID&#39;</span><span class="p">,</span><span class="s">&#39;Actor2Geo_Type&#39;</span><span class="p">,</span><span class="s">&#39;Actor2Geo_FullName&#39;</span><span class="p">,</span><span class="s">&#39;Actor2Geo_CountryCode&#39;</span><span class="p">,</span>
</span><span class='line'><span class="s">&#39;Actor2Geo_ADM1Code&#39;</span><span class="p">,</span><span class="s">&#39;Actor2Geo_Lat&#39;</span><span class="p">,</span><span class="s">&#39;Actor2Geo_Long&#39;</span><span class="p">,</span><span class="s">&#39;Actor2Geo_FeatureID&#39;</span><span class="p">,</span>
</span><span class='line'><span class="s">&#39;ActionGeo_Type&#39;</span><span class="p">,</span><span class="s">&#39;ActionGeo_FullName&#39;</span><span class="p">,</span><span class="s">&#39;ActionGeo_CountryCode&#39;</span><span class="p">,</span><span class="s">&#39;ActionGeo_ADM1Code&#39;</span><span class="p">,</span>
</span><span class='line'><span class="s">&#39;ActionGeo_Lat&#39;</span><span class="p">,</span><span class="s">&#39;ActionGeo_Long&#39;</span><span class="p">,</span><span class="s">&#39;ActionGeo_FeatureID&#39;</span><span class="p">,</span><span class="s">&#39;DATEADDED&#39;</span><span class="p">,</span><span class="s">&#39;SOURCEURL&#39;</span><span class="p">]</span>
</span><span class='line'>
</span><span class='line'>
</span><span class='line'><span class="n">gdelt</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s">&#39;20130604.export.CSV&#39;</span><span class="p">,</span> <span class="n">sep</span><span class="o">=</span><span class="s">&#39;</span><span class="se">\t</span><span class="s">&#39;</span><span class="p">,</span> <span class="n">header</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">names</span><span class="o">=</span><span class="n">names</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s">&quot;utf-8&quot;</span><span class="p">)</span>
</span></code></pre></td></tr></table></div></figure>


<p>I found that is necessary to indicate the text encoding when importing the data, else an error is
thrown when trying to use <code>pandasql</code>. The resulting dataframe contains 41,569 events. For this
example, I&#8217;m going to take a look at the ongoing crisis in Syria and gather a subset of the
data that reflects this. To do this, I choose only events that have both country codes
as &#8216;SYR&#8217;.</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
</pre></td><td class='code'><pre><code class='python'><span class='line'><span class="n">q</span> <span class="o">=</span> <span class="s">&quot;&quot;&quot;</span>
</span><span class='line'><span class="s">SELECT</span>
</span><span class='line'><span class="s">*</span>
</span><span class='line'><span class="s">FROM</span>
</span><span class='line'><span class="s">gdelt</span>
</span><span class='line'><span class="s">WHERE</span>
</span><span class='line'><span class="s">Actor1CountryCode == &#39;SYR&#39;</span>
</span><span class='line'><span class="s">AND</span>
</span><span class='line'><span class="s">Actor2CountryCode == &#39;SYR&#39;;</span>
</span><span class='line'><span class="s">&quot;&quot;&quot;</span>
</span><span class='line'>
</span><span class='line'><span class="n">syr_subset</span> <span class="o">=</span> <span class="n">sqldf</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="nb">globals</span><span class="p">())</span>
</span></code></pre></td></tr></table></div></figure>


<p>This query results in a dataset with 91 events. The query also took roughly 5 seconds to
complete on my laptop. Not too bad. While this subset can now be used to perform some analyses,
it&#8217;s also possible to perform a simple analysis using SQL queries. For instance, I might wish
to know who the different source actors are, along with the sum of the <code>GoldsteinScale</code>
values for each different source actor. SQL queries make this extremely easy and elegant.</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
</pre></td><td class='code'><pre><code class='python'><span class='line'><span class="n">q</span> <span class="o">=</span> <span class="s">&quot;&quot;&quot;</span>
</span><span class='line'><span class="s">SELECT</span>
</span><span class='line'><span class="s">Actor1Code, sum(GoldsteinScale)</span>
</span><span class='line'><span class="s">FROM</span>
</span><span class='line'><span class="s">gdelt</span>
</span><span class='line'><span class="s">WHERE</span>
</span><span class='line'><span class="s">Actor1CountryCode == &#39;SYR&#39;</span>
</span><span class='line'><span class="s">AND</span>
</span><span class='line'><span class="s">Actor2CountryCode == &#39;SYR&#39;</span>
</span><span class='line'><span class="s">GROUP BY</span>
</span><span class='line'><span class="s">Actor1Code;</span>
</span><span class='line'><span class="s">&quot;&quot;&quot;</span>
</span><span class='line'>
</span><span class='line'><span class="n">sqldf</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="nb">globals</span><span class="p">())</span>
</span></code></pre></td></tr></table></div></figure>


<p>This gives the result of:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
</pre></td><td class='code'><pre><code class='python'><span class='line'><span class="n">Actor1Code</span>  <span class="nb">sum</span><span class="p">(</span><span class="n">GoldsteinScale</span><span class="p">)</span>
</span><span class='line'><span class="mi">0</span>        <span class="n">SYR</span>                <span class="o">-</span><span class="mf">74.8</span>
</span><span class='line'><span class="mi">1</span>     <span class="n">SYRCVL</span>                 <span class="mf">16.6</span>
</span><span class='line'><span class="mi">2</span>     <span class="n">SYRGOV</span>                <span class="o">-</span><span class="mf">81.1</span>
</span><span class='line'><span class="mi">3</span>  <span class="n">SYRGOVMED</span>                <span class="o">-</span><span class="mf">50.0</span>
</span><span class='line'><span class="mi">4</span>     <span class="n">SYRMIL</span>                <span class="o">-</span><span class="mf">29.5</span>
</span><span class='line'><span class="mi">5</span>  <span class="n">SYROPPUAF</span>                <span class="o">-</span><span class="mf">20.0</span>
</span><span class='line'><span class="mi">6</span>     <span class="n">SYRREB</span>                <span class="o">-</span><span class="mf">24.6</span>
</span></code></pre></td></tr></table></div></figure>


<p>  Additionally, we might want to see the same information for dyadic interactions between actors
  along with information regarding how many events occur within the dyad,
  which only requires a small modification of the previous query.</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
</pre></td><td class='code'><pre><code class='python'><span class='line'><span class="n">q</span> <span class="o">=</span> <span class="s">&quot;&quot;&quot;</span>
</span><span class='line'><span class="s">SELECT</span>
</span><span class='line'><span class="s">Actor1Code, Actor2Code, sum(GoldsteinScale), count(Actor1Code)</span>
</span><span class='line'><span class="s">FROM</span>
</span><span class='line'><span class="s">gdelt</span>
</span><span class='line'><span class="s">WHERE</span>
</span><span class='line'><span class="s">Actor1CountryCode == &#39;SYR&#39;</span>
</span><span class='line'><span class="s">AND</span>
</span><span class='line'><span class="s">Actor2CountryCode == &#39;SYR&#39;</span>
</span><span class='line'><span class="s">GROUP BY</span>
</span><span class='line'><span class="s">Actor1Code, Actor2Code;</span>
</span><span class='line'><span class="s">&quot;&quot;&quot;</span>
</span><span class='line'>
</span><span class='line'><span class="n">sqldf</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="nb">globals</span><span class="p">())</span>
</span><span class='line'>
</span><span class='line'>    <span class="n">Actor1Code</span> <span class="n">Actor2Code</span>  <span class="nb">sum</span><span class="p">(</span><span class="n">GoldsteinScale</span><span class="p">)</span>  <span class="n">count</span><span class="p">(</span><span class="n">Actor1Code</span><span class="p">)</span>
</span><span class='line'><span class="mi">0</span>         <span class="n">SYR</span>     <span class="n">SYRCVL</span>                 <span class="o">-</span><span class="mf">3.4</span>                  <span class="mi">3</span>
</span><span class='line'><span class="mi">1</span>         <span class="n">SYR</span>     <span class="n">SYRGOV</span>                <span class="o">-</span><span class="mf">50.0</span>                 <span class="mi">15</span>
</span><span class='line'><span class="mi">2</span>         <span class="n">SYR</span>     <span class="n">SYRLEG</span>                  <span class="mf">7.0</span>                  <span class="mi">3</span>
</span><span class='line'><span class="mi">3</span>         <span class="n">SYR</span>     <span class="n">SYRMIL</span>                <span class="o">-</span><span class="mf">24.0</span>                  <span class="mi">3</span>
</span><span class='line'><span class="mi">4</span>         <span class="n">SYR</span>     <span class="n">SYRREB</span>                 <span class="o">-</span><span class="mf">4.0</span>                  <span class="mi">2</span>
</span><span class='line'><span class="mi">5</span>         <span class="n">SYR</span>     <span class="n">SYRREF</span>                 <span class="o">-</span><span class="mf">0.4</span>                  <span class="mi">1</span>
</span><span class='line'><span class="mi">6</span>      <span class="n">SYRCVL</span>        <span class="n">SYR</span>                 <span class="mf">16.6</span>                  <span class="mi">4</span>
</span><span class='line'><span class="mi">7</span>      <span class="n">SYRGOV</span>        <span class="n">SYR</span>                <span class="o">-</span><span class="mf">70.9</span>                 <span class="mi">30</span>
</span><span class='line'><span class="mi">8</span>      <span class="n">SYRGOV</span>     <span class="n">SYRGOV</span>                <span class="o">-</span><span class="mf">10.2</span>                 <span class="mi">14</span>
</span><span class='line'><span class="mi">9</span>   <span class="n">SYRGOVMED</span>        <span class="n">SYR</span>                <span class="o">-</span><span class="mf">40.0</span>                  <span class="mi">4</span>
</span><span class='line'><span class="mi">10</span>  <span class="n">SYRGOVMED</span>     <span class="n">SYRCVL</span>                <span class="o">-</span><span class="mf">10.0</span>                  <span class="mi">1</span>
</span><span class='line'><span class="mi">11</span>     <span class="n">SYRMIL</span>        <span class="n">SYR</span>                <span class="o">-</span><span class="mf">29.5</span>                  <span class="mi">3</span>
</span><span class='line'><span class="mi">12</span>  <span class="n">SYROPPUAF</span>     <span class="n">SYRGOV</span>                <span class="o">-</span><span class="mf">20.0</span>                  <span class="mi">2</span>
</span><span class='line'><span class="mi">13</span>     <span class="n">SYRREB</span>        <span class="n">SYR</span>                <span class="o">-</span><span class="mf">24.6</span>                  <span class="mi">6</span>
</span></code></pre></td></tr></table></div></figure>


<p>It is also possible to perform more complicated groupings and analyses using only SQL but, as
these examples show, even with a very limited knowledge of SQL one is able to draw subsets of
the data and perform some simple, but interesting, analyses. Additionally, since the data is stored
in a <code>pandas</code> dataframe, any analytical methods that are contained in <code>pandas</code> can be used, which
provides another set of powerful options for analysis. The final benefit of <code>pandasql</code> is that writing
these queries is <em>much</em> faster than writing a subsetting script, and the queries are also
remarkably fast; none of the queries used in this post took more than 10 seconds to run.</p>

<h3>Notes</h3>

<p>While <code>pandasql</code>, and SQL in general, is very useful and far more elegant than hacked together
scripts, there are some limitations. The primary limitation is that this method likely will not
scale well to iterating over the entire GDELT dataset, i.e., all the data in the historical backfile
downloads. The default behavior of <code>sqldf</code> is to hold the data in memory. There is an option to
temporarily write the data to disk, but this still does not address the issue of reading in the
yearly or monthly data into the initial dataframe. There is likely a workaround using the
chunk iterator for <code>pd.read_csv</code>, but in my mind this defeats part of the appeal for using
<code>pandasql</code> in the first place: quickness and ease of use. For working with the entire dataset,
a technology such as Hive, or a full SQL database, is likely a more viable option. I will have
more to say on the use of Hive in a future post. With all that said, however, <code>pandasql</code> is
a very useful tool when analyzing more atomic chunks of the GDELT data; daily, monthly, and perhaps
even a single year for the earlier years is not outside the realm of possibility. Finally, if you
need a resource to learn SQL, I recommend Zed Shaw&#8217;s <a href="http://sql.learncodethehardway.org/">Learn SQL The Hard Way</a>,
which will provide as much SQL as you need to perform data subsetting tasks.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Making GDELT Downloads Easy]]></title>
    <link href="http://johnb30.github.com/blog/2013/06/01/making-gdelt-downloads-easy/"/>
    <updated>2013-06-01T00:06:00-04:00</updated>
    <id>http://johnb30.github.com/blog/2013/06/01/making-gdelt-downloads-easy</id>
    <content type="html"><![CDATA[<h2>Downloading GDELT</h2>

<p>Yesterday, I started downloading the GDELT data from the <a href="gdelt.utdallas.edu">website</a>,
having previously pulled the data from the servers at Penn State.
Having to navigate the website and download each file individually
caused me far more frustration that it should have, not due to the
design of the website but due to my own impatience. I&#8217;ll have more
to say about the general state of data distribution in the social
sciences in another post, but for now it&#8217;s enough to say that I&#8217;m
not a fan of downloading data by hand. Because of this, I wrote some
scripts in Python to help with downloading the data. There are two
scripts, <code>download_historical.py</code> and <code>download_daily.py</code> to
download either the historical files or the continuously-updated
daily files. The code is on <a href="https://github.com/johnb30/gdelt_download">github</a>.
I&#8217;ve copied the contents of the README file below so you can determine
if the scripts would be useful for you.</p>

<!-- more -->


<h3>README</h3>

<h2>GDELT Download</h2>

<p>The GDELT data is spread across multiple files, with a new file added each day.
Downloading each and every file is not a fun endeavor. These scripts were
written in order to aid in the download of the GDELT data. The first script
<code>download_historical.py</code> is aimed at downloading the historical data, and the
previous daily updated. The second script, <code>download_daily.py</code>, is aimed
at downloading the new files that are uploaded to the GDELT website each day.
This script enables the user to either call the script each day to fetch the
newest upload, or to run the process in the background to download the new
updates each day at 10:00am.</p>

<p>Each script implements a 30 second delay where appropriate in order to
avoid swamping the server.</p>

<h2><code>download_historical</code> Usage</h2>

<p>The script has three modes: <code>daily</code>, <code>single</code>, and <code>range</code>.</p>

<p><em>Note</em>: If you wish to use the <code>daily</code> mode, the <code>requests</code> and <code>lxml</code> libaries
are necessary. You can install both using <code>pip install library_name</code>. The script
also makes use of <code>argparse</code>, which is included in the standard libary from
Python 2.7+. If you are using an older version, it is necessary to install
<code>argparse</code> using <code>pip</code> or <code>easy_install</code>.</p>

<h3>Daily:</h3>

<p>The daily mode downloads the daily updates that are currently uploaded to the
GDELT website.</p>

<h4>Usage:</h4>

<p><code>python download_historical.py daily -d ~/gdelt/ -U</code></p>

<p>Where <code>-d</code> is the flag for the directory to which the files should be written,
and <code>-U</code> is the optional flag indicating whether each downloaded file should
be unzipped.</p>

<h3>Single:</h3>

<p>The single mode downloads the updates for a single year.</p>

<h4>Usage:</h4>

<p><code>python download_historical.py single -y 1979 -d ~/gdelt/ -U</code></p>

<p>Where <code>-y</code> is the flag that indicates which year should be downloaded, <code>-d</code>
is the flag for the directory to which the files should be written, and <code>-U</code>
is the optional flag indicating whether each downloaded file should be unzipped.</p>

<h3>Range:</h3>

<p>The range mode downloads the updates for a range of years.</p>

<h4>Usage:</h4>

<p><code>python download_historical.py range -y 1979-2012 -d ~/gdelt/ -U</code></p>

<p>Where <code>-y</code> is the flag that indicates which years should be downloaded, <code>-d</code>
is the flag for the directory to which the files should be written, and <code>-U</code>
is the optional flag indicating whether each downloaded file should be unzipped.</p>

<h2><code>download_daily</code> Usage</h2>

<p>The script has two modes: <code>fetch</code>, and <code>schedule</code>.</p>

<p><em>Note</em>: If you wish to use the <code>schedule</code> mode, the <code>schedule</code> library
is necessary. You can install using <code>pip install schedule</code>.</p>

<h3>Fetch:</h3>

<p>The fetch mode downloads only the current date&#8217;s update.</p>

<h4>Usage:</h4>

<p><code>python download_daily.py fetch -d ~/gdelt/ -U</code></p>

<p>Where <code>-d</code> is the flag for the directory to which the files should be written,
and <code>-U</code> is the optional flag indicating whether each downloaded file should
be unzipped.</p>

<h3>Schedule:</h3>

<p>The fetch schedule mode sets the script to run in the background and request
each day at 10:00am that date&#8217;s upload from the server. In order to work, the
script must be left running in a terminal tab. The use of a utility such as
<code>screen</code> or <code>tmux</code> is recommended in order to allow the program to run
unmonitored in the background.</p>

<h4>Usage:</h4>

<p><code>python download_daily.py schedule -d ~/gdelt/ -U</code></p>

<p>Where <code>-d</code> is the flag for the directory to which the files should be written,
and <code>-U</code> is the optional flag indicating whether each downloaded file should
be unzipped.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Terrorism Studies and Prediction]]></title>
    <link href="http://johnb30.github.com/blog/2013/04/29/terrorism-studies-and-prediction/"/>
    <updated>2013-04-29T13:44:00-04:00</updated>
    <id>http://johnb30.github.com/blog/2013/04/29/terrorism-studies-and-prediction</id>
    <content type="html"><![CDATA[<p>This semester I&#8217;m taking a class on terrorism. Overall I&#8217;ve found the class
very enjoyable; the topic of political violence is one that is always
fascinating. With that said, I found one issue that would repeatedly pop up
in the readings for the seminar. Almost every paper would demarcate some type
of theory, discuss the data used, and run some statistical tests, which is
pretty standard social science research. The issue arose in the &#8220;Discussion&#8221;
or &#8220;Conclusion&#8221; sections. Almost invariably the authors would discuss the
practical implications of their research, which is fine until the dreaded
<strong>prediction</strong> word appears. Then claims about the predictive accuracy of
the models used in the paper would rear their ugly heads. These models were
explicitly <em>not</em> predictive models. This became the soap box that I would
drag out repeatedly throughout the semester. Finally, I decided to put my own
assertions to the test and see how some models performed on out-of-sample
predictive tests.</p>

<!-- more -->


<h3>The Paper</h3>

<p>The week I chose to perform this analysis we discussed papers that focused on
counterterrorism policies. One paper was <a href="http://jcr.sagepub.com/content/54/2/237">&#8220;Foreign Aid Versus Military Intervention in the War on Terror&#8221;</a>
(behind a paywall) by Jean-Paul Azam and Veronique Thelen. A full discussion
of their theory is beyond the scope of this post, but the authors&#8217; basic
argument is that foreign investment can reduce the number of transnational
terrorist events for a number of reasons. They test this general assertion
using a variety of models. Luckily, they provide replication data for their
paper on the Journal of Conflict Resolution website linked above. I set off
to exactly replicate the model presented in Table 2, Model 4 for those
following along in the paper.</p>

<h3>The Replication</h3>

<p>The first step to replicating the paper was to run their Stata <code>do-file</code> on
the data they provided. This is done to ensure that I have exactly the same
variables in my models that they do in theirs, specifically the residualized
variables they present. As a note, I will avoid commenting on the general
modeling choices made in the paper; this post is only about the predictive
accuracy of models on out-of-sample data. I work with what the authors worked
with. After running the <code>do-file</code>, I check to make sure that the results
generated by Stata are the same as those presented in the paper, which is
roughly true in the case of the coefficient estimates. I then take the data,
including the newly generated variables, and read it into Python using
<code>statsmodels</code>. After the data is read in, I replicate the formula used in
the paper, generate a train-test split of 75% and 25%, and then fit the
negative binomial model. The random 75%-25% split is appropriate here since
the data is cross sectional and not time series in nature.</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
<span class='line-number'>41</span>
<span class='line-number'>42</span>
<span class='line-number'>43</span>
<span class='line-number'>44</span>
<span class='line-number'>45</span>
</pre></td><td class='code'><pre><code class='python'><span class='line'><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">cross_validation</span>
</span><span class='line'><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">RandomForestRegressor</span>
</span><span class='line'><span class="kn">import</span> <span class="nn">statsmodels.api</span> <span class="kn">as</span> <span class="nn">sm</span>
</span><span class='line'><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
</span><span class='line'><span class="kn">import</span> <span class="nn">patsy</span>
</span><span class='line'>
</span><span class='line'><span class="n">data</span> <span class="o">=</span> <span class="n">sm</span><span class="o">.</span><span class="n">iolib</span><span class="o">.</span><span class="n">genfromdta</span><span class="p">(</span><span class="s">&#39;rep_data.dta&#39;</span><span class="p">,</span> <span class="n">pandas</span> <span class="o">=</span> <span class="bp">True</span><span class="p">)</span>
</span><span class='line'><span class="c">#Make an R-style formula</span>
</span><span class='line'><span class="n">form</span> <span class="o">=</span> <span class="s">&#39;arqade ~ gdppc + logpop + odapc + secenroll + oecd + cad + ssh + ethnic + law + rodapc + rsecenroll&#39;</span>
</span><span class='line'><span class="n">y</span><span class="p">,</span> <span class="n">X</span> <span class="o">=</span> <span class="n">patsy</span><span class="o">.</span><span class="n">dmatrices</span><span class="p">(</span><span class="n">form</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="n">return_type</span> <span class="o">=</span> <span class="s">&#39;dataframe&#39;</span><span class="p">)</span>
</span><span class='line'>
</span><span class='line'><span class="c">#Generate 75%-25% train-test split</span>
</span><span class='line'><span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">cross_validation</span><span class="o">.</span><span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.25</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</span><span class='line'>
</span><span class='line'><span class="c">#Fit the model</span>
</span><span class='line'><span class="n">mod</span> <span class="o">=</span> <span class="n">sm</span><span class="o">.</span><span class="n">GLM</span><span class="p">(</span><span class="n">y_train</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">family</span> <span class="o">=</span> <span class="n">sm</span><span class="o">.</span><span class="n">families</span><span class="o">.</span><span class="n">NegativeBinomial</span><span class="p">())</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
</span><span class='line'>
</span><span class='line'><span class="k">print</span> <span class="n">mod</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</span><span class='line'>
</span><span class='line'><span class="n">Generalized</span> <span class="n">Linear</span> <span class="n">Model</span> <span class="n">Regression</span> <span class="n">Results</span>
</span><span class='line'><span class="o">==============================================================================</span>
</span><span class='line'><span class="n">Dep</span><span class="o">.</span> <span class="n">Variable</span><span class="p">:</span>                      <span class="n">y</span>   <span class="n">No</span><span class="o">.</span> <span class="n">Observations</span><span class="p">:</span>                   <span class="mi">99</span>
</span><span class='line'><span class="n">Model</span><span class="p">:</span>                            <span class="n">GLM</span>   <span class="n">Df</span> <span class="n">Residuals</span><span class="p">:</span>                       <span class="mi">87</span>
</span><span class='line'><span class="n">Model</span> <span class="n">Family</span><span class="p">:</span>        <span class="n">NegativeBinomial</span>   <span class="n">Df</span> <span class="n">Model</span><span class="p">:</span>                           <span class="mi">11</span>
</span><span class='line'><span class="n">Link</span> <span class="n">Function</span><span class="p">:</span>                    <span class="n">log</span>   <span class="n">Scale</span><span class="p">:</span>                   <span class="mf">1.54679011354</span>
</span><span class='line'><span class="n">Method</span><span class="p">:</span>                          <span class="n">IRLS</span>   <span class="n">Log</span><span class="o">-</span><span class="n">Likelihood</span><span class="p">:</span>                <span class="o">-</span><span class="mf">190.88</span>
</span><span class='line'><span class="n">Date</span><span class="p">:</span>                <span class="n">Sun</span><span class="p">,</span> <span class="mi">21</span> <span class="n">Apr</span> <span class="mi">2013</span>   <span class="n">Deviance</span><span class="p">:</span>                       <span class="mf">132.43</span>
</span><span class='line'><span class="n">Time</span><span class="p">:</span>                        <span class="mi">16</span><span class="p">:</span><span class="mi">32</span><span class="p">:</span><span class="mi">56</span>   <span class="n">Pearson</span> <span class="n">chi2</span><span class="p">:</span>                     <span class="mf">135.</span>
</span><span class='line'><span class="n">No</span><span class="o">.</span> <span class="n">Iterations</span><span class="p">:</span>                    <span class="mi">17</span>
</span><span class='line'><span class="o">==============================================================================</span>
</span><span class='line'><span class="n">coef</span>    <span class="n">std</span> <span class="n">err</span>          <span class="n">t</span>      <span class="n">P</span><span class="o">&gt;|</span><span class="n">t</span><span class="o">|</span>      <span class="p">[</span><span class="mf">95.0</span><span class="o">%</span> <span class="n">Conf</span><span class="o">.</span> <span class="n">Int</span><span class="o">.</span><span class="p">]</span>
</span><span class='line'><span class="o">------------------------------------------------------------------------------</span>
</span><span class='line'><span class="n">const</span>          <span class="mf">7.5576</span>      <span class="mf">4.717</span>      <span class="mf">1.602</span>      <span class="mf">0.113</span>        <span class="o">-</span><span class="mf">1.687</span>    <span class="mf">16.802</span>
</span><span class='line'><span class="n">x1</span>          <span class="mf">5.861e-05</span>    <span class="mf">4.2e-05</span>      <span class="mf">1.397</span>      <span class="mf">0.166</span>     <span class="o">-</span><span class="mf">2.36e-05</span>     <span class="mf">0.000</span>
</span><span class='line'><span class="n">x2</span>             <span class="mf">0.1561</span>      <span class="mf">0.203</span>      <span class="mf">0.768</span>      <span class="mf">0.444</span>        <span class="o">-</span><span class="mf">0.242</span>     <span class="mf">0.554</span>
</span><span class='line'><span class="n">x3</span>            <span class="o">-</span><span class="mf">0.0339</span>      <span class="mf">0.014</span>     <span class="o">-</span><span class="mf">2.380</span>      <span class="mf">0.019</span>        <span class="o">-</span><span class="mf">0.062</span>    <span class="o">-</span><span class="mf">0.006</span>
</span><span class='line'><span class="n">x4</span>            <span class="o">-</span><span class="mf">0.0633</span>      <span class="mf">0.019</span>     <span class="o">-</span><span class="mf">3.315</span>      <span class="mf">0.001</span>        <span class="o">-</span><span class="mf">0.101</span>    <span class="o">-</span><span class="mf">0.026</span>
</span><span class='line'><span class="n">x5</span>             <span class="mf">0.9221</span>      <span class="mf">0.878</span>      <span class="mf">1.050</span>      <span class="mf">0.297</span>        <span class="o">-</span><span class="mf">0.800</span>     <span class="mf">2.644</span>
</span><span class='line'><span class="n">x6</span>             <span class="mf">5.5344</span>      <span class="mf">2.225</span>      <span class="mf">2.487</span>      <span class="mf">0.015</span>         <span class="mf">1.173</span>     <span class="mf">9.896</span>
</span><span class='line'><span class="n">x7</span>            <span class="o">-</span><span class="mf">3.3479</span>      <span class="mf">0.888</span>     <span class="o">-</span><span class="mf">3.771</span>      <span class="mf">0.000</span>        <span class="o">-</span><span class="mf">5.088</span>    <span class="o">-</span><span class="mf">1.608</span>
</span><span class='line'><span class="n">x8</span>             <span class="mf">0.7016</span>      <span class="mf">0.200</span>      <span class="mf">3.510</span>      <span class="mf">0.001</span>         <span class="mf">0.310</span>     <span class="mf">1.093</span>
</span><span class='line'><span class="n">x9</span>            <span class="o">-</span><span class="mf">0.2140</span>      <span class="mf">0.261</span>     <span class="o">-</span><span class="mf">0.820</span>      <span class="mf">0.415</span>        <span class="o">-</span><span class="mf">0.726</span>     <span class="mf">0.298</span>
</span><span class='line'><span class="n">x10</span>            <span class="mf">0.0441</span>      <span class="mf">0.018</span>      <span class="mf">2.477</span>      <span class="mf">0.015</span>         <span class="mf">0.009</span>     <span class="mf">0.079</span>
</span><span class='line'><span class="n">x11</span>            <span class="mf">0.0724</span>      <span class="mf">0.022</span>      <span class="mf">3.304</span>      <span class="mf">0.001</span>         <span class="mf">0.029</span>     <span class="mf">0.115</span>
</span><span class='line'><span class="o">==============================================================================</span>
</span></code></pre></td></tr></table></div></figure>


<p>The results of this analysis roughly match the results presented in the paper.
Some differences are to be expected, of course, due to the reduced number of
observations. Since these results seem to match those in the paper, the
out-of-sample forecasts can be run. This is done using the <code>predict</code> method
of the <code>mod</code> object. The true and predicted results are then compared using
the root mean squared error. I&#8217;m not completely confident in the use of the
RMSE, but it is a quick and easy way to perform a comparison in this case.</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
</pre></td><td class='code'><pre><code class='python'><span class='line'><span class="n">y_pred</span> <span class="o">=</span> <span class="n">mod</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
</span><span class='line'>
</span><span class='line'><span class="c">#Calculate RMSE</span>
</span><span class='line'><span class="n">rmse</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">y_test</span> <span class="o">-</span> <span class="n">y_pred</span><span class="p">)))</span>
</span><span class='line'>
</span><span class='line'><span class="k">print</span> <span class="s">&#39;The RMSE is {}&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">rmse</span><span class="p">)</span>
</span><span class='line'><span class="n">The</span> <span class="n">RMSE</span> <span class="ow">is</span> <span class="mf">22.7156218086</span>
</span></code></pre></td></tr></table></div></figure>


<p>The value of the RMSE error shows that the predicted values tend to be off by
about 23 events. This indicates that the model does not predict the impact
of foreign aid on terrorist events well. Admittedly, this analysis may not be
the most favorable to Azam and Thelen. In order to give them the benefit of
the doubt, I also used another modeling technique to see if this inaccuracy persists.
I used a random forest to fit the same model used in the negative binomial
and then examined the RMSE.</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
</pre></td><td class='code'><pre><code class='python'><span class='line'><span class="c">#Create the RF regressor object</span>
</span><span class='line'><span class="n">reg</span> <span class="o">=</span> <span class="n">RandomForestRegressor</span><span class="p">(</span><span class="mi">1000</span><span class="p">,</span> <span class="n">n_jobs</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
</span><span class='line'><span class="c">#Fit the model</span>
</span><span class='line'><span class="n">reg</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</span><span class='line'>
</span><span class='line'><span class="c">#Predict using the X_test data</span>
</span><span class='line'><span class="n">rf_pred</span> <span class="o">=</span> <span class="n">reg</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
</span><span class='line'>
</span><span class='line'><span class="c">#Calculate RMSE</span>
</span><span class='line'><span class="n">rmse</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">y_test</span> <span class="o">-</span> <span class="n">rf_pred</span><span class="p">)))</span>
</span><span class='line'>
</span><span class='line'><span class="k">print</span> <span class="s">&#39;The RMSE is {}&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">rmse</span><span class="p">)</span>
</span><span class='line'><span class="n">The</span> <span class="n">RMSE</span> <span class="ow">is</span> <span class="mf">21.7068829972</span>
</span></code></pre></td></tr></table></div></figure>


<p>Even using a method that is possibly more robust, the prediction still doesn&#8217;t
seem very satisfactory.</p>

<h3>Models in the Social Sciences</h3>

<p>In case it was not explicitly clear in the beginning of this post, I want to
make it clear now: this analysis is not a direct critique of Azam and Thelen.
I think their theory is sound and it makes sense that foreign investment and
the other variables they analyze can make a difference in terrorism. I also
do not think that the type of inferential statistics used by Azam and Thelen,
and commonly used elsewhere in political science, are wrong in all cases.
I do believe we can gain knowledge from models like those
presented by Azam and Thelen. What I do take issue with, however, is the call
to arms that is frequently sounded in the conclusions of papers, which are
often directed towards policy makers, that the results of the paper predict
phenomenon <em>X</em> and thus policy <em>Y</em> should be enacted. Especially when policy
<em>Y</em> is something like military intervention. In order to make these types of
claims, some type of out-of-sample test should be performed in order to
determine the effect of the variables under examination.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Actor Codes in GDELT]]></title>
    <link href="http://johnb30.github.com/blog/2013/04/23/actor-codes-in-gdelt/"/>
    <updated>2013-04-23T18:40:00-04:00</updated>
    <id>http://johnb30.github.com/blog/2013/04/23/actor-codes-in-gdelt</id>
    <content type="html"><![CDATA[<p>As I&#8217;m sure anyone who has begun exploring the GDELT data has noticed, there
are a large number of unique actors coded in the data. While working on a
project for a colleague, I began to wonder exactly how many unique actor
codes exist in the dataset, and what the maximum level of actor code complexity
is within the dataset. For those who don&#8217;t know, the actor codes are created
by chaining together three character CAMEO actor codes, which results in actor
codes that look like <code>USAMIL</code>, which indicates the United States military. The complexity
of the actor code, then, refers to how many of these three character codes are
chained together.</p>

<h4>Results</h4>

<p>Using the Python code available <a href="https://gist.github.com/johnb30/5448269">here</a>, I iterated over the dataset and
identified each unique actor code in the data, along with how many times the
unique code appeared. The resulting data is available
<a href="http://johnb30.github.com/blog/downloads/unique_actors.csv">here</a>. The results indicate that there are <strong>22,857</strong>
unique actors in the dataset, with a maximum of <strong>6</strong> three character codes chained
together. The following two plots show the top twenty actor codes overall,
along with the top twenty USA actor codes.</p>

<p><img src="http://johnb30.github.com/blog/downloads/top20.png"></p>

<p><img src="http://johnb30.github.com/blog/downloads/usa_top20.png"></p>

<p>This analysis shows the incredible complexity that is present in the actor codes,
which I would argue is the most important, fascinating, and challenging part
of working with event data on this scale.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[GDELT, Big Data, and Theory]]></title>
    <link href="http://johnb30.github.com/blog/2013/04/12/gdelt/"/>
    <updated>2013-04-12T14:56:00-04:00</updated>
    <id>http://johnb30.github.com/blog/2013/04/12/gdelt</id>
    <content type="html"><![CDATA[<p>I made the remark on Twitter that it seemed like GDELT week due to a
<a href="http://ideas.foreignpolicy.com/posts/2013/04/10%20what_can_we_learn_from_the_last_200_million_things_that_happened_in_the_world">Foreign Policy</a>
piece about the dataset, Phil and Kalev&#8217;s <a href="http://eventdata.psu.edu/papers.dir/ISA.2013.GDELT.pdf">paper</a>
for the ISA 2013 meeting, and a <a href="https://dartthrowingchimp.wordpress.com/2013/04/10/the-future-of-political-science-just-showed-up/">host</a>
of <a href="http://badhessian.org/gdelt-and-social-movements/">blog</a>
<a href="https://willopines.wordpress.com/2013/04/11/excitement-about-gdelt-and-some-personal-intellectual-history/">posts</a>
about the data. So, in the spirit of GDELT week, I thought I would throw my hat into
the ring. But instead of taking the approach of lauding the new age that is approaching
for political and social research due to the monstrous scale of the data now available, I thought
I would write a little about the issues that come along with dealing with such massive data.</p>

<h3>Dealing with GDELT</h3>

<p>As someone who has spent the better part of the past 8 months dealing with the GDELT dataset,
including <a href="http://johnbeieler.org/blog/2013/04/04/how-do-i-gdelt-subsetting-and-aggregating-the-gdelt-dataset/">writing</a> a little about
working with the data, I feel that I have a somewhat unique perspective. The long and the
short of my experience is: working with data on this scale is hard. This may strike some
as obvious, especially given the cottage industry that has sprung up around Hadoop and
and other services for processing data. GDELT is 200+ million events spread across several
years. Each year of the <a href="http://eventdata.psu.edu/data.dir/GDELT.html">reduced data</a> is in
a separate file and contains information about many, many different actors. This is part of
what makes the data so intriguing and useful, but the data is also unlike data such as the
ever-popular <a href="http://www.correlatesofwar.org/COW2%20Data/MIDs/MID310.html">MID data</a> in
political science that is easily managed in a program like Stata or <code>R</code>. The data requires
subsetting, massaging, and aggregating; having so much data can, at some points, become
overwhelming. What states do I want to look at? What type of actors? What type of actions?
What about substate actors? Oh, what about the dyadic interactions? These questions and
more quickly come to the fore when dealing with data on this scale. So while the GDELT
data offers an avenue to answer some existing questions, it also brings with it many
potential problems.</p>

<h3>Careful Research</h3>

<p>So, that all sounds kind of depressing. We have this new, cool dataset that could
be tremendously useful, but it also presents many hurdles. What, then, should we
as social science researchers do about it? My answer is careful theorizing and
thinking about the processes under examination. This might be a &#8220;well, duh&#8221;
moment to those in the social sciences, but I think it is worth saying when
there are some heralding <a href="http://www.wired.com/science/discoveries/magazine/16-07/pb_theory">&#8220;The End of Theory&#8221;</a>.
This type of large-scale data does not reduce theory and the scientific
method to irrelevance. Instead, theory is elevated to a position of
higher importance. What states do I want to look at? What type of
actions? Well, what does the theory say? As Hilary Mason noted in
a <a href="https://twitter.com/hmason/status/294930646122504192">tweet</a>:</p>

<blockquote><p>Data tells you whether to use A or B. Science tells you what A and B should be in the first place.</p></blockquote>

<p>Put into more social-scientific language, data tells us the relationship
between A and B, while science tells us what A and B should be and what type
of observations should be used. The data under examination in a given study
should be driven by careful consideration of the processes of interest.
This idea should not, however, be construed as a rejection of &#8220;big data&#8221; in the
social sciences. I personally believe the exact opposite; give me as many features,
measures, and observations as possible and let algorithms sort out what is important.
Instead, I think the social sciences, and science in general, is about asking
interesting questions of the data that will often require more finesse than taking an
&#8220;ANALYZE ALL THE DATA&#8221; approach. Thus, while datasets like GDELT provide new opportunities,
they are not opportunities to relax and let the data do the talking. If anything, big data
generating processes will require more work on the part of the researcher than previous
data sources.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[How Do I GDELT?: Subsetting and Aggregating the GDELT Dataset]]></title>
    <link href="http://johnb30.github.com/blog/2013/04/04/how-do-i-gdelt-subsetting-and-aggregating-the-gdelt-dataset/"/>
    <updated>2013-04-04T19:15:00-04:00</updated>
    <id>http://johnb30.github.com/blog/2013/04/04/how-do-i-gdelt-subsetting-and-aggregating-the-gdelt-dataset</id>
    <content type="html"><![CDATA[<h3>GDELT</h3>

<p>Over the past week, the Global Data on Events, Location and Tone (GDELT)
dataset was finally released to the general public. The data is available
at the Penn State event data <a href="http://eventdata.psu.edu/data.dir/GDELT.html">website</a>.
We at Penn State had the
good fortune to have access to this dataset for many months before its public
release. This allowed us to gain some experience working with this massive
collection of data. As a brief background, GDELT is comprised of event
data records spanning 1979 - mid 2012. The events are coded according to the
<a href="http://eventdata.psu.edu/data.dir/cameo.html">CAMEO</a> coding scheme, with the
addition of a &#8220;QuadCategory,&#8221; which separates the events into the material conflict,
material cooperation, verbal conflict, and verbal conflict categories. The data is
spread across 33 different files, each of which is substantially large on its own.
This makes it fairly difficult to work with, and almost guarantees that some
subset of the data is necessary in order to perform analysis. Phil Schrodt has
included some programs with the data to aid in this subsetting, but I thought there
might be some who would prefer to get their hands dirty and write some of their
own code. Given this, I thought I would share some of the knowledge I gained
while working with the GDELT dataset.</p>

<p>For the purposes of this brief introduction, I will work under the assumption
that the desired events are those that originate from the United States, are
directed at some type of state actor, and are either verbal cooperation or
conflict. The following code, written in Python, demonstrates how this subset
might be selected from the GDELT data. The code also assumes the reader
has the <code>pandas</code> and <code>path</code> Python modules installed. Both can be installed using
the normal <code>pip install</code> method. Finally, the complete code is available as
a gist on <a href="https://gist.github.com/johnb30/5316196">github</a>.</p>

<p>Before starting, it is always useful to take a peek at the data. This is as
simple as opening up the terminal and using <code>head 1979.reduced.txt</code>. Doing this
shows the various columns in the reduced data and how they are arranged. We
can see that the date is in the 0 index, with <code>Actor1Code</code> and <code>Actor2Code</code>
in spots 1 and 2, respectively. Additionally, <code>EventCode</code> is located in spot 3,
while the <code>QuadCategory</code> variable is, fittingly, in position 4. These indices
will prove crucial when it comes time to split and subset the data.</p>

<!-- more -->


<h3>Code</h3>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
</pre></td><td class='code'><pre><code class='python'><span class='line'><span class="kn">from</span> <span class="nn">path</span> <span class="kn">import</span> <span class="n">path</span>
</span><span class='line'><span class="kn">import</span> <span class="nn">pandas</span> <span class="kn">as</span> <span class="nn">pd</span>
</span><span class='line'>
</span><span class='line'><span class="n">allActors</span> <span class="o">=</span> <span class="p">[</span><span class="s">&#39;AFG&#39;</span><span class="p">,</span> <span class="s">&#39;ALA&#39;</span><span class="p">,</span> <span class="s">&#39;ALB&#39;</span><span class="p">,</span> <span class="s">&#39;DZA&#39;</span><span class="p">,</span> <span class="s">&#39;ASM&#39;</span><span class="p">,</span> <span class="s">&#39;AND&#39;</span><span class="p">,</span> <span class="s">&#39;AGO&#39;</span><span class="p">,</span> <span class="s">&#39;AIA&#39;</span><span class="p">,</span> <span class="s">&#39;ATG&#39;</span><span class="p">,</span>
</span><span class='line'>            <span class="s">&#39;ARG&#39;</span><span class="p">,</span> <span class="s">&#39;ARM&#39;</span><span class="p">,</span> <span class="s">&#39;ABW&#39;</span><span class="p">,</span> <span class="s">&#39;AUS&#39;</span><span class="p">,</span> <span class="s">&#39;AUT&#39;</span><span class="p">,</span> <span class="s">&#39;AZE&#39;</span><span class="p">,</span> <span class="s">&#39;BHS&#39;</span><span class="p">,</span> <span class="s">&#39;BHR&#39;</span><span class="p">,</span> <span class="s">&#39;BGD&#39;</span><span class="p">,</span>
</span><span class='line'>            <span class="s">&#39;BRB&#39;</span><span class="p">,</span> <span class="s">&#39;BLR&#39;</span><span class="p">,</span> <span class="s">&#39;BEL&#39;</span><span class="p">,</span> <span class="s">&#39;BLZ&#39;</span><span class="p">,</span> <span class="s">&#39;BEN&#39;</span><span class="p">,</span> <span class="s">&#39;BMU&#39;</span><span class="p">,</span> <span class="s">&#39;BTN&#39;</span><span class="p">,</span> <span class="s">&#39;BOL&#39;</span><span class="p">,</span> <span class="s">&#39;BIH&#39;</span><span class="p">,</span>
</span><span class='line'>            <span class="s">&#39;BWA&#39;</span><span class="p">,</span> <span class="s">&#39;BRA&#39;</span><span class="p">,</span> <span class="s">&#39;VGB&#39;</span><span class="p">,</span> <span class="s">&#39;BRN&#39;</span><span class="p">,</span> <span class="s">&#39;BGR&#39;</span><span class="p">,</span> <span class="s">&#39;BFA&#39;</span><span class="p">,</span> <span class="s">&#39;BDI&#39;</span><span class="p">,</span> <span class="s">&#39;KHM&#39;</span><span class="p">,</span> <span class="s">&#39;CMR&#39;</span><span class="p">,</span>
</span><span class='line'>            <span class="s">&#39;CAN&#39;</span><span class="p">,</span> <span class="s">&#39;CPV&#39;</span><span class="p">,</span> <span class="s">&#39;CYM&#39;</span><span class="p">,</span> <span class="s">&#39;CAF&#39;</span><span class="p">,</span> <span class="s">&#39;TCD&#39;</span><span class="p">,</span> <span class="s">&#39;CHL&#39;</span><span class="p">,</span> <span class="s">&#39;CHN&#39;</span><span class="p">,</span> <span class="s">&#39;COL&#39;</span><span class="p">,</span> <span class="s">&#39;COM&#39;</span><span class="p">,</span>
</span><span class='line'>            <span class="s">&#39;COD&#39;</span><span class="p">,</span> <span class="s">&#39;COG&#39;</span><span class="p">,</span> <span class="s">&#39;COK&#39;</span><span class="p">,</span> <span class="s">&#39;CRI&#39;</span><span class="p">,</span> <span class="s">&#39;CIV&#39;</span><span class="p">,</span> <span class="s">&#39;HRV&#39;</span><span class="p">,</span> <span class="s">&#39;CUB&#39;</span><span class="p">,</span> <span class="s">&#39;CYP&#39;</span><span class="p">,</span> <span class="s">&#39;CZE&#39;</span><span class="p">,</span>
</span><span class='line'>            <span class="s">&#39;DNK&#39;</span><span class="p">,</span> <span class="s">&#39;DJI&#39;</span><span class="p">,</span> <span class="s">&#39;DMA&#39;</span><span class="p">,</span> <span class="s">&#39;DOM&#39;</span><span class="p">,</span> <span class="s">&#39;TMP&#39;</span><span class="p">,</span> <span class="s">&#39;ECU&#39;</span><span class="p">,</span> <span class="s">&#39;EGY&#39;</span><span class="p">,</span> <span class="s">&#39;SLV&#39;</span><span class="p">,</span> <span class="s">&#39;GNQ&#39;</span><span class="p">,</span>
</span><span class='line'>            <span class="s">&#39;ERI&#39;</span><span class="p">,</span> <span class="s">&#39;EST&#39;</span><span class="p">,</span> <span class="s">&#39;ETH&#39;</span><span class="p">,</span> <span class="s">&#39;FRO&#39;</span><span class="p">,</span> <span class="s">&#39;FLK&#39;</span><span class="p">,</span> <span class="s">&#39;FJI&#39;</span><span class="p">,</span> <span class="s">&#39;FIN&#39;</span><span class="p">,</span> <span class="s">&#39;FRA&#39;</span><span class="p">,</span> <span class="s">&#39;GUF&#39;</span><span class="p">,</span>
</span><span class='line'>            <span class="s">&#39;PYF&#39;</span><span class="p">,</span> <span class="s">&#39;GAB&#39;</span><span class="p">,</span> <span class="s">&#39;GMB&#39;</span><span class="p">,</span> <span class="s">&#39;GEO&#39;</span><span class="p">,</span> <span class="s">&#39;DEU&#39;</span><span class="p">,</span> <span class="s">&#39;GHA&#39;</span><span class="p">,</span> <span class="s">&#39;GIB&#39;</span><span class="p">,</span> <span class="s">&#39;GRC&#39;</span><span class="p">,</span> <span class="s">&#39;GRL&#39;</span><span class="p">,</span>
</span><span class='line'>            <span class="s">&#39;GRD&#39;</span><span class="p">,</span> <span class="s">&#39;GLP&#39;</span><span class="p">,</span> <span class="s">&#39;GUM&#39;</span><span class="p">,</span> <span class="s">&#39;GTM&#39;</span><span class="p">,</span> <span class="s">&#39;GIN&#39;</span><span class="p">,</span> <span class="s">&#39;GNB&#39;</span><span class="p">,</span> <span class="s">&#39;GUY&#39;</span><span class="p">,</span> <span class="s">&#39;HTI&#39;</span><span class="p">,</span> <span class="s">&#39;VAT&#39;</span><span class="p">,</span>
</span><span class='line'>            <span class="s">&#39;HND&#39;</span><span class="p">,</span> <span class="s">&#39;HKG&#39;</span><span class="p">,</span> <span class="s">&#39;HUN&#39;</span><span class="p">,</span> <span class="s">&#39;ISL&#39;</span><span class="p">,</span> <span class="s">&#39;IND&#39;</span><span class="p">,</span> <span class="s">&#39;IDN&#39;</span><span class="p">,</span> <span class="s">&#39;IRN&#39;</span><span class="p">,</span> <span class="s">&#39;IRQ&#39;</span><span class="p">,</span> <span class="s">&#39;IRL&#39;</span><span class="p">,</span>
</span><span class='line'>            <span class="s">&#39;IMY&#39;</span><span class="p">,</span> <span class="s">&#39;ISR&#39;</span><span class="p">,</span> <span class="s">&#39;ITA&#39;</span><span class="p">,</span> <span class="s">&#39;JAM&#39;</span><span class="p">,</span> <span class="s">&#39;JPN&#39;</span><span class="p">,</span> <span class="s">&#39;JOR&#39;</span><span class="p">,</span> <span class="s">&#39;KAZ&#39;</span><span class="p">,</span> <span class="s">&#39;KEN&#39;</span><span class="p">,</span> <span class="s">&#39;KIR&#39;</span><span class="p">,</span>
</span><span class='line'>            <span class="s">&#39;PRK&#39;</span><span class="p">,</span> <span class="s">&#39;KOR&#39;</span><span class="p">,</span> <span class="s">&#39;KWT&#39;</span><span class="p">,</span> <span class="s">&#39;KGZ&#39;</span><span class="p">,</span> <span class="s">&#39;LAO&#39;</span><span class="p">,</span> <span class="s">&#39;LVA&#39;</span><span class="p">,</span> <span class="s">&#39;LBN&#39;</span><span class="p">,</span> <span class="s">&#39;LSO&#39;</span><span class="p">,</span> <span class="s">&#39;LBR&#39;</span><span class="p">,</span>
</span><span class='line'>            <span class="s">&#39;LBY&#39;</span><span class="p">,</span> <span class="s">&#39;LIE&#39;</span><span class="p">,</span> <span class="s">&#39;LTU&#39;</span><span class="p">,</span> <span class="s">&#39;LUX&#39;</span><span class="p">,</span> <span class="s">&#39;MAC&#39;</span><span class="p">,</span> <span class="s">&#39;MKD&#39;</span><span class="p">,</span> <span class="s">&#39;MDG&#39;</span><span class="p">,</span> <span class="s">&#39;MWI&#39;</span><span class="p">,</span> <span class="s">&#39;MYS&#39;</span><span class="p">,</span>
</span><span class='line'>            <span class="s">&#39;MDV&#39;</span><span class="p">,</span> <span class="s">&#39;MLI&#39;</span><span class="p">,</span> <span class="s">&#39;MLT&#39;</span><span class="p">,</span> <span class="s">&#39;MHL&#39;</span><span class="p">,</span> <span class="s">&#39;MTQ&#39;</span><span class="p">,</span> <span class="s">&#39;MRT&#39;</span><span class="p">,</span> <span class="s">&#39;MUS&#39;</span><span class="p">,</span> <span class="s">&#39;MYT&#39;</span><span class="p">,</span> <span class="s">&#39;MEX&#39;</span><span class="p">,</span>
</span><span class='line'>            <span class="s">&#39;FSM&#39;</span><span class="p">,</span> <span class="s">&#39;MDA&#39;</span><span class="p">,</span> <span class="s">&#39;MCO&#39;</span><span class="p">,</span> <span class="s">&#39;MNG&#39;</span><span class="p">,</span> <span class="s">&#39;MTN&#39;</span><span class="p">,</span> <span class="s">&#39;MSR&#39;</span><span class="p">,</span> <span class="s">&#39;MAR&#39;</span><span class="p">,</span> <span class="s">&#39;MOZ&#39;</span><span class="p">,</span> <span class="s">&#39;MMR&#39;</span><span class="p">,</span>
</span><span class='line'>            <span class="s">&#39;NAM&#39;</span><span class="p">,</span> <span class="s">&#39;NRU&#39;</span><span class="p">,</span> <span class="s">&#39;NPL&#39;</span><span class="p">,</span> <span class="s">&#39;NLD&#39;</span><span class="p">,</span> <span class="s">&#39;ANT&#39;</span><span class="p">,</span> <span class="s">&#39;NCL&#39;</span><span class="p">,</span> <span class="s">&#39;NZL&#39;</span><span class="p">,</span> <span class="s">&#39;NIC&#39;</span><span class="p">,</span> <span class="s">&#39;NER&#39;</span><span class="p">,</span>
</span><span class='line'>            <span class="s">&#39;NGA&#39;</span><span class="p">,</span> <span class="s">&#39;NIU&#39;</span><span class="p">,</span> <span class="s">&#39;NFK&#39;</span><span class="p">,</span> <span class="s">&#39;MNP&#39;</span><span class="p">,</span> <span class="s">&#39;NOR&#39;</span><span class="p">,</span> <span class="s">&#39;PSE&#39;</span><span class="p">,</span> <span class="s">&#39;OMN&#39;</span><span class="p">,</span> <span class="s">&#39;PAK&#39;</span><span class="p">,</span> <span class="s">&#39;PLW&#39;</span><span class="p">,</span>
</span><span class='line'>            <span class="s">&#39;PAN&#39;</span><span class="p">,</span> <span class="s">&#39;PNG&#39;</span><span class="p">,</span> <span class="s">&#39;PRY&#39;</span><span class="p">,</span> <span class="s">&#39;PER&#39;</span><span class="p">,</span> <span class="s">&#39;PHL&#39;</span><span class="p">,</span> <span class="s">&#39;PCN&#39;</span><span class="p">,</span> <span class="s">&#39;POL&#39;</span><span class="p">,</span> <span class="s">&#39;PRT&#39;</span><span class="p">,</span> <span class="s">&#39;PRI&#39;</span><span class="p">,</span>
</span><span class='line'>            <span class="s">&#39;QAT&#39;</span><span class="p">,</span> <span class="s">&#39;REU&#39;</span><span class="p">,</span> <span class="s">&#39;ROM&#39;</span><span class="p">,</span> <span class="s">&#39;RUS&#39;</span><span class="p">,</span> <span class="s">&#39;RWA&#39;</span><span class="p">,</span> <span class="s">&#39;SHN&#39;</span><span class="p">,</span> <span class="s">&#39;KNA&#39;</span><span class="p">,</span> <span class="s">&#39;LCA&#39;</span><span class="p">,</span> <span class="s">&#39;SPM&#39;</span><span class="p">,</span>
</span><span class='line'>            <span class="s">&#39;VCT&#39;</span><span class="p">,</span> <span class="s">&#39;WSM&#39;</span><span class="p">,</span> <span class="s">&#39;SMR&#39;</span><span class="p">,</span> <span class="s">&#39;STP&#39;</span><span class="p">,</span> <span class="s">&#39;SAU&#39;</span><span class="p">,</span> <span class="s">&#39;SEN&#39;</span><span class="p">,</span> <span class="s">&#39;SRB&#39;</span><span class="p">,</span> <span class="s">&#39;SYC&#39;</span><span class="p">,</span> <span class="s">&#39;SLE&#39;</span><span class="p">,</span>
</span><span class='line'>            <span class="s">&#39;SGP&#39;</span><span class="p">,</span> <span class="s">&#39;SVK&#39;</span><span class="p">,</span> <span class="s">&#39;SVN&#39;</span><span class="p">,</span> <span class="s">&#39;SLB&#39;</span><span class="p">,</span> <span class="s">&#39;SOM&#39;</span><span class="p">,</span> <span class="s">&#39;ZAF&#39;</span><span class="p">,</span> <span class="s">&#39;ESP&#39;</span><span class="p">,</span> <span class="s">&#39;LKA&#39;</span><span class="p">,</span> <span class="s">&#39;SDN&#39;</span><span class="p">,</span>
</span><span class='line'>            <span class="s">&#39;SUR&#39;</span><span class="p">,</span> <span class="s">&#39;SJM&#39;</span><span class="p">,</span> <span class="s">&#39;SWZ&#39;</span><span class="p">,</span> <span class="s">&#39;SWE&#39;</span><span class="p">,</span> <span class="s">&#39;CHE&#39;</span><span class="p">,</span> <span class="s">&#39;SYR&#39;</span><span class="p">,</span> <span class="s">&#39;TJK&#39;</span><span class="p">,</span> <span class="s">&#39;TZA&#39;</span><span class="p">,</span> <span class="s">&#39;THA&#39;</span><span class="p">,</span>
</span><span class='line'>            <span class="s">&#39;TGO&#39;</span><span class="p">,</span> <span class="s">&#39;TKL&#39;</span><span class="p">,</span> <span class="s">&#39;TON&#39;</span><span class="p">,</span> <span class="s">&#39;TTO&#39;</span><span class="p">,</span> <span class="s">&#39;TUN&#39;</span><span class="p">,</span> <span class="s">&#39;TUR&#39;</span><span class="p">,</span> <span class="s">&#39;TKM&#39;</span><span class="p">,</span> <span class="s">&#39;TCA&#39;</span><span class="p">,</span> <span class="s">&#39;TUV&#39;</span><span class="p">,</span>
</span><span class='line'>            <span class="s">&#39;UGA&#39;</span><span class="p">,</span> <span class="s">&#39;UKR&#39;</span><span class="p">,</span> <span class="s">&#39;ARE&#39;</span><span class="p">,</span> <span class="s">&#39;GBR&#39;</span><span class="p">,</span> <span class="s">&#39;USA&#39;</span><span class="p">,</span> <span class="s">&#39;VIR&#39;</span><span class="p">,</span> <span class="s">&#39;URY&#39;</span><span class="p">,</span> <span class="s">&#39;UZB&#39;</span><span class="p">,</span> <span class="s">&#39;VUT&#39;</span><span class="p">,</span>
</span><span class='line'>            <span class="s">&#39;VEN&#39;</span><span class="p">,</span> <span class="s">&#39;VNM&#39;</span><span class="p">,</span> <span class="s">&#39;WLF&#39;</span><span class="p">,</span> <span class="s">&#39;ESH&#39;</span><span class="p">,</span> <span class="s">&#39;YEM&#39;</span><span class="p">,</span> <span class="s">&#39;ZMB&#39;</span><span class="p">,</span> <span class="s">&#39;ZWE&#39;</span><span class="p">]</span>
</span><span class='line'>
</span><span class='line'><span class="n">quad_codes</span> <span class="o">=</span> <span class="p">[</span><span class="s">&#39;2&#39;</span><span class="p">,</span> <span class="s">&#39;3&#39;</span><span class="p">]</span>
</span><span class='line'>
</span><span class='line'><span class="n">filepaths</span> <span class="o">=</span> <span class="n">path</span><span class="o">.</span><span class="n">getcwd</span><span class="p">()</span><span class="o">.</span><span class="n">files</span><span class="p">(</span><span class="s">&#39;*.reduced.txt&#39;</span><span class="p">)</span>
</span><span class='line'><span class="n">output</span> <span class="o">=</span> <span class="nb">list</span><span class="p">()</span>
</span></code></pre></td></tr></table></div></figure>


<p>This first portion is simply importing the necessary modules, defining the list
of state actors, obtaining the relevant filepaths, and defining an empty list,
which will serve as the container for the subset of the data.
As a brief note, the <code>path</code> module is really fantastic and makes working with
filepaths and directories extremely simple.</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
</pre></td><td class='code'><pre><code class='python'><span class='line'><span class="k">for</span> <span class="n">path</span> <span class="ow">in</span> <span class="n">filepaths</span><span class="p">:</span>
</span><span class='line'>    <span class="n">data</span> <span class="o">=</span> <span class="nb">open</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="s">&#39;r&#39;</span><span class="p">)</span>
</span><span class='line'>    <span class="k">print</span> <span class="s">&#39;Just read in the </span><span class="si">%s</span><span class="s"> data...&#39;</span> <span class="o">%</span> <span class="n">path</span>
</span><span class='line'>    <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">data</span><span class="p">:</span>
</span><span class='line'>        <span class="n">line</span> <span class="o">=</span> <span class="n">line</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s">&#39;</span><span class="se">\n</span><span class="s">&#39;</span><span class="p">,</span> <span class="s">&#39;&#39;</span><span class="p">)</span>
</span><span class='line'>        <span class="n">split_line</span> <span class="o">=</span> <span class="n">line</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s">&#39;</span><span class="se">\t</span><span class="s">&#39;</span><span class="p">)</span>
</span><span class='line'>        <span class="n">condition1</span> <span class="o">=</span> <span class="n">split_line</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="mi">0</span><span class="p">:</span><span class="mi">3</span><span class="p">]</span> <span class="o">==</span> <span class="s">&#39;USA&#39;</span>
</span><span class='line'>        <span class="n">condition2</span> <span class="o">=</span> <span class="n">split_line</span><span class="p">[</span><span class="mi">2</span><span class="p">][</span><span class="mi">0</span><span class="p">:</span><span class="mi">3</span><span class="p">]</span> <span class="o">!=</span> <span class="s">&#39;USA&#39;</span>
</span><span class='line'>        <span class="n">condition3</span> <span class="o">=</span> <span class="n">split_line</span><span class="p">[</span><span class="mi">2</span><span class="p">][</span><span class="mi">0</span><span class="p">:</span><span class="mi">3</span><span class="p">]</span> <span class="ow">in</span> <span class="n">allActors</span>
</span><span class='line'>        <span class="n">condition4</span> <span class="o">=</span> <span class="n">split_line</span><span class="p">[</span><span class="mi">4</span><span class="p">]</span> <span class="ow">in</span> <span class="n">quad_codes</span>
</span></code></pre></td></tr></table></div></figure>


<p>The next portion of code iterates over the filepaths obtained using <code>path</code>.
Each file is then opened and iterated over line by line. Each line
has any new-line characters replaced, which makes the data easier to work with,
and is then split on the basis of tab characters (&#8216;\t&#8217;). The following four lines
define the logical conditions for subsetting the data. The first condition
indicates that the first three characters of <code>Actor1Code</code> should be
&#8216;USA&#8217;, while <code>condition2</code> states that the first three characters of
<code>Actor2Code</code> should <em>not</em> equal &#8216;USA&#8217;. <code>condition3</code> checks if the first three
characters of <code>Actor2Code</code> are in the <code>allActors</code> list defined earlier, while
<code>condition4</code> checks if the <code>QuadCategory</code> is one of the desired values.</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class='python'><span class='line'><span class="k">try</span><span class="p">:</span>
</span><span class='line'>    <span class="k">if</span> <span class="nb">all</span><span class="p">([</span><span class="n">condition1</span><span class="p">,</span> <span class="n">condition2</span><span class="p">,</span> <span class="n">condition3</span><span class="p">,</span> <span class="n">condition4</span><span class="p">]):</span>
</span><span class='line'>        <span class="n">output</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">split_line</span><span class="p">)</span>
</span><span class='line'><span class="k">except</span> <span class="ne">IndexError</span><span class="p">:</span>
</span><span class='line'>    <span class="k">pass</span>
</span></code></pre></td></tr></table></div></figure>


<p>The above code simply checks if all of the various conditions were met, and
if so appends the <code>split_line</code> to <code>output</code>. This code is wrapped in a try-except
statement since there can be some malformed lines floating in the data, but this
should not affect the actual event data. The try-except statements allow for
an attempt at a certain block of code, and if an error is raised, in this case
an <code>IndexError</code>, for some other actions to occur.</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
</pre></td><td class='code'><pre><code class='python'><span class='line'><span class="n">header</span> <span class="o">=</span> <span class="nb">open</span><span class="p">(</span><span class="n">filepaths</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="s">&#39;r&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">readline</span><span class="p">()</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s">&#39;</span><span class="se">\t</span><span class="s">&#39;</span><span class="p">)</span>
</span><span class='line'><span class="n">subset</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">columns</span> <span class="o">=</span> <span class="n">header</span><span class="p">)</span>
</span><span class='line'>
</span><span class='line'><span class="n">subset</span><span class="p">[</span><span class="s">&#39;year&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">subset</span><span class="p">[</span><span class="s">&#39;Day&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="nb">int</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">x</span><span class="p">)[</span><span class="mi">0</span><span class="p">:</span><span class="mi">4</span><span class="p">]))</span>
</span><span class='line'><span class="n">subset</span><span class="p">[</span><span class="s">&#39;month&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">subset</span><span class="p">[</span><span class="s">&#39;Day&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="nb">int</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">x</span><span class="p">)[</span><span class="mi">4</span><span class="p">:</span><span class="mi">6</span><span class="p">]))</span>
</span><span class='line'>
</span><span class='line'><span class="n">keep_columns</span> <span class="o">=</span> <span class="p">[</span><span class="s">&#39;year&#39;</span><span class="p">,</span> <span class="s">&#39;month&#39;</span><span class="p">,</span> <span class="s">&#39;Actor1Code&#39;</span><span class="p">,</span> <span class="s">&#39;Actor2Code&#39;</span><span class="p">,</span> <span class="s">&#39;QuadCategory&#39;</span><span class="p">]</span>
</span><span class='line'><span class="n">subset</span> <span class="o">=</span> <span class="n">subset</span><span class="p">[</span><span class="n">keep_columns</span><span class="p">]</span>
</span></code></pre></td></tr></table></div></figure>


<p>Once the code from the previous sections is finished, the subset of the data is
contained in <code>output</code>, which is a list-of-lists with the internal lists representing
the individual events. It is possible, using the <code>pandas</code> library, to convert this
list-of-lists to a pandas DataFrame object, with the header drawn from the first line
of the first file in <code>filepaths</code>. In order to aggregate the data to a specific time period,
it is useful to break out the <code>Day</code> variable into months and years using the <code>.map</code>
functionality of a Series object in <code>pandas</code>. The <code>.map</code> functionality is combined
with a lambda function to select eaach observation in the series and slice it
in order to obtain the year and month. Finally, the last two lines of the above
code reduce the data down to only the columns relevant for this subset.</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
</pre></td><td class='code'><pre><code class='python'><span class='line'><span class="n">subset</span><span class="p">[</span><span class="s">&#39;verbal_coop&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
</span><span class='line'><span class="n">subset</span><span class="p">[</span><span class="s">&#39;verbal_conf&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
</span><span class='line'>
</span><span class='line'><span class="n">subset</span><span class="p">[</span><span class="s">&#39;verbal_coop&#39;</span><span class="p">][</span><span class="n">subset</span><span class="p">[</span><span class="s">&#39;QuadCategory&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="s">&#39;2&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
</span><span class='line'><span class="n">subset</span><span class="p">[</span><span class="s">&#39;verbal_conf&#39;</span><span class="p">][</span><span class="n">subset</span><span class="p">[</span><span class="s">&#39;QuadCategory&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="s">&#39;3&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
</span><span class='line'>
</span><span class='line'><span class="n">subset_grouped</span> <span class="o">=</span> <span class="n">subset</span><span class="o">.</span><span class="n">groupby</span><span class="p">([</span><span class="s">&#39;year&#39;</span><span class="p">,</span> <span class="s">&#39;month&#39;</span><span class="p">,</span> <span class="s">&#39;Actor1Code&#39;</span><span class="p">,</span>
</span><span class='line'><span class="s">&#39;Actor2Code&#39;</span><span class="p">],</span> <span class="n">as_index</span> <span class="o">=</span> <span class="bp">False</span><span class="p">)</span>
</span><span class='line'><span class="n">subset_aggregated</span> <span class="o">=</span> <span class="n">subset_grouped</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
</span><span class='line'>
</span><span class='line'><span class="n">subset_aggregated</span><span class="o">.</span><span class="n">to_csv</span><span class="p">(</span><span class="s">&#39;gdelt_subset.csv&#39;</span><span class="p">,</span> <span class="n">index</span> <span class="o">=</span> <span class="bp">False</span><span class="p">)</span>
</span></code></pre></td></tr></table></div></figure>


<p>Now that the data is in a properly formatted DataFrame, the next step is to create
variables from which to draw counts of the various event types. Two variables,
<code>verbal_coop</code> and <code>verbal_conf</code> are created and assigned values of zero. Then,
these variables are assigned values of one if the <code>QuadCategory</code> matches the
value for that event type. This functionality in <code>pandas</code> is similar that of
<code>R</code>, and I plan on doing a more in-depth tutorial on <code>pandas</code> at a later date.
With the event variables created, the data can be grouped and aggregated. <code>pandas</code>
has the <code>.groupby</code> functionality for DataFrames, which allows you to group the data
by specific variables. For the purposes of this dataset, a multi-level grouping is
desired, with groups created by the year, the month, and the dyad. Once this grouping
is created, the values are summed within each grouping leading to the final, aggregated
dataset. This final data can be saved using the <code>.to_csv</code> method.</p>

<p>And there you have it, you now have a subset of the GDELT dataset.
This is a relatively simple task thanks to Python&#8217;s built in tools. It is
possible to make this task run more quickly using parallel processing, as
outlined in my previous <a href="http://johnbeieler.org/blog/2012/12/07/parallel-data-subsetting/">post</a>
on parallel subsetting. As a brief recap, it is simply
a matter of using the <code>jobilb</code> module, wrapping the above subsetting code in
a function, and adding something along the lines of</p>

<pre><code>data = Parallel(n_jobs=-1)(delayed(subset)(x) for x in list_of_paths)
</code></pre>

<p>to the script. I hope this brief intro will prove helpful to those who wish
to use GDELT in their own research. This tutorial only scratched the surface
of working with event data, and there is much more to consider beyond just
what subset you will select. A good resource on working with event data
was written by <a href="http://jayyonamine.com/wp-content/uploads/2012/06/Working-with-Event-Data-A-Guide-to-Aggregation-Choices.pdf">Jay Yonamine</a>
and will likely prove useful to those engaging in event-data research.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Web Scraping Tutorial]]></title>
    <link href="http://johnb30.github.com/blog/2013/02/08/web-scraping-tutorial/"/>
    <updated>2013-02-08T20:39:00-05:00</updated>
    <id>http://johnb30.github.com/blog/2013/02/08/web-scraping-tutorial</id>
    <content type="html"><![CDATA[<p>I had the opportunity to give a short tutorial on web scraping for the Event
Data class here at Penn State. I used an IPython notebook to give the
presentation and I&#8217;ve put the code in a gist. The link to the IPython notebook
is <a href="http://nbviewer.ipython.org/4743272">http://nbviewer.ipython.org/4743272/</a>.</p>

<p>The PITF project I make reference to is hosted on
<a href="https://github.com/johnb30/atrocitiesProject">github</a>.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Parallel Data Subsetting]]></title>
    <link href="http://johnb30.github.com/blog/2012/12/07/parallel-data-subsetting/"/>
    <updated>2012-12-07T18:26:00-05:00</updated>
    <id>http://johnb30.github.com/blog/2012/12/07/parallel-data-subsetting</id>
    <content type="html"><![CDATA[<h3>The Challenge</h3>

<p>I&#8217;ve been working with some data that is spread over multiple
tab-delimited files, and is rather large (on the order of 20-30gb). The task has
been to comb through the data, and extract observations (rows) if they match
certain characteristics. Specifically, each file is iterated over and if a field
within a line matches a given value then the line should be extracted and
appended to the final output. This task is relatively straightforward in Python,
but to iterate over all of the files takes around 45 minutes. While this isn&#8217;t
an exorbiant amount of time, fast is always better. This problem is also
embarassingly parallel; the different files do not need to communicate their
results to each other, the results simply need to be stacked into a final array
in order to be saved to a file. Thus began my saga to implement a parallel
version of a script I wrote to iterate over the files and select the lines.</p>

<h3>The Code</h3>

<div><script src='https://gist.github.com/4237347.js?file='></script>
<noscript><pre><code>import numpy as np
from joblib import Parallel, delayed

def subset(file):
    dataOut = []
    data = open(file, 'r')
    data.readline()
    for line in data:
        splitLine = line.split('\t')
        if splitLine[3] == '57':
            dataOut.append(splitLine)
    return dataOut

def stack(list_of_data, hold_data):
    for i in xrange(len(list_of_data)):
        current = np.array(data[i])
        hold = np.vstack((hold_data, current))
    return hold

if __name__ == &quot;__main__&quot;:
    filepath = ['testData1.txt', 'testData2.txt']

    hold  = []
    temp = open(filepath[0], 'r')
    hold.append(temp.readline().split('\t'))
    data = Parallel(n_jobs=-1)(delayed(subset)(x) for x in filepath)
    finalData = stack(data, hold)
</code></pre></noscript></div>


<p>As with my other posts I&#8217;ll walk through the code line by line.</p>

<ul>
<li><p>1-2 Just imports. I&#8217;ll be using the <code>Parallel</code> and
<code>delayed</code> functions from the joblib module.</p></li>
<li><p>4-13 Defining the function to subset out the data. The code is fairly easy to
understand here. The file is opened, the first line is read since this
contains the column names and should not be appended to the result. Then each
line is iterated over, split on the basis of the tabs, and appended if it
meets a certain criteria.</p></li>
<li><p>15-19 Function to stack the data. The joblib <code>Parallel</code> function will return a
list of lists, with each list within the list being the results from the
individual files. The <code>stack</code> function iterates over the list, converts the
inner lists to numpy arrays, and stacks the current data with the previous
data.</p></li>
<li><p>22-28 Running the script. The main focus here is on line 27. The first
argument that <code>Parallel</code> takes is the number of jobs to be used. Setting
<code>n_jobs</code> to -1 says to use all possible cores. The second argument is the
function to be run in parallel. The joblib docs indicate that &#8220;The delayed
function is a simple trick to be able to create a tuple
(function, args, kwargs) with a function-call syntax.&#8221; So delayed is passed
the <code>subset</code> function with the arg <code>x</code>, which represents the file to be opened
as held in <code>filepath</code>. This data is then stored as a list of lists, and
stacked using the <code>stack</code> function.</p></li>
</ul>


<p>Some quick prelimenary examinations shows that this parallel implementation is
much, much faster than running in serial. Running on two files is almost
instananeous, which is a drastic improvement.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[R Magic and Bootstrapped t-test]]></title>
    <link href="http://johnb30.github.com/blog/2012/11/29/r-magic-and-bootstrapped-t-test/"/>
    <updated>2012-11-29T21:53:00-05:00</updated>
    <id>http://johnb30.github.com/blog/2012/11/29/r-magic-and-bootstrapped-t-test</id>
    <content type="html"><![CDATA[<p>Following up on my last post, I wanted a way to test my bootstrapped t-test
function against the regular t-test function in R. While I was able to do this
by copy-pasting between R and a Python shell, this was less than ideal. I then
saw, however, a <a href="http://nbviewer.ipython.org/4166681/">post</a> by Christopher
Fonnesbeck that discussed the use of the rmagic function in ipython, which can
be loaded using the %load_ext magic function. So, with this in mind, I decided
to test it out using a comparison between my bootstrap function and the
<code>t.test</code> function in R. As a note, the rmagic extension requires
<a href="http://rpy.sourceforge.net/rpy2.html">rpy2</a>, so just <code>pip install rpy2</code> and
you should be good to go.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>import bootFunction
</span><span class='line'>
</span><span class='line'>%load_ext rmagic
</span><span class='line'>%R treatment = c(24,25,28,28,28,29,29,31,31,35,35,35)
</span><span class='line'>%R control = c(21,22,24,27,27,28,29,32,32)
</span><span class='line'>%Rpull treatment control
</span><span class='line'>
</span><span class='line'>bootFunction.bootstrap_t_test(treatment, control, direction = "greater")
</span><span class='line'>
</span><span class='line'>%R print(t.test(treatment, control, alternative = "greater"))</span></code></pre></td></tr></table></div></figure>


<p>I first import the set of functions from the bootFunction. I then load the
rmagic extension using the <code>%load_ext</code> magic function. Using the <code>%R</code> magic
function I then defined two vectors of data, treatment and control, in the R
space. I then used <code>%Rpull</code> to pull the two vectors from the R space into the
Python shell. The two variables become structured numpy arrays.
I then perform the bootstrapped t-test as described in the earlier post.
Finally, using the <code>%R</code> magic function again, I print out the results of the
<code>t.test</code> function in R using the same data. The p-values aren&#8217;t exactly the
same, as is to be expected, but are at least within the same ballpark (the R
t-test gives .05, while the boostrap function has returned a range between .05
and .03).</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Bootstrapped t-test]]></title>
    <link href="http://johnb30.github.com/blog/2012/11/28/bootstrapping-t-test/"/>
    <updated>2012-11-28T19:47:00-05:00</updated>
    <id>http://johnb30.github.com/blog/2012/11/28/bootstrapping-t-test</id>
    <content type="html"><![CDATA[<p>The below code is to perform a nonparametric two-sample t-test using
bootstrapping. First I present the code, and will then follow up with a
line-by-line description of what&#8217;s going on.</p>

<div><script src='https://gist.github.com/4166016.js?file='></script>
<noscript><pre><code>from __future__ import division
import numpy as np
import pandas as pd
import random

def sample(data):
    sample = [random.choice(data) for _ in xrange(len(data))]
    return sample

def bootstrap_t_test(treatment, control, nboot = 1000, direction = &quot;less&quot;):
    ones = np.vstack((np.ones(len(treatment)),treatment))
    treatment = ones.conj().transpose()
    zeros = np.vstack((np.zeros(len(control)), control))
    control = zeros.conj().transpose()
    Z = np.vstack((treatment, control))
    tstat = np.mean(treatment[:,1])-np.mean(control[:,1])
    tboot = np.zeros(nboot)
    for i in xrange(nboot):
        sboot = sample(Z)
        sboot = pd.DataFrame(np.array(sboot), columns=['treat', 'vals'])
        tboot[i] = np.mean(sboot['vals'][sboot['treat'] == 1]) - np.mean(sboot['vals'][sboot['treat'] == 0]) - tstat
    if direction == &quot;greater&quot;:
        pvalue = np.sum(tboot&gt;=tstat-0)/nboot
    elif direction == &quot;less&quot;:
        pvalue = np.sum(tboot&lt;=tstat-0)/nboot
    else:
        print 'Enter a valid arg for direction'

    print 'The p-value is %f' % (pvalue)
</code></pre></noscript></div>


<ul>
<li><p>1-4: Just some imports. We need the floating point division from the
future module, numpy, pandas, and the random module.</p></li>
<li><p>6-8: Defining a function <code>sample</code> that samples with replacement from the
dataset, creating a new dataset of the same length as the original data.
This makes use of the <code>random.choice</code> function, which samples one item
randomly from the data. This function is called the same number of times as
there are observations in the data.</p></li>
<li><p>10-17: Defining a function to perform the t-test, with two data inputs,
the number of repititions to be performed, and the direction of the
alternative hypothesis. First a 2 x <em>n</em> matrix is defined, with row 1 being
all ones and row 2 being the data. This is then flipped to create an <em>n</em> x 2
matrix. The same procedure is then repeated for the control data, except with
0s instead of 1s. These two matrices are then stacked on top of each other.
<code>tstat</code> is the difference between the two groups, and tboot is a vector of
zeros with length equal to the number of repititions for the bootstrap.</p></li>
<li><p>18-21: This for-loop actually performs the bootstrap for the number of times
indicated by <code>nboot</code>. First, a sample of the data (<code>Z</code>) is taken using the
<code>sample</code> function defined above. This is then transformed into a pandas
DataFrame, and given appropriate column names. Finally, the difference in
means of the two groups is taken for each iteration of the loop and stored in
the appropriate location in <code>tboot</code>.</p></li>
<li><p>22-28: This is simply calculating a proportion of samples that were greater or
less than the test statistic, based on the direction of the alternative
hypothesis. The final line (29) then prints the p-value as a float.</p></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Starting With Python]]></title>
    <link href="http://johnb30.github.com/blog/2012/11/24/starting-with-python/"/>
    <updated>2012-11-24T19:10:00-05:00</updated>
    <id>http://johnb30.github.com/blog/2012/11/24/starting-with-python</id>
    <content type="html"><![CDATA[<h2>Overview</h2>

<h3>What is this?</h3>

<p>In short, setting up Python (and other things) for scientific computing and
research can be entirely more complicated than necessary. With that said, this
aims to be a short how-to guide pointing to some resources that can make life
much easier. This post is geared towards Political Scientists coming from either
1) using R as a programming language or 2) having no programming and minimal
computing experience. Most of the things listed here have been attempted by me,
but I make no guarantees that anything will work properly or won&#8217;t mess
something up when attempted. As with anything proceed with caution and at your
own risk.</p>

<p>This how-to is mainly geared towards OS X, but many of the suggestions should
also work on Linux (and are probably easier). I don&#8217;t have any experience
setting up Windows and would probably suggest looking into dual booting Linux
(see <a href="http://www.ubuntu.com/">here</a> for more). Downloading Ubuntu to a CD and
setting up a dual boot is extremely easy.</p>

<p>I&#8217;ll be adding to this as I have time and think of different things that have
helped me. I know this post is long, but there is a large amount of information
to share, and I think it is easier to get a lot of it in one place, rather than
spread out.</p>

<!-- more -->


<h3>Basics</h3>

<p>I&#8217;m going to start from the assumption that if you&#8217;re reading this you&#8217;ve never
dealt with a command-line interface, or if you have that you&#8217;ve only been
briefly exposed (maybe through things like R or Stata). So, before working with
these sorts of things it&#8217;s helpful to get acquainted with the Terminal
(Command Prompt in Windows). You should be able to find this in your
Applications folder in the Utilities folder. Go ahead and drag the app to your
dock. It will make life easier for you to have it here.</p>

<p>If you open it up you will see something like</p>

<pre><code>John-B-MacBook-Pro:~ john$ 
</code></pre>

<p>The word (john here) right before the $ is your username. There are some basic
commands for working with the Terminal:</p>

<ul>
<li><code>ls</code> shows you all of the files and folders in your current working directory</li>
<li><code>ls -a</code> shows all files and folders including those that are hidden</li>
<li><code>cd</code> allows you to move from one location to another</li>
<li><code>mv</code> lets you move files</li>
<li><code>cp</code> is copying</li>
</ul>


<p>and many, many more. A basic workflow is as below:</p>

<pre><code>$ ls
Applications    Desktop     Documents

$ cd Documents
$ ls -a
test.txt    .hidden.txt

$ mv test.txt /Users/johnbeieler/Desktop
</code></pre>

<p>When in doubt Google what you want to do followed by &#8220;Terminal.&#8221; So, something
like &#8220;remove file terminal&#8221; or &#8220;copy file terminal.&#8221; A word of warning, you&#8217;re
&#8220;closer to the metal&#8221; when using a command-line interface. This means that you
have a lot of power and flexibility when working with things, but it also means
you have the ability to completely wipe your hard drive if you type the wrong
command. Be careful.</p>

<p>You should probably go ahead and install XCode. This is included on the install
CD that came with your Mac, or it can be downloaded from the Mac App Store.
Make sure that you&#8217;re using the correct version for your version of OS X.
This will install some things that will be necessary to work with later.
Linux has an awesome concept called package managers, which allow you to easily
install different applications and, well, packages by typing something like
&#8220;sudo apt-get package&#8221; into the Terminal and things are automagically downloaded
and installed. Macs lack this functionality. But! Some enterprising individuals
have come up with a way to help. By heading to
<a href="http://mxcl.github.com/homebrew/">http://mxcl.github.com/homebrew/</a> you can
download a package manager that makes
life much easier. Feel free to look around for utilities that can be installed
using homebrew that might be of use to you. As a word of advice, if you are on
the verge of installing something, first check and see if it is available using
homebrew since brew keeps things nice and organized in your usr/local folder
instead of spread all over your computer.</p>

<h2>Python</h2>

<p>OS X comes with Python preloaded and is required by the operating system for
many functions. This is good and bad. Good since you can type</p>

<pre><code>$ python 
</code></pre>

<p>and get up and running in an interactive session. It&#8217;s also bad because the
structure of the Python installation on OS X can create some difficulties with
certain libraries. This leaves two options:
First, you can go with the default Python implementation.
This will necessitate (sort of) the use of the Scipy Superpack
<a href="http://fonnesbeck.github.com/ScipySuperpack/">http://fonnesbeck.github.com/ScipySuperpack/</a>.
The Superpack installs nearly every awesome Python library that your scientific
researcher heart could desire. As a brief rundown of what each package does:</p>

<h4>Numpy and Scipy</h4>

<p>The heart of numerical computing in Python. These two libraries give array and
matrix functions along with many other cool things. Numpy is short for Numeric
Python and Scipy is short for Scientific Python. Many other libraries in Python
are dependent on these. Much (digital) ink has been spilled on using these two
so feel free to search around for more on how an array is different than a
matrix in Numpy (hint: You should probably use an array).</p>

<h4>Matplotlib</h4>

<p>Plotting functions in Python. Allows you to make pretty graphs.</p>

<h4>IPython</h4>

<p>Stands for Interactive Python. When running python from the Terminal you should
type <code>ipython</code> instead of <code>python</code>.  IPython gives many different
magic functions and has all kinds of need goodies in it that generally make life
easier.</p>

<h4>Pandas</h4>

<p>The best thing since sliced bread and pockets on jeans. Allows the R dataframe
functionality in Python. Supports complex indexing for panel data, creation of
various statistics such as moving averages, includes various read and write
functionalities. It has some awesome documentation so go check it out.</p>

<h4>Statsmodels</h4>

<p>Statistical models in Python. This one is pretty self explanatory but is
tremendously useful and is more intuitive than R in many ways.</p>

<h4>Scikit-learn</h4>

<p>Machine learning in Python. Has some of the most comprehensive documentation
around, including a series of tutorials on how to get started with machine
learning in general.</p>

<h4>PyMC</h4>

<p>Bayesian inference in Python. MCMC and more.</p>

<h4>Other Utilities</h4>

<p>nose, readline and DateUtils. Things that are useful for other packages.
You can read up on these more if you would like. Of importance, however, is
nose. Nose is a testing suite for Python that allows you to see if anything is
wonky in your installation. You can (maybe) get away with skipping these, but
it never hurts. Look up the different tests for each of the utilities if you
want to run them.</p>

<p>In all honesty it is probably easiest to use the Superpack. The other option is
to download another Python distrubution that has all of these things included
plus a &#8220;vanilla&#8221; (non-Apple) build of Python. Some examples of these are:</p>

<ul>
<li>Enthought Python <a href="https://enthought.com/products/edudownload.php">https://enthought.com/products/edudownload.php</a></li>
<li>Python(x,y) <a href="https://code.google.com/p/pythonxy/">https://code.google.com/p/pythonxy/</a></li>
</ul>


<p>But really, just use the Superpack.</p>

<h2>Other things</h2>

<p>There are some other things that are useful (read: necessary) to use Python in
any meaningful way. One of the most important is setuptools.</p>

<p>To install setuptools:</p>

<p>1) Go to <a href="http://pypi.python.org/pypi/setuptools">http://pypi.python.org/pypi/setuptools</a>.
Download the .egg file located towards the bottom of the page. Since your
version is 2.7 you would download (as of 08/29/2012) setuptools-0.6c11-py2.7.egg.</p>

<p>2) Place it on your desktop. Do NOT change the name.</p>

<p>3) Cd to your desktop in the Terminal:</p>

<pre><code>$ cd Desktop
$ sh setuptools-0.6c11-py2.7.egg 
</code></pre>

<p>4) That should be it.</p>

<p>What setuptools allows you to do is type <code>easy_install package</code> and it will
install that package for your use in Python. Some people suggest that a program
called pip is better because it has additional features such as the ability to
easily uninstall programs. To install pip you just type:</p>

<pre><code>$ easy_install pip 
</code></pre>

<p>That&#8217;s right. Pip is installed using easy_install.</p>

<p>Let&#8217;s try it out for a library called Scrapy. Scrapy is described as</p>

<blockquote><p>Scrapy is a fast high-level screen scraping and web crawling framework, used to
crawl websites and extract structured data from their pages. It can be used for
a wide range of purposes, from data mining to monitoring and automated testing.</p></blockquote>

<p>Sounds pretty cool. All you do is open up Terminal and type:</p>

<pre><code>$ easy_install Scrapy 
</code></pre>

<p>Alternatively if you&#8217;re using pip:</p>

<pre><code>$ pip install Scrapy 
</code></pre>

<h2>Miscellany</h2>

<p>There are some other things that are useful to have when doing this kind of
programming/coding/scripting type work that aren&#8217;t related directly to working
with Python.</p>

<h4>Version Control</h4>

<p>First is git and github. Git is what&#8217;s called a &#8220;version control system.&#8221; Have
you ever been working on a paper and saved your work only to realize that you
wrote over some changes that you didn&#8217;t mean to? Me too. Git keeps a detailed
list of the versions of a file, including any changes or additions made to a
specific version, and allows you to roll back to a previous version. So, if you
wrote over a file and want to revert back you just have to find the version you
want.</p>

<p>Github is a &#8220;social coding platform.&#8221; It&#8217;s basically git on the internet. You&#8217;re
reading this on github right now. Normally git and github involve some (semi)
complicated Terminal commands. Github has, however, provided a nice, functional
program to use git and github. The Mac version is here <a href="http://mac.github.com/">http://mac.github.com/</a>.</p>

<h4>Text Editors</h4>

<p>Text editors can cause a surprising amount of consternation on the internet. A
decent editor can make your life easier with features such as syntax
highlighting, autoindentation, tab completion, and other features. I won&#8217;t say
which editor to use, but will give you a list of some of the big ones.
(This was written using vim, MacVim to be specific).</p>

<p><strong>Gedit:</strong> Standard on Linux distros. Simple. Includes some syntax highlighting.</p>

<p><strong>Emacs:</strong> One of the big two editors. Built on LISP. Can basically be whatever
you want it to be. Uses extensive, and sometimes complicated, key combinations
to get things done.</p>

<p><strong>Vim:</strong> The second of the big two. Has different modes such as insert and
normal. Takes some getting used to. Has a ton of different add-ons.</p>

<p><strong>Sublime Text 2:</strong> Probably more straightforward than Emacs or Vim, but more
powerful than gedit. More modern than either Emacs or Vim. Free to try for a
bit.</p>

<h4>Other</h4>

<p>I&#8217;ll add a shameless plug here for some code that I wrote, py_apsrtable. This
is designed to provide easy functions to take output from Python statistical
packages and turn it into pretty LaTeX tables. To install</p>

<pre><code>$ pip install py_apsrtable
</code></pre>

<p>Documentation is on <a href="http://johnb30.github.com/py_apsrtable">github</a>.</p>

<p>I will also add as a final point that it is probably nice to take a look at
&#8220;The Zen of Python&#8221; by opening up a python shell, <code>ipython</code>, and typing</p>

<pre><code>import this
</code></pre>

<p>Next, take a look at the Python style guide contained in <a href="http://www.python.org/dev/peps/pep-0008/#code-lay-out">PEP8</a>
(PythonEnhancement Proposal 8). Following these guidelines will allow your code
to be consistent with the prevailing style for Python code.</p>

<p>I know this was slightly rambling, and there are <em>numerous</em> points that I have
missed, but I hope this provides some information that will be useful to those
trying to get setup with Python for research. If you have any questions or
suggestions, please feel free to contact me.</p>
]]></content>
  </entry>
  
</feed>
